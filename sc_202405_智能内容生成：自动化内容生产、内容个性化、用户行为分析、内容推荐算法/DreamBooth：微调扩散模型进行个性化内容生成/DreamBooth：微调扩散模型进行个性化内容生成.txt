


            
DreamBooth：微调扩散模型进行个性化内容生成
          




大家在使用Huggingface的Diffusers时，会看到提供了一些Fine-Tune的方法，比如Textual Inversion、LORA、DreamBooth等等，它们均是为了实现"personalization" of text-to-image diffusion models这一任务。简单来说，就是实现了一种个性化文生图模型，给定某个物体的几张图片作为输入，通过微调预训练的文生图模型（如Imagen），将一个独特的标识符和该物体进行绑定，这样就可以通过含有该标识符的prompt在不同场景下生成包含该物体的新颖图片。如下图所示，输入图片中包含一个闹钟，我们希望在不同场景下生成的图片里闹钟的样子尽可能与输入图片保持一致。现有基于扩散模型的文生图模型已经可以根据给定的prompt生成高质量的图片。但是这些模型并不能模仿给定参考图片中物体的样子在不同场景中来生成新的图片。DreamBooth 针对的使用场景是，期望生成同一个主体的多张不同图像， 就像照相馆一样，可以为同一个人或者物体照多张不同背景、不同姿态、不同服装的照片。 DreamBooth 就类似照相馆，可以通过文本 Prompt 控制每次都生成同一个主体的不同背景、姿态、服装的图像， 这个主体可以是人、动物、物体等等。我们用一个例子说明。假设你养了一条小狗，你非常喜爱它，你想为它生成各种可爱的照片，把它打造成一个网红。然而每次用照相机拍照成本太高，很多场景也布置不出来， 这时候就可以用 DreamBooth + Stable Diffusion 批量生成多彩多姿的图片。只需要提前准备好 3~5 张小狗的图片，给它起个名字吗，然后使用 DreamBooth 方法微调一下预训练的 Stable Diffusion 模型，生成图像时，在提示词里加入小狗的名字，就能生成以你的小狗为主体的各式各样的图片。如下图所示：DreamBooth 并不是一个独立的图像生成模型，它是一种微调技术，通过对现有的预训练好的图像生成模型进行微调， 进而实现能通过文本 Prompt 控制生成同一个主体的不同场景的图像。与 ControlNet 不同的是， DreamBooth 要简单很多，它并没有修改 Unet 的网络结构，也没有增加额外的网络。 DreamBooth 仅仅是在文本 Prompt 动了手脚，不需要对扩散模型做任何改动。首先，需要准备3~5张的目标主题的照片，可以是你的自拍美照，也可以是宠物的照片，任何特定实体的多张照片。最好是不同角度的照片，但要突出你先要的实体。然后，给这个实体起一个名字，这个名字最好是比较罕见的词汇，尽量不要用常见词，必然会产生歧义。这个名字作为这个主体的 唯一标识符 ，接下来的目的是要让模型学习到 (唯一标识符，特定主体)这个关系。接下来，就要为每一张照片设置一段文本提示语，这个文本提示语的格式为”一个[标识符][类名词]” 其中 [标识符] 是主体的唯一标识符， [类名词] 是主体的粗略类描述符（例如，猫、狗、手表等） ，类描述符可以由用户提供或通过分类器获得，类描述符指明了这个主体是什么，是非常重要的信息。它相当于一种先验知识，把我们的主体和这个类别联系起来， 论文中提到，使用错误的类描述符或没有类描述符会增加训练时间和语言漂移，同时降低性能。从本质上讲， 我们试图利用模型对特定类别的先验，并将其与我们主体的独特标识符的嵌入联系起来， 这样我们就可以利用视觉先验来生成主体在不同语境中的新姿势和表述。唯一标识符的选择也很重要，模型需要把这个标识符和照片中的主体联系起来， 如果是一个本身就很常见的单词或者词汇，在仅有少量样本的情况下， 模型是很难学习到的。如果样本太多或者学习轮次太多，又容易造成过拟合现象， 此时模型就会忘记这个词汇原本的含义，这就造成了语言漂移现象。这就促使需要一个在语言模型和扩散模型中都具有弱先验的标识符。一个危险的方法是选择英语中的随机字符，并将它们连接起来，生成一个罕见的标识符（例如 “xy5syt00”）。在现实中， tokenizer 可能会对每个字母分别进行分割，而扩散模型的先验对这些字母是很强的。我们经常发现，这些字母 tokens 会招致与使用普通英语单词类似的弱点。解决的方法是，首先在文本模型的在词汇表（vocabulary）中寻找出罕见的 tokens ， 把这些稀有 tokens 作为候选集合，从这些稀有的 tokens 中选出k = {1，... ，3}个出来组成文本作为主体的唯一描述符。论文中介绍，对于 Imagen ，使用 T5-XXL 的 tokenizer 生成的编号为{5000，... ，10000}范围内的 tokens 作为候选集， 从中随机抽取3个罕见 tokens 或者更少一点的 Unicode字符（不含空格），效果很不错。根据论文作者的经验，通过对模型的所有层进行微调， 可以获得最大的主体保真度的最佳结果。这包括对文本编码器进行微调，但是这就产生了 语言漂移 的问题。语言漂移是语言模型中观察到的一个问题， 即一个在大型文本语料库中预训练的模型，后来为一个特定的任务进行了微调， 逐渐失去了语言的句法和语义知识。这会影响扩散模型，模型慢慢地忘记了如何生成与目标主体 相同类别 的主体。另一个问题是可能会减少图像生成的多样性。 文本到图像的扩散模型自然拥有大量的输出多样性， 在对一小部分图像进行微调时，我们希望能够以新的视角、姿势和衔接方式生成主体。 然而，有一种风险是主体的姿势和视角可能会变得单一。 这种情况经常发生，特别是当模型训练时间过长时。论文提出的方法是想用少量图片（如3到5张）去微调文生图模型，微调过程中这些图片中都包含有相同的物体，且图片对应的prompt基本相同，都为a[identifier] [class noun]的形式，如果只用普通的微调方式，会出现两个问题：（1）过拟合（2）语言漂移：在大量文本语料上预训练的语言模型，在特定任务上微调时，它会逐渐忘记通用的语言知识，而仅仅适配特定的任务作者因此用了一个简单的方法来解决这个问题，也就是上图中提到的Class-Specific Prior Preservation Loss，对原有Diffusion模型的Loss Function进行了修改，保证模型能够在保证类别先验的前提下，学到我们新提供的subject知识。具体的Loss Function如下：我们可以发现，这个Loss分为两个部分：第一部分就是用新提供的subject images训练的Reconstruction Loss，与一般的Diffusion模型损失形式一样，对Diffusion的所有层进行微调；第二部分就是新引入的Class-Specific Prior Preservation Loss，用一个现有的Diffusion生成图像来监督模型，λ控制该项的相对权重。第一部分让模型学习特定物品的表示，后半通过生成图片的监督防止模型忘记先验知识，其中代表扩散模型，即输入图片 ，通过扩散模型加噪去噪后生成的图片尽量要和原始输入图片尽量保持一致，后半部分对模型生成的训练数据也一样。以下是总结的具体使用DreamBooth的方法：准备3~5张，特定的主题的照片，并设定好图片的描述文本：”一个[标识符][类名词]”，作为微调训练样本 A.用预训练的扩散模型以 “一个[类名词]” 作为提示语生成一批相同类别的图片样本，作为微调训练样本 B.合并练样本 A 和 B，对扩散模型进行微调训练。在论文中，研究人员也提出了DreamBooth目前存在的缺陷：第一个是无法准确生成提示的上下文。可能的原因是这些上下文的先验较弱，或者由于训练集中共现概率较低，难以同时生成主题和指定概念。第二个是上下文外观纠缠，由于提示的上下文，主题的外观会发生变化，如上图所示，背包的颜色变化。第三，我们还观察到对提示与看到主题的原始设置相似的真实图像的过度拟合。比如上图森林里一个包的图片，生成的基本和原先投入训练的没有任何区别。其他限制是某些主题比其他主题更容易学习（例如狗和猫）。有时，对于更罕见的主题，模型无法支持尽可能多的主题变化。最后，根据模型先验的强度和语义修改的复杂性，主题保真度和一些生成的图像也可能包含幻觉的主题特征。DreamBooth 方法可以为同一个主体生成不同的图像，这是非常常见的场景，所以 DreamBooth 技术其实非常有价值。而且它和 ControlNet 并不冲突，可以和 ControlNet 一同使用，生成更加随心所欲的图片。 DreamBooth 的训练方法并不复杂，简单实用。huggingface 提供了详细的训练教程和例子，地址在： https://huggingface.co/docs/diffusers/training/dreambooth 之后应该会更新一篇具体如何使用DreamBooth的教程~参考文献：Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: fine tuning text-to-image diffusion models for subject-driven generation. 2023. arXiv:2208.12242.https://zzh-blog.readthedocs.io/zh-cn/latest/aigc/dreamBooth.htmlhttps://zhuanlan.zhihu.com/p/659774932#/下一页  上一页




