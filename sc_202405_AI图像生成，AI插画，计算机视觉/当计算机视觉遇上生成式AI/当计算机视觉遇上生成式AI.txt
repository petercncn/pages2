当计算机视觉遇上生成式AI
我们花篇幅探讨计算机视觉(或者更狭义的机器视觉)也不是第一次了。去年电子工程专辑的封面故事也谈过计算机视觉——但当时的内容更多是限定在了计算机从外界获取、理解影像信息，还偏向关注其中的感知部分。

从维基百科的定义来看，计算机视觉(computer 
vision)广义上是包括了获取、处理、分析、理解数字图像，以及从现实世界剖析高维度数据，生成数字与符号信息、做出决策的技术。其中关键仍然是从图像(image，包含了视频，也包含像是来自LiDAR产生的3D点云)里面来获取和理解信息。

这么说还是挺抽象的，从一些子领域，可以让我们更好地搞明白究竟什么是计算机视觉：场景重建、对象检测/识别、事件检测、动作识别、视频追踪、3D姿势判断、信息检索、3D场景建模、影像恢复......这些都是计算机视觉。

比如其中的影像恢复(image restoration)，就是基于对原有影像画面的理解和信息获取，借助算法、AI来修复、强化原影像。

去年有关计算机视觉的探讨，我们将AI视作其当前发展的重要推力。这里的AI当然是指神经网络。很显然AI 
CV(计算机视觉)现如今是经常一同出现的，这没什么好奇怪的。不过今年AI领域发生了众所周知的大事件：生成式AI、Transformer模型成为热词。

其实在今年世界人工智能大会上，我们和Graphcore的工程师聊了聊——这是一家英国的AI大算力芯片企业。他们就提到目前就生成式AI技术，已经在CV领域内与相关企业开展合作，落地场景包括了修图、绘画辅助、建筑渲染等等。

AI Accelerator Institute也在年中列出的2023计算机视觉5大趋势中，将生成式AI排在头一个。

本文中，我们就尝试谈谈计算机视觉与生成式AI，究竟是怎么搭上伙儿的，以及生成式AI能为计算机视觉带来些什么;更多的，也算是给Transformer于计算机视觉价值的科普。


图1：Midjourney生成的以“计算机视觉”为关键词的图片。



从CNN到Transformer

关注AI的同学应该很清楚，AI 
CV以往总是和CNN(卷积神经网络)挂钩的。或者说CNN对于完成图像、视频相关的工作更被众人所知。从医疗到汽车，CNN应用于CV的局势也是一片大好。

通过大量的训练数据，CNN本身能够“学习”某个对象的特性。比如说要识别图像中的狗，神经网络层级结构更靠前的层学习诸如图像对象边缘、纹理之类的特性;更深入一点的层则做更高级别的特性学习，比如耳朵、尾巴，和其他能够做出区分的样貌特性......

CNN的关键就在C(convolution)卷积上。卷积的过程，可以看做是在输入图像或者特征图上，不停移动一个小型的filter，在每一块空间区域内执行乘加运算。CNN卷积操作的一个关键是参数共享(parameter 
sharing)，同组权重可用于输入的所有位置，减少了参数量，跨不同的空间位置做到了网络的通用化。总之这样一个过程就实现了可行的图像信息获取。

那我们现在常说的Transformer又是什么呢?Transformer结构采用所谓的self-attention(自注意力)机制，捕捉全局相关性、在一个队列内不同element的关系。整个过程差不多是这样的：输入包含了一系列的token(像我们与ChatGPT对话的单字)，每个token嵌入到连续向量空间中。然后会有个encoder位置编码操作，应用于嵌入的token，提供token在序列中的相对位置，让模型能够理解其先后顺序。

这里encoder的每一层都有两个子层，第一层就是self-attention机制实现，让队列中的每个token都能参考其他的token，获取全局相关性;第二层是个前馈神经网络(feed-forward 
NN)，独立针对每个token操作。

而decoder解码部分则有多个层，每一层再包含了3个子层。decoder中的self-attention机制能够让token关联其他decoder 
token，在decoder内抓取相关性信息。encoder-decoder的attention子层则让decoder关联到编码的输入队列。最后decoder里面的前馈神经网络子层独立针对每个token操作，参见图2。

还有包括deocder的输出层预测队列下一个token的可能性之类的操作。从整个流程就不难发现，Transformer更适合NLP(natrual 
language 
processing)，其中的self-attention机制能够让队列中的每个元素与所有其他元素做关联，模型就能基于元素关联上下文，来权衡其重要性。

总的来说，CNN和Transformer的特性决定了它们的不同应用方向。很自然地CNN擅长图像分类、对象识别之类的工作，而Transformer在自然语言理解和生成方面就有优势。


图2：Transformer模型结构。来源：Attention Is All You Need(arXiv:1706.03762)



生成式AI与计算机视觉

如果说有这样一个任务，同时包含了图像和文字工作，比如一张图像，再配个文字输入请求，又该怎么办呢?考虑让CNN做文字相关工作，或者让Transformer做图像相关工作如何?

对于CNN而言，通过前面的解释，我们知道它能抓取图像中，限定区域内由像素构成的特定图案;但针对token文本，获取上下文关联性，CNN就显得无能为力。而且前面就提到了CNN特性之一的参数共享，它假设了相同模型在输入数据的任意位置都适用。这对文本处理工作不适用，因为不同文字出现在句子不同位置都会表达不同的意思。

如果用Transformer来处理图像工作，那就变成了self-attention要求图像中每个像素都和其他像素做上下文关联。以当代图像的像素量级，以及芯片可提供的算力水平而言是不现实的。

不过2021年Google 
Research团队有人发了个paper。他们提出一种解决方案，把一张图像切割成小片，然后把每一片当成一个单字、token。那么Transformer基于这些单字，就能以较高的精度来学习如何识别对象了。这种方法有很不错的并行度与灵活性，适用于大规模图像识别任务。

而且还能把Transformer输出图像的小切片与文本融合，这就带来了应用的极大可能性。然后2年内就因此诞生了海量的相关研究paper。刨去那些知名度很高的不谈，近代颇具代表性的，应该是Meta的SAM(Segment 
Anything Model)，团队用MAE(Masked Auto-Encoder)这个Vision 
Transformer，加上CLIP，搭配图像+输入提示词，来生成所谓的mask。

SAM主要解决的是图像里面的分割(segmentation)问题，也就是将图像分成不同区域或对象的过程，就像是用PS抠图(见图3)。此后已经有不少演示用SAM，搭配Diffusion，把输入照片中的对象，换个样子，比如说头像直接换个发型、换个不同材质的衣服。


图3：SAM官网的图像分割演示。

实际上2020年就已经诞生了ViT，也就是Vision 
Transformer这个专有词汇，用于描述专为计算机视觉设计的Transformer。基本结构如前所述，就是将输入图像剖成小块小块的，做所谓的“tokenized”，再将这些token应用到标准的Transformer结构中：就像把NLP上的那种方式平移到了CV上一样。

在输入给生成式AI，到给出新的图像或者文字结果的这个过程，对于很多相关计算机视觉的行业而言，就已经产生了潜在颠覆的可能，也真正拓宽了计算机视觉的边界。

这里列举国外计算机视觉研究人员常谈到的三类算法，作为构成计算机视觉领域内生成式AI的基本模块——内部结构部分多少都有Transformer的影子：GAN(生成式对抗网络)、VAE(Variational 
Autoencoder)、DDPM(Denoising Deiffusion Probabilistic Model)。

GAN的知名度应该很高了，两个神经网络：生成器(generator)和鉴别器(discriminator)。其中生成器生成合成图像期望能骗过鉴别器，鉴别器则努力分辨出有问题的，最终可达成持续的提升。

VAE也在很多谈生成式AI的场合出现过，采用的就是encoder-decoder结构，encoder将输入图像转入隐空间(latent 
space);一番炼丹过后，decoder则从低维表达中把图像重构出来。据说VAE擅长理解复杂数据，对于艺术创作而言价值巨大。

VAE也是Stable 
Diffusion和DALL-E的重要组成部分，即便似乎VAE在这两者内部扮演的角色是不一样的。DDPM应当是专注于提升信噪比的，不过在流程上似乎是增加杂讯，则在逆向路径上就能理解如何消除噪声。



真正的应用会有哪些?

如果说芯片之上都是应用，那本文所谈的东西都还挺靠上层应用的，多少有点不符“电子工程专辑”的名号。不过不要忘记，我们在任何一次AI芯片技术介绍文章里，都会花将近一半的篇幅去谈生态和软件——因为这些都是AI芯片企业需要去做的。

对这些大算力芯片企业而言，软件工具和生态构建难度一点也不比造个芯片出来简单;且在这个内卷的时代里，还得尽可能地替下游客户做更多的事情。那么上述部分也一定是他们关注的发展趋势。毕竟什么样的算法成为热门，芯片和对应的工具就该有什么样的应变。

就好像这一代英伟达GPU特别加了个Transformer 
Engine那样。如果说生成式AI对CV如此有价值，那么具体都能做些什么呢?感觉这个问题能畅想的空间非常大，或许此处可以尝试列举一些。

基于传统算法的很多CV技术，都可以借助生成式AI搞定。比如说图像超分(super 
resolution)，也就是把低分辨率的画面转为高分辨率——可能超分并不符合“生成式AI”的确切定义，但至少它给出了原本缺失的信息量。而且超分并不单纯是把旧照片、旧视频显示得更高清，据说现在已经有应用将低分辨率的医疗扫描影像转为高分辨率、更细节化的影像作为参考，就能提高医疗审定的精度。

艺术与内容创作领域，大概也是最直接的应用市场。所以今年AI相关的展会上，AI芯片企业普遍都在展示这些应用，甚至发布企业级产品——比如世界人工智能大会上，燧原科技发布的曜图。因为生成式AI能够生成细节化，且精细的视觉作品。

我们起初一直有个困惑，就是内容创作也能算“计算机视觉”吗?这个问题或许应当考虑的是，像ViT之类的模型，其产生的过程就符合计算机视觉的定义。因为无论如何，它都要求计算机首先理解那些喂给它的数据，并最终给出决策。只不过现在的这些应用还显得非常局限。所以即便是输入文字-生成图像，它也是计算机视觉技术的反映。电子工程专辑现有的某些插图，比如本文里的，就是生成式AI的产物。


图4：Nvidia Canvas演示。

此前在英伟达GTC大会上看到Canvas的演示(图4)，应该是内容创作方向上，生成式AI最为惊艳的应用之一。在画布上随意画几笔，形成一幅简笔画，Canvas就能将其转为细节丰富的拟真照片。这应该属于场景构建类的应用了。当时英伟达为我们演示了基于Canvas生成的素材来做CG动画。

今年的秋季GTC上，黄仁勋演示了从一张二维平面图，将一个简简单单的PDF文档，加上和生成式AI之间的文字沟通，就生成可交互的3D虚拟工厂，还是相当震撼的。虽然我们认为其可用性还相当值得怀疑，但这表明了生成式AI在工业制造、元宇宙、数字孪生领域的潜在价值。仔细想一想，这是不是也完美符合计算机视觉的定义?

那么在此，决定生成式AI与计算机视觉市场潜力的问题，应该也就转到了各行各业的数字化转型，能够创造多大价值的问题。因为一切数字孪生技术的基础都会与计算机视觉与AI强相关。

一些老生常谈的应用，比如时尚设计领域，AI能生成定制化的设计方案，并且在零售环节给出虚拟的试穿体验;对汽车领域，生成式AI应该正准备铺设到各个环节，包括自动驾驶的虚拟测试，汽车工厂开工前的虚拟数字孪生建设、零售环节的AR体验等等;那么在建筑、城市规划、娱乐、工业等等市场的潜在价值自无须赘述…

感觉讲着讲着，应用方向列举就已经走向了“生成式AI”能干什么的问题。实际上，在社会数字化转型过程里，AI 
CV一定是作为技术组成部分参与到全过程来的。所以生成式AI也好，计算机视觉也好，它们都是数字经济时代的发展基础。




