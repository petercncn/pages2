


            
AI视频生成的新方法
          




继前两篇文章我们讲解了AI视频生成的两种方法，今天我们将继续介绍目前较热门的Pika视频生成网站及SVD方法的使用与测试效果。美国AI初创公司Pika Labs日前发布了其首款视频生成产品Pika1.0，在官网发布的宣传片中，Pika1.0生成的视频效果非常惊艳，让人称奇。我们也是第一时间就申请了试用，经过了一段时间的等待，终于通过了，下面就让我们一起看看Pika的生成效果如何吧。Pika的主界面如下Pika跟上期我们提到的Runway gen-2类似，也可通过文字生成视频、文字+图像生成视频、图像生成视频（无需文字）、视频生成视频（用文字和示例图像进行视频风格改变）。文字生成视频，我们可以先在框1处填写想要展现的视频内容，也叫正向提示词；然后点击框2，输入不想在视频里出现的内容，即反向提示词（Negative prompt），还可填写随机种子数（Seed），不同的随机种子数生成的视频不一样，也可设置视频与文本提示词相关性（Consistency with the text）。接着我们点开框3，这里可以设置相机运动方向和速率，从上至下依次是左、右、上、下、逆时针、顺时针、变焦放大、变焦缩小。点击框4，就能设置视频画面比例以及帧率。最后点击生成，即可生成一段3秒的视频，如果觉得时长不够可以在视频生成后再点击延长视频，点击一次增加4秒。用“a big ocean wave at daybreak, Deep sea swell, cinematic, film, moody, high resolution”做提示词生成的视频如下：若想对生成的视频进行修改，可以点开视频，以下的功能按钮依次为重试、重新输入提示词、编辑、增加4秒、放大视频（提示分辨率）。其中，点击编辑按钮后，可通过Modify region单独修改视频中某个主体，而Expand canvas能够调整视频长宽比例。此外，想要使用图片+文字生成视频或者进行视频风格变化，只需要点击下图框中的按钮来上传图片或视频即可。下面给出一些Pika与Gen-2的对比视频。这个对比中我输入的提示词为“A train pulled into the platform”，可以看出Pika生成的视频，分辨率和流畅度明显比Gen-2要高，也出现了站台和行人，但是它似乎理解错了我想让它前进的方向。而这个对比，就可以看出Pika更好的理解了我输入“An astronaut is walk”的意思，Gen-2就是简单的进行了相机的运动。最后这个对比视频是仅文字生成视频的。看起来Pika与Gen-2都有着不错的效果。创造了Stable diffusion的Stability AI也在最近发布了生成式视频模型Stable Video Diffusion（SVD），该模型通过在小型、高质量的视频数据集上插入时间层并微调，将用于2D图像合成的潜在扩散模型转化为生成视频模型。按照官方的说法，使用SVD-XT模型可以生成25帧的视频，并且在他们调研的用户偏好研究中胜出了runway gen-2和Pika labs。我们测试了其发布的图像到视频模型。测试的效果如下：其中第二个视频里的测试图与runway gen-2的笔刷测试其中一张为同一张，大家可以比较一下哪个更好。目前想使用SVD，电脑需要有强大的显卡性能，有条件的小伙伴可以去尝试一下。11月28日，阿里巴巴智能计算研究院也开发了Animate Anyone，只需要提供一个静态的角色图像以及一些预设的动作（或姿势序列），即可生成该角色的动画视频，并且视频也能保证静态角色图像中的外观特征一致性。该方法不仅适用于真实人类，还可以用于卡通动漫角色和类人角色。该方法设计了ReferenceNet，通过空间注意力来合并特征，保持参考图像中的复杂外观特征一致性；还引入了一个高效的姿势引导器来指导角色的动作，采用有效的时间建模方法来确保视频帧之间的过渡。截至本文完成时，此项目源码和模型还未公开，等有更新时，我们会第一时间进行测试。当然，随着人工智能技术的不断进步，AI视频生成的应用范围远不止于广告制作。在电影特效、游戏开发、虚拟现实（VR）、混合现实（MR）等领域，特别是在个性化内容创作上，AI可以根据用户的喜好和行为模式生成定制化的视频内容，从而提供更加个性化的观看体验。未来，随着算力的提升和算法的优化，有理由相信AI视频生成技术将更加成熟和普及。从而涌现出更多创新的应用场景，推动多媒体娱乐、内容创作等行业的革新，为使用者带来前所未有的便利和体验。  END




