
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <meta name="description" content="本文详细介绍了GAN的其中一种应用cycleGAN，并将它应用到图像风格的转换">
                <meta name="keywords" content="CycleGAN 生成对抗网络图像处理工具, 本文详细介绍了GAN的其中一种应用cycleGAN，并将它应用到图像风格的转换">
                <meta property="og:title" content="CycleGAN 生成对抗网络图像处理工具">
                <title>CycleGAN 生成对抗网络图像处理工具</title>
                <style>.hidden{visibility:hidden;font-size:1px;}</style>
            </head>
            <body>
                <div class="rich_media_wrp" id="img-content">
<h1 class="rich_media_title" id="activity-name">
            
CycleGAN 生成对抗网络图像处理工具
          </h1>

<div class="rich_media_content js_underline_content" id="js_content" style="visibility: visible;"><p data-mpa-powered-by="yiban.io" style="text-align: center;"><img class="rich_pages" data-ratio="0.5051963048498845" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1b1rku8qVKcXZ00FC4ky8WSeWqkxXwQbv3hdptWCNSyUZg1Ipa4WHVjw/640?wx_fmt=png" data-type="png" data-w="1732" src="20240525_024158_0.jpeg" style=""/></p><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com" style='font-size: 16px;color: black;padding-right: 10px;padding-left: 10px;line-height: 1.6;letter-spacing: 0px;word-break: break-word;text-align: left;font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;'><h2 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 22px;line-height: 1.75em;"><span style="letter-spacing: 2px;">1. GAN简介</span></h2><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">"干饭人，干饭魂，干饭都是人上人"。</span></section><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">此GAN饭人非彼干饭人。本文要讲的GAN是Goodfellow2014提出的生成产生对抗模型，即Generative Adversarial Nets。那么GAN到底有什么神奇的地方?</span></section><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">常规的深度学习任务如图像分类，目标检测以及语义分割或者实例分割，这些任务的结果都可以归结为预测。图像分类是预测单一的类别，目标检测是预测bbox和类别，语义分割或者实例分割是预测每个像素的类别。而GAN是生成一个新的东西如一个图片。</span></section><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">GAN的原理用一句话来说明：</span></section><ul class="list-paddingleft-2" data-tool="mdnice编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;"><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">通过对抗的方式，去学习数据分布的生成式模型。GAN是无监督的过程，能够捕捉数据集的分布，以便于可以从随机噪声中生成同样分布的数据</span></section></li></ul><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">GAN的组成：判别式模型和生成式模型的左右手博弈</span></section><ul class="list-paddingleft-2" data-tool="mdnice编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;"><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">D判别式模型：学习真假边界，判断数据是真的还是假的</span></section></li><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">G生成式模型：学习数据分布并生成数据</span></section></li></ul><p style="text-align: center;"><img class="rich_pages" data-ratio="0.6388101983002833" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bSdSemxsko9VHHFDjs5jBaLCKAu1bHcBZ1icRcINAIlxB85jNVct9Bkw/640?wx_fmt=png" data-type="png" data-w="706" src="20240525_024200_1.jpeg" style=""/></p><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">GAN经典的loss如下（minmax体现的就是对抗）</span></section><p style="text-align: center;"><img class="rich_pages" data-ratio="0.10202020202020202" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bCYOviauH9WYBn62dLSratIjfh1CXIG3sw1KBKqUBwfJEshcAck8OILA/640?wx_fmt=png" data-type="png" data-w="990" src="20240525_024201_2.jpeg" style=""/></p><h2 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 22px;line-height: 1.75em;"><span style="letter-spacing: 2px;">2. 实战cycleGAN 风格转换</span></h2><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">了解了GAN的作用，来体验的GAN的神奇效果。这里以cycleGAN为例子来实现图像的风格转换。所谓的风格转换就是改变原始图片的风格，如下图左边是原图，中间是风格图（梵高画），生成后是右边的具有梵高风格的原图，可以看到总体上生成后的图保留大部分原图的内容。</span></section><p style="text-align: center;"><img class="rich_pages" data-ratio="0.2681704260651629" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bpjxCI70olgnqfS7HAQrzwZcwpZXrbYlT7b9iaNiao3xo5MIkAqQy2gfA/640?wx_fmt=png" data-type="png" data-w="798" src="20240525_024202_3.jpeg" style=""/></p><h3 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 20px;line-height: 1.75em;"><span style="letter-spacing: 2px;">2.1 cycleGAN简介</span></h3><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">cycleGAN本质上和GAN是一样的，是学习数据集中潜在的数据分布。GAN是从随机噪声生成同分布的图片，cycleGAN是在有意义的图上加上学习到的分布从而生成另一个领域的图。cycleGAN假设image-to-image的两个领域存在的潜在的联系。</span></section><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">众所周知，GAN的映射函数很难保证生成图片的有效性。cycleGAN利用cycle consistency来保证生成的图片与输入图片的结构上一致性。我们看下cycleGAN的结构：</span></section><p style="text-align: center;"><img class="rich_pages" data-ratio="0.38102893890675243" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bQB7icj7JQlrDFJH4q32MEOlDblT8EqicbtxGbt3Picxfn6BtzIaXFHWrQ/640?wx_fmt=png" data-type="png" data-w="1244" src="20240525_024203_4.jpeg" style=""/></p><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;"><strong>特点总结如下：</strong></span></section><ul class="list-paddingleft-2" data-tool="mdnice编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;"><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">两路GAN：两个生成器[ G：X-&gt;Y , F：Y-&gt;X ]  和两个判别器[Dx, Dy], G和Dy目的是生成的对象，Dy（正类是Y领域）无法判别。同理F和Dx也是一样的。</span></section></li><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">cycle consistency：G是生成Y的生成器， F是生成X的生成器，cycle consistency是为了约束G和F生成的对象的范围，  是的G生成的对象通过F生成器能够回到原始的领域如：x-&gt;G(x)-&gt;F(G(x))=x</span></section></li></ul><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;"><strong>对抗loss如下：</strong></span></section><p style="text-align: left;"><img class="rich_pages" data-ratio="0.2727272727272727" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bPF9TYZSEtH2UYKrTG9TGRDAByWbKry9miaicP5sK4TGMPInfbFrSNPibw/640?wx_fmt=png" data-type="png" data-w="429" src="20240525_024204_5.jpeg" style=""/></p><p style="text-align: left;"><img class="rich_pages" data-ratio="0.22509225092250923" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bibTywZ3Pbkdwfq8yKLriaaMPIgvZxaiaaQy7XpCsl3fFkc8gEBXqNqgbA/640?wx_fmt=png" data-type="png" data-w="542" src="20240525_024205_6.jpeg" style=""/><span style="letter-spacing: 2px;"></span></p><p style="text-align: left;"><img class="rich_pages" data-ratio="0.18510158013544017" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bdgrFcMmuOcGl8zW4OGYVTMmibzfiaUWicFOWkqaMXvL8Eh3ouIKCAco7w/640?wx_fmt=png" data-type="png" data-w="443" src="20240525_024206_7.jpeg" style=""/></p><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;"></span></section><h3 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 20px;line-height: 1.75em;"><span style="letter-spacing: 2px;">2.2 实现cycleGAN</span></h3><h4 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 18px;line-height: 1.75em;"><span style="letter-spacing: 2px;">2.2.1 生成器</span></h4><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">从上面简介中生成器有两个生成器，一个是正向，一个是反向的。结构是参考论文</span><code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'><span style="letter-spacing: 2px;">Perceptual Losses for Real-Time Style Transfer and Super-Resolution: Supplementary Material</span></code><span style="letter-spacing: 2px;">。大致可以分为：下采样 + residual 残差block + 上采样，如下图（摘自论文）：</span></section><p style="text-align: center;"><img class="rich_pages" data-ratio="0.6016949152542372" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1btagAWmzgrPDTgnnpb7pdwBwuiakypSgE4keJNiaCsfduSKND4jYQiaOKw/640?wx_fmt=png" data-type="png" data-w="944" src="20240525_024207_8.jpeg" style=""/></p><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">实现上下采样是stride=2的卷积， 上采样用nn.Upsample：</span></section><pre data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;border-radius: 5px;box-shadow: rgba(0, 0, 0, 0.55) 0px 2px 10px;"><code style="overflow-x: auto;padding: 16px;color: #ddd;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;font-size: 12px;-webkit-overflow-scrolling: touch;padding-top: 15px;background: #272822;border-radius: 5px;"><span style="color: #75715e;line-height: 26px;"># 残差block</span><br/><span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">class</span> <span style="font-weight: bold;color: white;line-height: 26px;">ResidualBlock</span><span style="line-height: 26px;">(nn.Module)</span>:</span><br/><br/>    <span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">def</span> <span style="color: #a6e22e;font-weight: bold;line-height: 26px;">__init__</span><span style="line-height: 26px;">(self, in_features)</span>:</span><br/>        super(ResidualBlock, self).__init__()<br/><br/>        self.block = nn.Sequential(<br/>            nn.ReflectionPad2d(<span style="line-height: 26px;">1</span>),<br/>            nn.Conv2d(in_features, in_features, <span style="line-height: 26px;">3</span>),<br/>            nn.InstanceNorm2d(in_features),<br/>            nn.ReLU(inplace=<span style="color: #f92672;font-weight: bold;line-height: 26px;">True</span>),<br/>            nn.ReflectionPad2d(<span style="line-height: 26px;">1</span>),<br/>            nn.Conv2d(in_features, in_features, <span style="line-height: 26px;">3</span>),<br/>            nn.InstanceNorm2d(in_features),<br/>        )<br/><br/>    <span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">def</span> <span style="color: #a6e22e;font-weight: bold;line-height: 26px;">forward</span><span style="line-height: 26px;">(self, x)</span>:</span><br/>        <span style="color: #f92672;font-weight: bold;line-height: 26px;">return</span> x + self.block(x)<br/><br/><span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">class</span> <span style="font-weight: bold;color: white;line-height: 26px;">GeneratorResNet</span><span style="line-height: 26px;">(nn.Module)</span>:</span><br/>    <span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">def</span> <span style="color: #a6e22e;font-weight: bold;line-height: 26px;">__init__</span><span style="line-height: 26px;">(self, input_shape, num_residual_blocks)</span>:</span><br/>        super(GeneratorResNet, self).__init__()<br/><br/>        channels = input_shape[<span style="line-height: 26px;">0</span>]<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Initial convolution block</span><br/>        out_features = <span style="line-height: 26px;">64</span><br/>        model = [<br/>            nn.ReflectionPad2d(channels),<br/>            nn.Conv2d(channels, out_features, <span style="line-height: 26px;">7</span>),<br/>            nn.InstanceNorm2d(out_features),<br/>            nn.ReLU(inplace=<span style="color: #f92672;font-weight: bold;line-height: 26px;">True</span>),<br/>        ]<br/>        in_features = out_features<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Downsampling</span><br/>        <span style="color: #f92672;font-weight: bold;line-height: 26px;">for</span> _ <span style="color: #f92672;font-weight: bold;line-height: 26px;">in</span> range(<span style="line-height: 26px;">2</span>):<br/>            out_features *= <span style="line-height: 26px;">2</span><br/>            model += [<br/>                nn.Conv2d(in_features, out_features, <span style="line-height: 26px;">3</span>, stride=<span style="line-height: 26px;">2</span>, padding=<span style="line-height: 26px;">1</span>),<br/>                nn.InstanceNorm2d(out_features),<br/>                nn.ReLU(inplace=<span style="color: #f92672;font-weight: bold;line-height: 26px;">True</span>),<br/>            ]<br/>            in_features = out_features<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Residual blocks</span><br/>        <span style="color: #f92672;font-weight: bold;line-height: 26px;">for</span> _ <span style="color: #f92672;font-weight: bold;line-height: 26px;">in</span> range(num_residual_blocks):<br/>            model += [ResidualBlock(out_features)]<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Upsampling</span><br/>        <span style="color: #f92672;font-weight: bold;line-height: 26px;">for</span> _ <span style="color: #f92672;font-weight: bold;line-height: 26px;">in</span> range(<span style="line-height: 26px;">2</span>):<br/>            out_features //= <span style="line-height: 26px;">2</span><br/>            model += [<br/>                nn.Upsample(scale_factor=<span style="line-height: 26px;">2</span>),<br/>                nn.Conv2d(in_features, out_features, <span style="line-height: 26px;">3</span>, stride=<span style="line-height: 26px;">1</span>, padding=<span style="line-height: 26px;">1</span>),<br/>                nn.InstanceNorm2d(out_features),<br/>                nn.ReLU(inplace=<span style="color: #f92672;font-weight: bold;line-height: 26px;">True</span>),<br/>            ]<br/>            in_features = out_features<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Output layer</span><br/>        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, <span style="line-height: 26px;">7</span>), nn.Tanh()]<br/><br/>        self.model = nn.Sequential(*model)<br/><br/>    <span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">def</span> <span style="color: #a6e22e;font-weight: bold;line-height: 26px;">forward</span><span style="line-height: 26px;">(self, x)</span>:</span><br/>        <span style="color: #f92672;font-weight: bold;line-height: 26px;">return</span> self.model(x)<br/></code></pre><h4 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 18px;line-height: 1.75em;"><span style="letter-spacing: 2px;">2.2.2 判别器</span></h4><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">传统的GAN 判别器输出的是一个值，判断真假的程度。而patchGAN输出是N*N值，每一个值代表着原始图像上的一定大小的感受野，直观上就是对原图上crop下可重复的一部分区域进行判断真假，可以认为是一个全卷积网络，最早是在pix2pix提出（Image-to-Image Translation with Conditional Adversarial Networks）。好处是参数少，另外一个从局部可以更好的抓取高频信息。</span></section><pre data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;border-radius: 5px;box-shadow: rgba(0, 0, 0, 0.55) 0px 2px 10px;"><code style="overflow-x: auto;padding: 16px;color: #ddd;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;font-size: 12px;-webkit-overflow-scrolling: touch;padding-top: 15px;background: #272822;border-radius: 5px;"><span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">class</span> <span style="font-weight: bold;color: white;line-height: 26px;">Discriminator</span><span style="line-height: 26px;">(nn.Module)</span>:</span><br/>    <span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">def</span> <span style="color: #a6e22e;font-weight: bold;line-height: 26px;">__init__</span><span style="line-height: 26px;">(self, input_shape)</span>:</span><br/>        super(Discriminator, self).__init__()<br/><br/>        channels, height, width = input_shape<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Calculate output shape of image discriminator (PatchGAN)</span><br/>        self.output_shape = (<span style="line-height: 26px;">1</span>, height // <span style="line-height: 26px;">2</span> ** <span style="line-height: 26px;">4</span>, width // <span style="line-height: 26px;">2</span> ** <span style="line-height: 26px;">4</span>)<br/><br/>        <span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">def</span> <span style="color: #a6e22e;font-weight: bold;line-height: 26px;">discriminator_block</span><span style="line-height: 26px;">(in_filters, out_filters, normalize=True)</span>:</span><br/>            <span style="color: #a6e22e;line-height: 26px;">"""Returns downsampling layers of each discriminator block"""</span><br/>            layers = [nn.Conv2d(in_filters, out_filters, <span style="line-height: 26px;">4</span>, stride=<span style="line-height: 26px;">2</span>, padding=<span style="line-height: 26px;">1</span>)]<br/>            <span style="color: #f92672;font-weight: bold;line-height: 26px;">if</span> normalize:<br/>                layers.append(nn.InstanceNorm2d(out_filters))<br/>            layers.append(nn.LeakyReLU(<span style="line-height: 26px;">0.2</span>, inplace=<span style="color: #f92672;font-weight: bold;line-height: 26px;">True</span>))<br/>            <span style="color: #f92672;font-weight: bold;line-height: 26px;">return</span> layers<br/><br/>        self.model = nn.Sequential(<br/>            *discriminator_block(channels, <span style="line-height: 26px;">64</span>, normalize=<span style="color: #f92672;font-weight: bold;line-height: 26px;">False</span>),<br/>            *discriminator_block(<span style="line-height: 26px;">64</span>, <span style="line-height: 26px;">128</span>),<br/>            *discriminator_block(<span style="line-height: 26px;">128</span>, <span style="line-height: 26px;">256</span>),<br/>            *discriminator_block(<span style="line-height: 26px;">256</span>, <span style="line-height: 26px;">512</span>),<br/>            nn.ZeroPad2d((<span style="line-height: 26px;">1</span>, <span style="line-height: 26px;">0</span>, <span style="line-height: 26px;">1</span>, <span style="line-height: 26px;">0</span>)),<br/>            nn.Conv2d(<span style="line-height: 26px;">512</span>, <span style="line-height: 26px;">1</span>, <span style="line-height: 26px;">4</span>, padding=<span style="line-height: 26px;">1</span>)<br/>        )<br/><br/>    <span style="line-height: 26px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">def</span> <span style="color: #a6e22e;font-weight: bold;line-height: 26px;">forward</span><span style="line-height: 26px;">(self, img)</span>:</span><br/>        <span style="color: #f92672;font-weight: bold;line-height: 26px;">return</span> self.model(img)<br/></code></pre><p style="text-align: center;"><img class="rich_pages" data-ratio="0.6218579234972678" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1b28ANpVMyqib7oOVKxHMiaPT7dY2P733k5DfwBdqxbtF78BTDsxHDD1SQ/640?wx_fmt=png" data-type="png" data-w="915" src="20240525_024208_9.jpeg" style=""/></p><h4 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 18px;line-height: 1.75em;"><span style="letter-spacing: 2px;">2.2.3 训练</span></h4><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;"><strong>loss和模型初始化</strong></span></section><pre data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;border-radius: 5px;box-shadow: rgba(0, 0, 0, 0.55) 0px 2px 10px;"><code style="overflow-x: auto;padding: 16px;color: #ddd;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;font-size: 12px;-webkit-overflow-scrolling: touch;padding-top: 15px;background: #272822;border-radius: 5px;"><span style="color: #75715e;line-height: 26px;"># Losses</span><br/>criterion_GAN = torch.nn.MSELoss()<br/>criterion_cycle = torch.nn.L1Loss()<br/>criterion_identity = torch.nn.L1Loss()<br/><br/>cuda = torch.cuda.is_available()<br/>input_shape = (opt.channels, opt.img_height, opt.img_width)<br/><br/><span style="color: #75715e;line-height: 26px;"># Initialize generator and discriminator</span><br/>G_AB = GeneratorResNet(input_shape, opt.n_residual_blocks)<br/>G_BA = GeneratorResNet(input_shape, opt.n_residual_blocks)<br/>D_A = Discriminator(input_shape)<br/>D_B = Discriminator(input_shape)<br/></code></pre><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;"><strong>优化器和训练策略</strong></span></section><pre data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;border-radius: 5px;box-shadow: rgba(0, 0, 0, 0.55) 0px 2px 10px;"><code style="overflow-x: auto;padding: 16px;color: #ddd;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;font-size: 12px;-webkit-overflow-scrolling: touch;padding-top: 15px;background: #272822;border-radius: 5px;"><span style="color: #75715e;line-height: 26px;"># Optimizers</span><br/>optimizer_G = torch.optim.Adam(<br/>    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2)<br/>)<br/>optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))<br/>optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))<br/><br/><span style="color: #75715e;line-height: 26px;"># Learning rate update schedulers</span><br/>lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(<br/>    optimizer_G, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step<br/>)<br/>lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(<br/>    optimizer_D_A, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step<br/>)<br/>lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(<br/>    optimizer_D_B, lr_lambda=LambdaLR(opt.n_epochs, opt.epoch, opt.decay_epoch).step<br/>)<br/></code></pre><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;"><strong>训练迭代</strong></span></section><ul class="list-paddingleft-2" data-tool="mdnice编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;"><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">训练数据是成对的数据，但是是非配对的数据，即A和B是没有直接的联系的。A是原图，B是风格图</span></section></li><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">生成器训练</span></section></li><ul class="list-paddingleft-2" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;color: black;list-style-type: square;"><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">GAN loss：判别器判别A和B生成的两个图fake_A、fake_B与GT的loss</span></section></li><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">Cycle loss：反过来fake_A和fake_B 生成的图与A和B像素上差异</span></section></li></ul><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">判别器训练：</span></section></li><ul class="list-paddingleft-2" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;color: black;list-style-type: square;"><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">loss_real: 判别A/B和GT的MSELoss</span></section></li><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">loss_fake:判别生成的fake_A/fake_B与GT的MSELoss</span></section></li></ul></ul><pre data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;border-radius: 5px;box-shadow: rgba(0, 0, 0, 0.55) 0px 2px 10px;"><code style="overflow-x: auto;padding: 16px;color: #ddd;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;font-size: 12px;-webkit-overflow-scrolling: touch;padding-top: 15px;background: #272822;border-radius: 5px;"><span style="color: #f92672;font-weight: bold;line-height: 26px;">for</span> epoch <span style="color: #f92672;font-weight: bold;line-height: 26px;">in</span> range(opt.epoch, opt.n_epochs):<br/>    <span style="color: #f92672;font-weight: bold;line-height: 26px;">for</span> i, batch <span style="color: #f92672;font-weight: bold;line-height: 26px;">in</span> enumerate(dataloader):<br/><br/>        <span style="color: #75715e;line-height: 26px;"># 数据是成对的数据，但是是非配对的数据，即A和B是没有直接的联系的</span><br/>        real_A = Variable(batch[<span style="color: #a6e22e;line-height: 26px;">"A"</span>].type(Tensor))<br/>        real_B = Variable(batch[<span style="color: #a6e22e;line-height: 26px;">"B"</span>].type(Tensor))<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Adversarial ground truths</span><br/>        valid = Variable(Tensor(np.ones((real_A.size(<span style="line-height: 26px;">0</span>), *D_A.output_shape))), requires_grad=<span style="color: #f92672;font-weight: bold;line-height: 26px;">False</span>)<br/>        fake = Variable(Tensor(np.zeros((real_A.size(<span style="line-height: 26px;">0</span>), *D_A.output_shape))), requires_grad=<span style="color: #f92672;font-weight: bold;line-height: 26px;">False</span>)<br/><br/>        <span style="color: #75715e;line-height: 26px;"># ------------------</span><br/>        <span style="color: #75715e;line-height: 26px;">#  Train Generators</span><br/>        <span style="color: #75715e;line-height: 26px;"># ------------------</span><br/><br/>        G_AB.train()<br/>        G_BA.train()<br/><br/>        optimizer_G.zero_grad()<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Identity loss</span><br/>        loss_id_A = criterion_identity(G_BA(real_A), real_A)<br/>        loss_id_B = criterion_identity(G_AB(real_B), real_B)<br/><br/>        loss_identity = (loss_id_A + loss_id_B) / <span style="line-height: 26px;">2</span><br/><br/>        <span style="color: #75715e;line-height: 26px;"># GAN loss</span><br/>        fake_B = G_AB(real_A)<br/>        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)<br/>        fake_A = G_BA(real_B)<br/>        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)<br/><br/>        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / <span style="line-height: 26px;">2</span><br/><br/>        <span style="color: #75715e;line-height: 26px;"># Cycle loss</span><br/>        recov_A = G_BA(fake_B)<br/>        loss_cycle_A = criterion_cycle(recov_A, real_A)<br/>        recov_B = G_AB(fake_A)<br/>        loss_cycle_B = criterion_cycle(recov_B, real_B)<br/><br/>        loss_cycle = (loss_cycle_A + loss_cycle_B) / <span style="line-height: 26px;">2</span><br/><br/>        <span style="color: #75715e;line-height: 26px;"># Total loss</span><br/>        loss_G = loss_GAN + opt.lambda_cyc * loss_cycle + opt.lambda_id * loss_identity<br/><br/>        loss_G.backward()<br/>        optimizer_G.step()<br/><br/>        <span style="color: #75715e;line-height: 26px;"># -----------------------</span><br/>        <span style="color: #75715e;line-height: 26px;">#  Train Discriminator A</span><br/>        <span style="color: #75715e;line-height: 26px;"># -----------------------</span><br/><br/>        optimizer_D_A.zero_grad()<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Real loss</span><br/>        loss_real = criterion_GAN(D_A(real_A), valid)<br/>        <span style="color: #75715e;line-height: 26px;"># Fake loss (on batch of previously generated samples)</span><br/>        <span style="color: #75715e;line-height: 26px;"># fake_A_ = fake_A_buffer.push_and_pop(fake_A)</span><br/>        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)<br/>        <span style="color: #75715e;line-height: 26px;"># Total loss</span><br/>        loss_D_A = (loss_real + loss_fake) / <span style="line-height: 26px;">2</span><br/><br/>        loss_D_A.backward()<br/>        optimizer_D_A.step()<br/><br/>        <span style="color: #75715e;line-height: 26px;"># -----------------------</span><br/>        <span style="color: #75715e;line-height: 26px;">#  Train Discriminator B</span><br/>        <span style="color: #75715e;line-height: 26px;"># -----------------------</span><br/><br/>        optimizer_D_B.zero_grad()<br/><br/>        <span style="color: #75715e;line-height: 26px;"># Real loss</span><br/>        loss_real = criterion_GAN(D_B(real_B), valid)<br/>        <span style="color: #75715e;line-height: 26px;"># Fake loss (on batch of previously generated samples)</span><br/>        <span style="color: #75715e;line-height: 26px;"># fake_B_ = fake_B_buffer.push_and_pop(fake_B)</span><br/>        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)<br/>        <span style="color: #75715e;line-height: 26px;"># Total loss</span><br/>        loss_D_B = (loss_real + loss_fake) / <span style="line-height: 26px;">2</span><br/><br/>        loss_D_B.backward()<br/>        optimizer_D_B.step()<br/><br/>        loss_D = (loss_D_A + loss_D_B) / <span style="line-height: 26px;">2</span><br/><br/>        <span style="color: #75715e;line-height: 26px;"># --------------</span><br/>        <span style="color: #75715e;line-height: 26px;">#  Log Progress</span><br/>        <span style="color: #75715e;line-height: 26px;"># --------------</span><br/><br/>        <span style="color: #75715e;line-height: 26px;"># Determine approximate time left</span><br/>        batches_done = epoch * len(dataloader) + i<br/>        batches_left = opt.n_epochs * len(dataloader) - batches_done<br/>        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))<br/>        prev_time = time.time()<br/><br/>    <span style="color: #75715e;line-height: 26px;"># Update learning rates</span><br/>    lr_scheduler_G.step()<br/>    lr_scheduler_D_A.step()<br/>    lr_scheduler_D_B.step()</code></pre><h4 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 18px;line-height: 1.75em;"><span style="letter-spacing: 2px;">2.2.4 结果展示</span></h4><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">本文训练的是莫奈风格的转变，如下图：第一二行是莫奈风格画转换为普通照片，第三四行为普通照片转换为莫奈风格画</span></section><p style="text-align: center;"><img class="rich_pages" data-ratio="0.8079930495221547" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bTwdj1WIF81nIhZI9xa2KucYS1Sdz1UibYHFAHVxXkWE3nZauheAOJPw/640?wx_fmt=png" data-type="png" data-w="1151" src="20240525_024210_10.jpeg" style=""/></p><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">再来看实际手机拍摄图片：</span></section><p style="text-align: center;"><img class="rich_pages" data-ratio="0.6256800870511425" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bGVmJKz2CvqK6lQxZdFNwH0Mbfu8X5ibCbaphr4NXGnnFeHCGRw5cwJA/640?wx_fmt=png" data-type="png" data-w="919" src="20240525_024211_11.jpeg" style=""/></p><h4 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 18px;line-height: 1.75em;"><span style="letter-spacing: 2px;">2.2.5 cycleGAN其他用途</span></h4><p style="text-align: center;"><img class="rich_pages" data-ratio="0.4643478260869565" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bKCQ5k95kUdgrdPwEfYaUCxFBqjTKSkLYyPHaQWtUQ3o06FGBicAdnyA/640?wx_fmt=png" data-type="png" data-w="1150" src="20240525_024212_12.jpeg" style=""/></p><p style="text-align: center;"><img class="rich_pages" data-ratio="0.6948853615520282" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/5mt0ewv9OS0rbz5bT6CT9SWia34vNFs1bPxJjpCMQl0l18NYc2iaV2S260sRK9pgPNvLAS7yiaJfibPdxicQcSjMpRQ/640?wx_fmt=png" data-type="png" data-w="1134" src="20240525_024214_13.jpeg" style=""/></p><h2 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 22px;line-height: 1.75em;"><span style="letter-spacing: 2px;">3. 总结</span></h2><section style="padding-top: 8px;padding-bottom: 8px;line-height: 1.75em;"><span style="letter-spacing: 2px;">本文详细介绍了GAN的其中一种应用cycleGAN，并将它应用到图像风格的转换。总结如下：</span></section><ul class="list-paddingleft-2" data-tool="mdnice编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;"><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">GAN是学习数据中分布，并生成同样分布但全新的数据</span></section></li><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">CycleGAN是两路GAN：两个生成器和两个判别器；为了保证生成器的生成的图片与输入图存在一定的关系，不是随机生产的图片， 引入cycle consistency，判定A-&gt;fake_B-&gt;recove_A和A的差异</span></section></li><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">生成器：下采样 + residual 残差block + 上采样</span></section></li><li style="letter-spacing: 2px;"><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.75em;"><span style="letter-spacing: 2px;">判别器: 不是一个图生成一个判定值，而是patchGAN方式，生成很N*N个值，而后取均值</span></section></li></ul></section><section class="mp_profile_iframe_wrp"><mpprofile class="js_uneditable custom_select_card mp_profile_iframe" data-alias="cryptocn" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/pHPnY2tVqxExbTgl5ovsgJthfXq6TRMsyS4x5yj3wOEIhvCDsnUZO3J08tn9yjezqMvZVmib4jIfBIoAbQJYXZA/0?wx_fmt=png" data-id="MzI1MjcwNTMxNQ==" data-nickname="程序员摸鱼指南" data-pluginname="mpprofile" data-signature="摸鱼哲学深度践行者"></mpprofile></section><p><br/></p><p data-darkmode-bgcolor="rgb(36, 36, 36)" data-darkmode-bgcolor-16031153228585="rgb(25, 25, 25)" data-darkmode-original-bgcolor="rgb(255, 255, 255)" data-darkmode-original-bgcolor-16031153228585="rgb(255, 255, 255)" data-style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; letter-spacing: 0.544px; white-space: normal; background-color: rgb(255, 255, 255);' style='letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);font-size: 16px;text-align: left;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;color: rgba(230, 230, 230, 0.9) !important;'><span data-darkmode-bgcolor="rgb(36, 36, 36)" data-darkmode-bgcolor-16031153228585="rgb(25, 25, 25)" data-darkmode-color="rgb(123, 127, 131)" data-darkmode-color-16031153228585="rgb(123, 127, 131)" data-darkmode-original-bgcolor="rgb(255, 255, 255)" data-darkmode-original-bgcolor-16031153228585="rgb(255, 255, 255)" data-darkmode-original-color="rgb(123, 127, 131)" data-darkmode-original-color-16031153228585="rgb(123, 127, 131)" style='color: rgb(123, 127, 131);font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;font-size: 14px;letter-spacing: 1.5px;'>作者简介：wedo实验君, 数据分析师；热爱生活，热爱写作</span></p><p data-darkmode-bgcolor="rgb(36, 36, 36)" data-darkmode-bgcolor-16031153228585="rgb(25, 25, 25)" data-darkmode-original-bgcolor="rgb(255, 255, 255)" data-darkmode-original-bgcolor-16031153228585="rgb(255, 255, 255)" data-style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; letter-spacing: 0.544px; white-space: normal; background-color: rgb(255, 255, 255);' style='letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);font-size: 16px;text-align: left;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;color: rgba(230, 230, 230, 0.9) !important;'><br data-darkmode-bgcolor="rgb(36, 36, 36)" data-darkmode-bgcolor-16031153228585="rgb(25, 25, 25)" data-darkmode-original-bgcolor="rgb(255, 255, 255)" data-darkmode-original-bgcolor-16031153228585="rgb(255, 255, 255)"/></p><p data-darkmode-bgcolor="rgb(36, 36, 36)" data-darkmode-bgcolor-16031153228585="rgb(25, 25, 25)" data-darkmode-color="rgb(230, 230, 230)" data-darkmode-color-16031153228585="rgb(163, 163, 163)" data-darkmode-original-bgcolor="rgb(255, 255, 255)" data-darkmode-original-bgcolor-16031153228585="rgb(255, 255, 255)" data-darkmode-original-color="rgb(0, 0, 0)" data-darkmode-original-color-16031153228585="rgb(0, 0, 0)" data-style='letter-spacing: 0.544px; white-space: normal; background-color: rgb(255, 255, 255); color: rgb(0, 0, 0); font-size: 16px; caret-color: rgb(51, 51, 51); text-size-adjust: auto; font-family: -apple-system-font, system-ui, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; text-align: center;' style='letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);color: rgb(0, 0, 0);font-size: 16px;font-family: -apple-system-font, system-ui, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;caret-color: rgb(51, 51, 51);text-size-adjust: auto;text-align: center;'><span data-darkmode-bgcolor="rgb(36, 36, 36)" data-darkmode-bgcolor-16031153228585="rgb(25, 25, 25)" data-darkmode-color="rgb(121, 123, 170)" data-darkmode-color-16031153228585="rgb(121, 123, 170)" data-darkmode-original-bgcolor="rgb(255, 255, 255)" data-darkmode-original-bgcolor-16031153228585="rgb(255, 255, 255)" data-darkmode-original-color="rgb(121, 123, 170)" data-darkmode-original-color-16031153228585="rgb(121, 123, 170)" style="color: rgb(121, 123, 170);"><strong data-darkmode-bgcolor="rgb(36, 36, 36)" data-darkmode-bgcolor-16031153228585="rgb(25, 25, 25)" data-darkmode-color="rgb(121, 123, 170)" data-darkmode-color-16031153228585="rgb(121, 123, 170)" data-darkmode-original-bgcolor="rgb(255, 255, 255)" data-darkmode-original-bgcolor-16031153228585="rgb(255, 255, 255)" data-darkmode-original-color="rgb(121, 123, 170)" data-darkmode-original-color-16031153228585="rgb(121, 123, 170)"><span data-darkmode-bgcolor="rgb(36, 36, 36)" data-darkmode-bgcolor-16031153228585="rgb(25, 25, 25)" data-darkmode-color="rgb(121, 123, 170)" data-darkmode-color-16031153228585="rgb(121, 123, 170)" data-darkmode-original-bgcolor="rgb(255, 255, 255)" data-darkmode-original-bgcolor-16031153228585="rgb(255, 255, 255)" data-darkmode-original-color="rgb(121, 123, 170)" data-darkmode-original-color-16031153228585="rgb(121, 123, 170)" style="font-size: 15px;">赞 赏 作 者</span></strong></span></p><p style='letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);color: rgb(0, 0, 0);font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;font-size: 16px;text-align: center;'><img class="rich_pages" data-ratio="1" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/5mt0ewv9OS0YHyPsrsxIklBia7MxiadpnIypaCf8Qtx4hfQBb9CULb3Bjr1Xic1LLteoZERMoLwcNJribXBl2GFgyA/640?wx_fmt=jpeg" data-type="jpeg" data-w="1037" src="20240525_024215_14.jpeg" style="width: 40%;box-sizing: border-box !important;visibility: visible !important;height: auto !important;"/></p><p><br/></p><section data-mpa-template="t" style='color: rgb(0, 0, 0);font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;font-size: 16px;text-align: left;white-space: normal;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);'><section data-tools-id="73294"><section style="margin-right: 10px;margin-left: 10px;display: flex;flex-direction: column;"><section style="margin-top: 10px;margin-bottom: 10px;padding: 10px;align-self: flex-start;border-radius: 5px;border-width: 1px;border-style: solid;border-color: rgb(0, 0, 0);"><p style="font-size: 15px;letter-spacing: 5.5px;">更多阅读</p></section><section style="width: 637px;border-top: 1px dashed rgb(0, 0, 0);"><br/></section><section><section style="margin-top: 15px;margin-bottom: 15px;display: flex;align-items: center;"><section style="margin-right: 10px;width: 10px;height: 10px;background-color: rgb(0, 0, 0);border-radius: 50%;"><br/></section><section><p style="font-size: 13px;letter-spacing: 5.5px;"><a data-itemshowtype="0" data-linktype="2" href="#" tab="innerlink" target="_blank">谷歌 AI 团队用 GAN 模型合成异形生物体</a><br/></p></section></section><section style="margin-top: 15px;margin-bottom: 15px;display: flex;align-items: center;"><section style="margin-right: 10px;width: 10px;height: 10px;background-color: rgb(0, 0, 0);border-radius: 50%;"><br/></section><section><p style="font-size: 13px;letter-spacing: 5.5px;"><a data-itemshowtype="11" data-linktype="2" href="#" tab="innerlink" target="_blank">英伟达研究出用较少数据集训练GAN的方法</a><br/></p></section></section><section style="margin-top: 15px;margin-bottom: 15px;display: flex;align-items: center;"><section style="margin-right: 10px;width: 10px;height: 10px;background-color: rgb(0, 0, 0);border-radius: 50%;"><br/></section><section><p style="font-size: 13px;letter-spacing: 5.5px;"><a data-itemshowtype="0" data-linktype="2" href="#" tab="innerlink" target="_blank">Python 中图像标题生成的注意力机制实战</a><br/></p></section></section></section></section></section></section><section data-mpa-template="t" style='color: rgb(0, 0, 0);font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;font-size: 16px;text-align: left;white-space: normal;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);'><section data-tools-id="73294"><section style="margin-right: 10px;margin-left: 10px;display: flex;flex-direction: column;"><section style="margin-top: 10px;margin-bottom: 10px;padding: 10px;align-self: flex-start;border-radius: 5px;border-width: 1px;border-style: solid;border-color: rgb(0, 0, 0);"><p style="font-size: 15px;letter-spacing: 5.5px;">特别推荐<br/></p></section><section style="width: 637px;border-top: 1px dashed rgb(0, 0, 0);"><br/></section></section></section></section><p style='color: rgb(0, 0, 0);font-size: 16px;white-space: normal;font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);text-align: center;'><img class="rich_pages" data-ratio="0.5555555555555556" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/5mt0ewv9OS2rWppRHh7JRLOaC7hd3FqLicga4eHksWR5ibDI3KGIYs6lvmTqy6qTID4ozckViamnTyaOkpgU6OUYg/640?wx_fmt=jpeg" data-type="jpeg" data-w="900" src="20240525_024216_15.jpeg" style="box-sizing: border-box !important;visibility: visible !important;width: 677px !important;"/></p><p style='color: rgb(0, 0, 0);font-size: 16px;white-space: normal;letter-spacing: 0.544px;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;background-color: rgb(255, 255, 255);text-align: center;'><br/></p><p style='color: rgb(0, 0, 0);font-size: 16px;white-space: normal;letter-spacing: 0.544px;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;background-color: rgb(255, 255, 255);text-align: center;'><br/></p><p style='color: rgb(0, 0, 0);font-size: 16px;white-space: normal;letter-spacing: 0.544px;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;background-color: rgb(255, 255, 255);text-align: center;'><span style="color: rgb(0, 82, 255);"><strong style='color: rgb(0, 0, 0);letter-spacing: 0.544px;font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;'><span style="color: rgb(0, 82, 255);font-family: PingFangSC-Light;letter-spacing: 3px;word-spacing: 1.5px;font-size: 14px;">点击下方阅读原文加入</span></strong><span style="text-align: left;font-family: PingFangSC-Light;letter-spacing: 3px;word-spacing: 1.5px;font-size: 14px;"><strong>社区会员</strong></span></span></p></div>

</div>
                <p></p>
                <p></p>
                <div>本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 </div>
                <div  class="hidden">本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 <br><p class="hidden">code/s?__biz=MzAxMjUyNDQ5OA==&mid=2653572121&idx=1&sn=6f3ff845fb727d2ac8bb5336cd814d21&chksm=806e62a4b719ebb2855dfbc50618268b86636a3c895bbfd4e37ec42a26df7130af05bdd86f59#rd </p></div>
            </body>
            </html>
            