


            
Google Astra ：未来的人工智能助手
          




“一个对日常生活有帮助的通用人工智能代理”    视频来自官方 demo 推荐：该演示展示了两次连续拍摄：一次是在 Google Pixel 手机上运行原型，另一次是在原型眼镜设备上运行。没有字幕原因是谷歌黑板报吃了原创，不能换上来。各位耐心观看效果就好。    Astra 正在为人工智能聊天机器人的未来铺平道路。从 OpenAI 和 Google 的发布中可以清楚地看出，人工智能正在从聊天机器人转向个人人工智能助理。即使在其发展的初期阶段，其令人惊叹的功能也令人惊叹。    Astra 真实地展示了将迄今为止仅在科幻电影中出现的概念变为现实的可能性。到目前为止看到的 Astra 演示都是实时工作的，其视觉功能远远超出了所见过的。吹逼是这样的，差点被 GPT-4o 把股价打崩。    在介绍Astra案例中，我们来看下李飞飞大神是怎么看待空间交互AGI。视频很长全程白话科普值得各位看官学习，将空间智能与现实生活知识联系起来，不愧是 AI 先驱者。Astra的特殊性    为了真正发挥作用，智能体需要像人类一样理解和响应复杂且动态的世界，并吸收并记住所看到和听到的内容，以了解上下文并采取行动。它还需要主动、可教和个性化，以便用户可以自然地与它交谈，没有滞后或延迟。这点小编我可能觉得不服，各位看官也能看看 GPT-4o 做做对比。（解密 ChatGPT-4o —— 下一代AI功能及其变革性影响）将视频和语音输入合并到事件时间线中连续编码视频帧缓存信息以实现高效调用    虽然在开发能够理解多模式信息的人工智能系统方面取得了令人难以置信的进展，但将响应时间缩短为对话式的内容是一项艰巨的工程挑战。在过去的几年里，一直致力于改进模型的感知、推理和对话方式，以使交互的速度和质量感觉更加自然。通过利用领先的语音模型（还行吧），还增强了人工智能代理的发音，为它们提供了更广泛的语调。这些代理可以更好地理解他们所使用的上下文，并在对话中快速响应。简单说就是推销自己的大模型 Gemini。    Project Astra 是一种新型多模式人工智能代理，能够通过文本、视频、图像和语音向其提供实时问题，并提取相关信息。它通过智能手机摄像头从网络以及您周围的世界中提取信息。将视频帧和语音编码到时间轴中，然后将其缓存以供调用。Project Astra 有能力记住它所看到的不再出现在画面中的事物。这是通过连续编码视频帧并将视频和语音输入组合到事件时间线中来实现的，从而可以有效地回忆过去的信息。    简而言之，它通过智能手机摄像头观察您周围的世界，感知、响应并记住它，就像我们一样。即使之前看到的物体不再出现在相机画面中，它也会这样做。作为智能Agent，充当你的助理    Project Astra 的目的是像一个助手一样在现实世界中为你提供指导。它可以通过识别物体、面孔、情绪和纺织品来回答有关您周围环境的问题。它甚至可以帮助您记住上次放置物品的位置。    当相机对准另一位记者 Pixel 8 Pro 时，Project Astra 可以识别出拍摄对象是一个人——我们明确告诉它这个人被识别为一名男性。然后，它正确识别出他携带了手机。在后续问题中，小组询问了他的衣服。它给出了一个笼统的答案：“他似乎穿着休闲服。”然后，我们问他在做什么，Project Astra 回答说，似乎戴着一副太阳镜（他确实戴上了），摆出了一个随意的姿势。另外小编在下面分别介绍几个case：通过草图解释赛车零件通过引导进行环境回溯，识别物体通过草图连续推理和找到相应文献    关于空间物体识别，小编觉得还是要抄一下 Meta 。不吹牛别人家的能力做元宇宙是真强（SceneScript —— 一种 3D 场景重建的新方法）GPT-4o 与 Project Astra比较多模态能力    在比较Google Project Astra和GPT-4o 的多模式功能时，明显的优势和劣势就会显现出来。Astra擅长利用智能手机摄像头了解用户环境，而GPT-4o 则专注于基于音频、视觉和文本输入的实时交互。    从本质上讲，Google Project Astra因其强调通过智能手机摄像头的视觉感知而脱颖而出。它旨在通过处理实时视频帧来提供对用户周围世界的全面了解。另一方面，GPT-4o优先考虑跨音频和文本输入等各种模式的即时响应，展示其在动态交互中的敏捷性。    Google Project Astra 的亮点在于它能够通过连续视频处理创建详细的事件时间表。此功能增强了上下文理解，但可能会给处理快速数据流带来挑战。相反，GPT-4o擅长实时交互和快速响应，但可能缺乏Astra提供的视觉理解深度。实时互动    每个人工智能处理实时数据的方式使它们显著不同。Google Project Astra专注于无缝捕获和解释视觉信息，而GPT-4o 则擅长同时处理不同的输入以获取即时反馈。Google Project Astra依靠智能手机摄像头持续捕捉视觉数据，使其能够根据现实世界的观察提供与上下文相关的响应。相比之下，GPT-4o可以快速处理音频提示和文本输入，确保用户查询的快速周转时间。    Google Project Astra强调通过实时视频处理进行情境理解，开创了沉浸式用户体验的先例。通过利用智能手机摄像头来解读周围环境，它为增强现实环境中增强交互铺平了道路。这种方法不仅增强了用户参与度，而且推动了计算机视觉应用的进步。另一方面，GPT-4o对实时多模态交互的关注重新定义了对话式 AI 范式。通过平等对待文本、音频和视觉输入，它促进了跨不同模式的无缝通信。这种包容性方法不仅简化了用户交互，还强调了模仿人类认知过程的综合人工智能系统的潜力。实际应用    实际应用因其核心功能而异。Google Project Astra的优势在于通过沉浸式视觉交互增强用户体验，使其成为需要详细环境分析的场景的理想选择。另一方面，GPT-4o的实时处理能力使其适合即时反馈至关重要的动态对话设置。    两者都是利用手机能力进行交互，其实最靠谱也是最能打破现阶段空间交互规则就是穿戴设备。各位可以看看这个（AI 和 全息技术为普通眼镜带来 3D 增强现实），直接眼镜上芯片。或者另一个被投资的穿戴设备（人工智能交互的下一个前沿 —— 大型动作模型 (LAM)）。这几个小编觉得都是未来交互场景。总结：    小编感觉未来将不再是通过点击与手机进行交互，而是更多地依赖于这种空间智能引导，完美的胜任你的数字伴侣。类似《钢铁侠》中的贾维斯吗？虚拟助理可以做任何事情，从发送托尼·斯塔克提醒到为佩珀·波茨女士的生日送花。或者也许是《星际穿越》中的 TARS ，可以做一切事情，从驾驶 “坚忍号” 到从他们访问过的许多世界收集数据。




