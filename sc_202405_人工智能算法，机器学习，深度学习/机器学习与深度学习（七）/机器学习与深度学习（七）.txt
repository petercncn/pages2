


            
机器学习与深度学习（七）
          




点击蓝字2024 2.4FEBRUARY关注我们MLP，全称为Multilayer Perceptron，译为多层感知机。其是一种强大的神经网络，具有多层隐藏层。它通过反向传播算法进行训练，不断调整权重以最小化输出误差。MLP在许多领域都有广泛应用，如图像识别、语音识别和自然语言处理等。由于其出色的性能和易用性，MLP已成为机器学习领域的重要工具。本期内容，源动随风为您带来MLP的相关知识。01神经元与感知机壹神经元一个神经元就是一个神经细胞，它是神经系统的基本组成单位。根据目前的认识，一个典型的神经元由以下几部分组成：(1)细胞体(cell body)，是神经细胞的主体，内有细胞核和细胞质，除实现细胞生存的各种基本功能外，这里是神经细胞进行信息加工的主要场所；(2)树突(dendrites),是细胞体外围的大量微小分支，是细胞的"触角"，一个神经元的树突可达103数量级，多数长度很短，主要担负从外界接收信息的功能；(3)轴突(axon)，是细胞的输出装置，负责把信号传递给另外的神经细胞；(4)突触(synapse)，是一个神经元的轴突与另一个神经元的轴突之间相连接的部位。图1 典型神经元构成示意图贰感知机基于生物神经元模型可得到感知机的基本结构，即输入、权重、求和、激活函数以及最后的输出。图2展示了一个上述内容。需要指出的是，图2中的Weights(权重)需要通过感知机学习算法来进行确定，该算法在后文中会提到。图2 感知机基本结构那么对于最后的输出值，可以由以下公式来确定：叁感知机学习算法图3 感知机学习算法从图3可以看到，在感知机学习算法中，首先定义迭代次数为T0，x和y的取值范围为{-1,1}，输出为θ，即感知机基本结构中的权重参数。在具体算法中，需要特别强调的是，更新参数时应遵循平行四边形法则。肆感知机学习算法的证明图4 感知机学习算法证明基本理论图5 感知机学习算法证明图5中的M代表更新次数，可以看出，更新次数同样本条数无关。伍感知机能够表达的函数利用感知机，人们可以表达一些简单的函数，例如布尔函数(Bool Function)。（a）(b)(c)图6 (a)用感知机表示与函数  (b)用感知机表示或函数(c)用感知机表示非函数但是，感知机无法表示复杂的布尔函数(如亦或逻辑函数)，这就使得多层感知机，即是MLP出现在了人们的视野之中。02多层感知机表达逻辑关系壹MLP简介图7 简单的多层感知机从图7的多层感知机我们可以发现，其包含输入层(Input)、隐藏层(Hidden Layer)以及输出层(Output)。同时，需要指出的是，该网络结构包含两层，初学者易将其误认为三层。并且，第一层包含d1个神经元，第二次包含2个神经元。贰用MLP表示复杂逻辑函数图8 用MLP表示亦或逻辑函数使用MLP表示亦或逻辑函数(XOR)，可使用图8中左边的网络来表达，其对应的逻辑表达式为红框所示。图9 使用MLP表示更复杂的逻辑函数第一层的4个感知机表达了黄框中对应布尔表达式小括号中的逻辑关系，第二层表达了稍大括号中的逻辑关系，第三层则表达了最后的与逻辑关系(&)。可以证明，任意一个布尔表达式都可以由一个多层感知机表达。此即，感知机的通用逼近定理。叁用MLP表达实函数图10 使用感知机表达实函数一个拥有实数输入以及实数权重的感知机可以表达一个超平面。图11 使用MLP表示一个五边形使用五个感知机，组成多层感知机，可表示一个五边形。图12 使用MLP表示组合图形在图11的基础上，最后两个五边形使用或(OR)逻辑关系，即可得到组合图形。03MLP初识壹MLP基本网络展示图13 MLP基本网络结构可以看到，图13包含了两个部分，一个是网络结构，另一个是损失函数(Loss Function)。下面分别进行讲解。首先来看这个网络结构，该网络结构由有3层神经元组成，同前文相同，深度学习中的层数是指每经过一次变化就多一层。在整个过程中，进行m次采样，输入参数为x，输出参数为y。L1为输入层，L2和L3组成了隐藏层，L4为输出层。最终得到的y1、y2以及y3代表隶属于某一类的概率。对于损失函数(Loss Fuction)，J中包含两部分，fθ(x(i))代表感知机输出的预测值，y(i)代表真实标签，每一项二者作差，在m个样本上加起来再取平均就得到了最后的J(θ)。贰激活函数(Activation Functions)图14 sigmoid函数图像人们最初将sigmoid函数作为MLP训练时的激活函数，但是后来发现其存在梯度消失现象，即当x较小和较大时图像的梯度均为0，故人们对其进行了改进，如图15所示。图15 各类激活函数在这三种激活函数中，只有Rectified Linear Unit(ReLU)函数解决了梯度消失问题，即其当x<0时梯度均为0，当x>0时梯度均为1，但仍存在一个不可导点，即x=0，这就引出了图16所示的GELU函数。图16 GELU函数GELU函数目前大量被使用在大语言模型中，如GPT-3、BERT以及其他基于Transformer开发的大语言模型中。需要指出的是，以上提到的激活函数都是被应用于MLP的隐藏层中。叁Softmax函数在激活函数部分中，提到的激活函数均是使用在隐藏层中的。而针对输出层的激活函数，人们通常使用Softmax函数来完成。图17 Softmax函数简介可以看到，图17中红框所示即为Softmax函数的表达式，其将属于实数集R的z映射为属于[0,1]的概率分布y。其两个特点分别是：(1)赢者通吃；(2)数值最大者决定最终的输出值。但是在实际使用中，容易出现数值溢出的情况(numerical overflow)，于是人们分子分母同时除以一个函数，如图17黄框所示。图18具体解析了Softmax函数表达式，读者可仔细阅读。肆目标函数我们可以看到，在输出层中使用的Softmax函数可以由下式表达：下面给出MLP常用的交叉熵损失函数(Cross-Entropy Loss)的表达式及其代入Softmax函数后的结果。03Softmax逻辑回归图18 Softmax Regression从图18我们可以看到，对于输入的一个猫的图像，通过神经网络提取特征后，利用MLP对隶属于每一类的可能性进行打分。可以看到，对隶属于horse(马)的打分为负值，于是我们对其取对数，再进行归一化，就可以得到隶属于每一类的概率。图19 从统计视角来看Softmax逻辑回归图19可能需要一定的数学知识，其中probability mass function可表示一个多伯努利分布，且qij代表第i个样本类别是j的概率。应当指出的是，1{yi=j|qij}是一个示性函数，其函数值只取0或1，具体用法读者可自行查阅相关资料。图20 最大似然估计最终我们会得到如图20所示的最大似然估计函数，可有效刻画离散问题中随机事件发生的概率。04反向传播算法首先，我们进行一些基本的定义，如图21所示。简而言之，就是用上标表示层号，用下标表示神经元的序号。图21 BP算法在MLP中实现的基本定义壹前向传播图22 前向传播由于最初θ是未知的，故可先假设一个θ。前向传播可计算出所有的x、y、z以及a贰反向传播图23 反向传播反向传播是链式规则的有效实现方式，总体来说，其使用了动态规划算法(dynamic programming)。同时，避免了一些子表达式的重复计算，从而提高了运算效率。BP算法整体上是用空间换时间。叁残差的计算首先，需要特别指出的是，在深度学习中，有两处会涉及到残差，但两处残差的含义完全不同。其中一处为反向传播中的残差，另一处则是残差网络（ICCV 2023,Best Paper）。对于第l个隐藏层中第i个神经元，可通过下式来计算其残差，即目标函数对其进行求导：而对于最后的输出层，根据链式法则，我们可以得到：针对隐藏层，可由图24来完成：图24 隐藏层的残差计算在图24的推导过程中，需要指出的是，使用链式法则以及：最终，可以得到以下推导过程：接着，我们来计算梯度：图25 梯度的计算最后，完成梯度更新：图26 梯度更新05自动求导机制在反向传播算法中，需要计算诸多梯度，这就容易导致错误的产生。我们先来看一个例子：图27 BP算法示例(a)图28 BP算法示例(b)从图28的a和b两张图片可以看出，手动求导在计算时十分复杂且容易出错。于是，人们便引入了自动求导机制(Automatic Differentiation,AutoDIff)。图29 自动求导机制示例在图29中，黑色部分代表前向传播，红色部分代表反向传播。易得，自动求导机制创造了一个计算图，用于计算前向传播和反向传播过程中的梯度。图30 标准反向传播和自动求导机制的对比自动求导机制通过引入数据结构中子节点和父节点的知识，建立梯度计算过程中的拓扑结构，从而大大提高了梯度计算的效率和正确率。06总结与展望本期内容详细讲述了MLP(多层感知机)的知识，诚然其已有几十年的历史，但其思想以及对深度学习发展所作出的贡献仍然不可忽视。希望各位读者能够仔细阅读相关知识，也欢迎各位同源动随风在公众号后台交流。下期将带来卷积神经网络的介绍，敬请期待~图文编辑：源动随风图文来源：源动随风




