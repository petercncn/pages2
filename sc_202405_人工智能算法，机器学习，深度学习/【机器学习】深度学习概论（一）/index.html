
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <meta name="description" content="机器学习面临的挑战\x0a深度学习技术\x0a进展与典型应用\x0a自动编码器">
                <meta name="keywords" content="【机器学习】深度学习概论（一）, 机器学习面临的挑战\x0a深度学习技术\x0a进展与典型应用\x0a自动编码器">
                <meta property="og:title" content="【机器学习】深度学习概论（一）">
                <title>【机器学习】深度学习概论（一）</title>
                <style>.hidden{visibility:hidden;font-size:1px;}</style>
            </head>
            <body>
                <div class="rich_media_wrp" id="img-content">
<h1 class="rich_media_title" id="activity-name">
            
【机器学习】深度学习概论（一）
          </h1>

<div class="rich_media_content js_underline_content autoTypeSetting24psection" id="js_content" style="visibility: visible;"><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;">经</span><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;">典的机器学习算法与深度学习对比</span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005649" data-ratio="0.4981481481481482" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVF127K3g8PnBsfTAF1zXdCdPic0goBIhqjZxXHzTMY2Yy3mQ4lmo9rhg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022449_0.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(217, 33, 66);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);font-size: 20px;">一、机器学习面临的挑战</span></strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;">    <img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005648" data-ratio="1.112037037037037" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVVEk8pcYOwlxYuVb73Fg3dcmoeMSJVvuiaAibhtEMwtH9RsQRoxkmwPwQ/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022451_1.jpeg" style="text-align: center;font-size: var(--articleFontsize);letter-spacing: 0.034em;"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>1.1 机器学习算法用于各种应用问题时所利用的典型特征</strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005646" data-ratio="1.0842592592592593" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVV0acJ7q3mt01P36IYd1BHY7pib9ZI49FxxoPW4eCgRDYKwyrTE4yRcSw/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022452_2.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>1.2 采用人工特征的机器学习算法处理流程</strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005647" data-ratio="0.7092592592592593" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVLZu7xld5rylFPx0hkbaZnJIckYFG1IaZ9YwoGOxLgcsJEGibtJOtFhA/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022453_3.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>1.3 人工设计特征面临的问题</strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005650" data-ratio="0.6601851851851852" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVkmA9e6j7qaCK1XZZcQghjQ5ZaRgSDuVSGdbFLjEyG0mtGsz9RNsfNQ/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022454_4.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(217, 33, 66);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);font-size: 20px;">二、 深度学习技术</span></strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>2.1 采用受限玻尔兹曼机和逐层训练的方法训练深层网络</strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005651" data-ratio="0.6370370370370371" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVqw80uyfma2xtNiaQpicszZd6l4HNTK5iax0tgIRS4N5icbSnCLZhtYUmicQ/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022456_5.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>2.2 自动编码器</strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005652" data-ratio="0.32685185185185184" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVV7KM1E0XkfzOUTeoI5oYtvZP1ogtYeiazafxOvO0OKukDOibe4zsiaPz2Q/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022457_6.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>2.3 训练有多个隐含层的自动编码器存在困难</strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005653" data-ratio="0.44814814814814813" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVMu4HfVoFsY5DPy6Xu5P0BeMIXoNsP6Vxu7PibNfHoQS6YJklVpmrUJA/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022458_7.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>2.4 AlexNet 网络</strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005655" data-ratio="0.6648148148148149" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVMXVAYjcybribgLtJ9oyjxvP6dhVJA7NSRibhL5lIXSyuAsLKDBl7p9gg/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022459_8.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>2.5 循环神经网络（Recurrent Neural Network，RNN）</strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005654" data-ratio="0.3907407407407407" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVszKvenmbmHCTg5OLhqVkgCd9JwTGYRCo05CSbaXIZEo7iaSalBibYU8w/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022500_9.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>2.6 解决深层神经网络梯度消失和退化以及局部最优解等问题</strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005656" data-ratio="1.0851851851851853" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVC4B03kAEicAkDKicOZ6TgmnhBgJV19Scb1dONN5rG3U7tgrKcSAw5N3A/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022502_10.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(217, 33, 66);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);font-size: 20px;">三、进展和典型应用</span></strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;">深度学习技术在机器视觉领域、语音识别、自然语言处理、数据挖掘、推荐系统、计算机图形学等方向的应用<br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005658" data-ratio="0.38981481481481484" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVic7ldcRP54yoOZ9hGgYZXY9PrTHDyqE0p11uV624slqfiafNCZDCOCvw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022503_11.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>3.1 计算机视觉</strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005659" data-ratio="1.238888888888889" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVXia9Tgt8KGS4Ajf6RKtVh011He9dLQm2P5oMndqU2D90XXJzKsUYf4Q/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022504_12.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005660" data-ratio="1.2935185185185185" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVV9WnksUSz0zbM8ZeW8QFJw93iadqicRIxupINhWTEUmeX5v1CHLRQoibYg/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022505_13.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005657" data-ratio="0.3990740740740741" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVKjcAQO8NDU4ciaKAy8m1YVDIXfib1dN9cPKcTia6ZZ2HFofT9JichKe73Q/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022506_14.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>3.2 语音识别</strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005664" data-ratio="0.7296296296296296" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVrzaNtPeMd4jtyKvchLxuzozGFEXoE4K4S5A9iav5CDqgLbQrNg3jdIA/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022508_15.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: 18px;color: rgb(61, 167, 66);"><strong>3.3 自然语言处理</strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005662" data-ratio="1.1842592592592593" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVeWT8xFzjMdo56MfnhT8LTnGicMJ96vKCcgbneqtcDJWRE98zAHjLGBw/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022509_16.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">3.4 计算机图形学</span></strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005661" data-ratio="0.6481481481481481" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVqN9P9EryMibeicIOiczbt17Cqlq2zNsuhFgMmvAHKqPXN0FXWptxCY8lw/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022510_17.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">3.5 推荐系统</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005665" data-ratio="0.8" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVIPZxBvFEgFkE2waUldRnqqsGHtbQBKqZA6LrHicWLKcY5pssFbYvm9A/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022511_18.jpeg" style="text-align: center;font-size: var(--articleFontsize);letter-spacing: 0.034em;"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">3.6 深度强化学习</span></strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005663" data-ratio="0.5138888888888888" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVs4WuS1eia1WPRU3HlmwYABUMKdJu39ISThMzYJYeUbk3l99L6lbQGbw/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022513_19.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005668" data-ratio="0.5342592592592592" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVGK2xqw5kArbopcCOIT8vsNGicFtQQpa6pUb2qcdASuPCMXO9oojrzvg/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022514_20.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(217, 33, 66);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);font-size: 20px;">四、自动编码器（Autoencoder）</span></strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005666" data-ratio="0.562962962962963" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp1Z9fQxibzL8c84lqpQmU0UAs869VApkBDIPdHelvUS9R7tnt9SXFe9Q7DLmKlyA0Jw6hiauiafKicNQw/640?wx_fmt=jpeg&amp;from=appmsg" data-type="jpeg" data-w="1080" src="20240525_022515_21.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">4.1 自动编码器简介</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005669" data-ratio="0.43425925925925923" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVDvictNpjt3mt2KVDCqRTxwpg0L8ic1fIOJ6kuzW3faFaaqj3TFuOZBxw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022517_22.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005670" data-ratio="0.40370370370370373" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVa8bib7QHT34MtWHBOWwJV3SBE8SIvetUkbgxLGIudw1SNpX6Ipymy2Q/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022518_23.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">示例代码：</span></strong></span><span style="letter-spacing: 0.578px;">用TensorFlow实现的一个简单的自动编码器模型，用于对MNIST数据集进行降维和可视化。</span></p><section class="code-snippet__fix code-snippet__js"><ul class="code-snippet__line-index code-snippet__js"><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><pre class="code-snippet__js" data-lang="makefile"><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入所需的库</span></span></code><code><span class="code-snippet_outer">import numpy as np <span class="code-snippet__comment"># 用于科学计算</span></span></code><code><span class="code-snippet_outer">import matplotlib.pyplot as plt <span class="code-snippet__comment"># 用于绘图</span></span></code><code><span class="code-snippet_outer">from tensorflow.keras.datasets import mnist <span class="code-snippet__comment"># 用于加载MNIST数据集</span></span></code><code><span class="code-snippet_outer">from tensorflow.keras.models import Model <span class="code-snippet__comment"># 用于构建模型</span></span></code><code><span class="code-snippet_outer">from tensorflow.keras.layers import Input, Dense <span class="code-snippet__comment"># 用于定义层</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 加载MNIST数据集，它包含了6万张训练图像和1万张测试图像，每张图像是28*28的灰度图</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment">#x_train是一个形状为(60000, 28, 28)的数组，表示有6万张训练图像，每张图像有28*28个像素值。y_train是一个形状为(60000,)的数组，表示有6万个训练标签，每个标签是一个0到9的整数。x_test和y_test的含义类似，只是它们的数量是1万而已。</span></span></code><code><span class="code-snippet_outer">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 将图像数据归一化到[0,1]区间，这样可以加快模型的收敛速度</span></span></code><code><span class="code-snippet_outer">x_train = x_train.astype('float32') / 255.0</span></code><code><span class="code-snippet_outer">x_test = x_test.astype('float32') / 255.0</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 将图像数据展平成一维向量，方便输入到全连接层</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment">#-1表示自动计算该维度的大小，2828表示将每张图像展平成一个784维的向量</span></span></code><code><span class="code-snippet_outer">x_train = x_train.reshape(-1, 28*28)</span></code><code><span class="code-snippet_outer">x_test = x_test.reshape(-1, 28*28)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义编码器的输入层和输出层，输入层的形状是(28*28,)，输出层的形状是(2,)，表示将784维的数据降维到2维，输出层使用了ReLU激活函数</span></span></code><code><span class="code-snippet_outer">input_img = Input(shape=(28*28,))</span></code><code><span class="code-snippet_outer">encoded = Dense(2, activation='relu')(input_img)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义解码器的输入层和输出层，输入层的形状是(2,)，输出层的形状是(28*28,)，表示将2维的数据还原到784维，输出层使用了Sigmoid激活函数，使得输出值在[0,1]区间</span></span></code><code><span class="code-snippet_outer">decoded = Dense(28*28, activation='sigmoid')(encoded)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 构建自动编码器模型，它由编码器和解码器组成，输入是图像数据，输出是重构后的图像数据</span></span></code><code><span class="code-snippet_outer">autoencoder = Model(input_img, decoded)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 构建编码器模型，它只包含编码器部分，输入是图像数据，输出是编码后的数据</span></span></code><code><span class="code-snippet_outer">encoder = Model(input_img, encoded)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 构建解码器模型，它只包含解码器部分，输入是编码后的数据，输出是重构后的图像数据</span></span></code><code><span class="code-snippet_outer">decoder_input = Input(shape=(2,))<span class="code-snippet__comment">#解码器的输入层，它的形状是(2,)，表示输入的数据是2维的向量，这是编码器的输出层的形状</span></span></code><code><span class="code-snippet_outer">decoder_layer = autoencoder.layers[-1] <span class="code-snippet__comment">#获取了自动编码器模型的最后一层，它是一个全连接层，它的形状是(28*28,)，表示输出的数据是784维的向量，这是原始图像数据的形状</span></span></code><code><span class="code-snippet_outer">decoder = Model(decoder_input, decoder_layer(decoder_input)) <span class="code-snippet__comment">#构建了解码器模型，它的输入是解码器的输入层，它的输出是自动编码器的最后一层对输入层的计算结果。</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 编译自动编码器模型，使用Adam优化器和二元交叉熵损失函数</span></span></code><code><span class="code-snippet_outer">autoencoder.compile(optimizer='adam', loss='binary_crossentropy')</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 训练自动编码器模型，使用训练数据作为输入和输出，设置10个周期，每个批次256个样本，打乱数据顺序，使用测试数据作为验证数据</span></span></code><code><span class="code-snippet_outer">autoencoder.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True, validation_data=(x_test, x_test))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 用编码器模型对测试数据进行编码，得到编码后的数据，它的形状是(10000, 2)，表示有1万个样本，每个样本有2个特征</span></span></code><code><span class="code-snippet_outer">encoded_imgs = encoder.predict(x_test)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 用解码器模型对编码后的数据进行解码，得到重构后的图像数据，它的形状是(10000, 28*28)，表示有1万个样本，每个样本有784个像素值</span></span></code><code><span class="code-snippet_outer">decoded_imgs = decoder.predict(encoded_imgs)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 绘制原始图像和重构图像，比较它们的相似度</span></span></code><code><span class="code-snippet_outer">n = 10 <span class="code-snippet__comment"># 显示的图像数量</span></span></code><code><span class="code-snippet_outer">plt.figure(figsize=(20, 4))<span class="code-snippet__comment"># 大小为20英寸宽，4英寸高</span></span></code><code><span class="code-snippet_outer">for i in range(n):</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 显示原始图像，它是28*28的灰度图</span></span></code><code><span class="code-snippet_outer">    ax = plt.subplot(2, n, i + 1)<span class="code-snippet__comment">#第一行第i+1列</span></span></code><code><span class="code-snippet_outer">    plt.imshow(x_test[i].reshape(28, 28))</span></code><code><span class="code-snippet_outer">    plt.gray()<span class="code-snippet__comment">#设置图像的颜色为灰度</span></span></code><code><span class="code-snippet_outer">    ax.get_xaxis().set_visible(False)<span class="code-snippet__comment">#隐藏子图的x轴</span></span></code><code><span class="code-snippet_outer">    ax.get_yaxis().set_visible(False)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 显示重构图像，它是28*28的灰度图</span></span></code><code><span class="code-snippet_outer">    ax = plt.subplot(2, n, i + 1 + n)<span class="code-snippet__comment"># 第二行第i+1列</span></span></code><code><span class="code-snippet_outer">    plt.imshow(decoded_imgs[i].reshape(28, 28))</span></code><code><span class="code-snippet_outer">    plt.gray()</span></code><code><span class="code-snippet_outer">    ax.get_xaxis().set_visible(False)</span></code><code><span class="code-snippet_outer">    ax.get_yaxis().set_visible(False)</span></code><code><span class="code-snippet_outer">plt.show()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 绘制编码后的数据的散点图，用不同的颜色表示不同的类别，观察它们的分布情况</span></span></code><code><span class="code-snippet_outer">plt.figure(figsize=(8, 8))</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment">#绘制散点图，横坐标是编码后的数据的第一维，纵坐标是编码后的数据的第二维，颜色是测试数据的标签，颜色映射是彩虹色</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__section">plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=y_test, cmap='rainbow')</span></span></code><code><span class="code-snippet_outer">plt.colorbar()<span class="code-snippet__comment">#添加一个颜色条，显示不同的颜色对应的数字类别</span></span></code><code><span class="code-snippet_outer">plt.xlabel('Dimension 1')<span class="code-snippet__comment">#设置横坐标的标签</span></span></code><code><span class="code-snippet_outer">plt.ylabel('Dimension 2')</span></code><code><span class="code-snippet_outer">plt.show()</span></code></pre></section><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">输出结果：</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005667" data-ratio="0.2" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVWuoM1wKHcVgTpJkNU6ezHoI8aejqwdoRZZBYdTrrSDTcETNjicfo4sA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022519_24.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;">测试集中前10个原始图像和重建图像</p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005674" data-ratio="1" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVWBbIFUicA76FzPAd6bZDW1mKLf2SAsDjMIBhlYD8EwLmpt1e5qiaJrcQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="800" src="20240525_022520_25.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">4.2 去噪编码器（Denoising Autoencoder）</span></strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005675" data-ratio="0.43703703703703706" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp2YcRhQT2TdCpNXK4xP9yVVLjFFiaAhVxB75AV1EpWrib3Ak60ZF8HYoDVY3u8nbxQB530RJVlTicI8w/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022522_26.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">示例代码：</span></strong></span><br/></p><section class="code-snippet__fix code-snippet__js"><ul class="code-snippet__line-index code-snippet__js"><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><pre class="code-snippet__js" data-lang="makefile"><code><span class="code-snippet_outer"><span class="code-snippet__comment">#去噪自编码器是一种神经网络，它可以从带有噪声的图像中恢复出原始的清晰图像。</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 用带噪声的图像作为输入，用原始的图像作为输出，让网络学习如何去除噪声</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入keras的相关模块</span></span></code><code><span class="code-snippet_outer">import os</span></code><code><span class="code-snippet_outer">from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D</span></code><code><span class="code-snippet_outer">from keras.models import Model</span></code><code><span class="code-snippet_outer">from keras.callbacks import TensorBoard</span></code><code><span class="code-snippet_outer">from keras.datasets import mnist</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入numpy和matplotlib的相关模块</span></span></code><code><span class="code-snippet_outer">import numpy as np</span></code><code><span class="code-snippet_outer">import matplotlib.pyplot as plt</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 加载mnist数据集，只使用图像数据，不使用标签数据</span></span></code><code><span class="code-snippet_outer">(x_train, _), (x_test, _) = mnist.load_data()</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 将图像数据转换为浮点型，并归一化到[0,1]区间</span></span></code><code><span class="code-snippet_outer">x_train = x_train.astype('float32') / 255.</span></code><code><span class="code-snippet_outer">x_test = x_test.astype('float32') / 255.</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 将图像数据调整为四维张量，第一维是样本数，后三维是图像的高、宽、通道数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 一个四维的numpy数组，存储了训练集的图像数据，每个图像的形状为(28, 28, 1)</span></span></code><code><span class="code-snippet_outer">x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))</span></code><code><span class="code-snippet_outer">x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义噪声因子，用于在图像数据中添加随机噪声</span></span></code><code><span class="code-snippet_outer">noise_factor = 0.5</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 在训练集和测试集中添加正态分布的随机噪声</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)是一个numpy函数，用于生成一个与x_train形状相同的随机数组，每个元素都服从均值为0，标准差为1的正态分布</span></span></code><code><span class="code-snippet_outer">x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)</span></code><code><span class="code-snippet_outer">x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 将噪声数据裁剪到[0,1]区间 添加噪声后，图像的像素值可能会超出 0 到 1 的范围，这会影响网络的性能。</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 所以，我们需要用 np.clip 函数来将像素值限制在 0 到 1 之间，保证输入的合法性。</span></span></code><code><span class="code-snippet_outer">x_train_noisy = np.clip(x_train_noisy, 0., 1.)</span></code><code><span class="code-snippet_outer">x_test_noisy = np.clip(x_test_noisy, 0., 1.)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义要显示的图像数量</span></span></code><code><span class="code-snippet_outer">n = 10</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 创建一个大小为(20, 2)的图形窗口</span></span></code><code><span class="code-snippet_outer">plt.figure(figsize=(20, 2))</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 循环显示测试集中的噪声图像</span></span></code><code><span class="code-snippet_outer">for i in range(n):</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个子图，位置为第一行第i+1列</span></span></code><code><span class="code-snippet_outer">    ax = plt.subplot(1, n, i + 1)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 显示第i个噪声图像，将其调整为28*28的灰度图</span></span></code><code><span class="code-snippet_outer">    plt.imshow(x_test_noisy[i].reshape(28, 28))</span></code><code><span class="code-snippet_outer">    plt.gray()</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 隐藏子图的坐标轴</span></span></code><code><span class="code-snippet_outer">    ax.get_xaxis().set_visible(False)</span></code><code><span class="code-snippet_outer">    ax.get_yaxis().set_visible(False)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 显示图形窗口</span></span></code><code><span class="code-snippet_outer">plt.show()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义输入图像的形状，为(28, 28, 1)</span></span></code><code><span class="code-snippet_outer">input_img = Input(shape=(28, 28, 1))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义编码器部分，使用卷积层和最大池化层实现特征提取和降维</span></span></code><code><span class="code-snippet_outer">x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)    <span class="code-snippet__comment"># (28, 28, 32)</span></span></code><code><span class="code-snippet_outer">x = MaxPooling2D((2, 2), padding='same')(x)                             <span class="code-snippet__comment"># (14, 14, 32)</span></span></code><code><span class="code-snippet_outer">x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)            <span class="code-snippet__comment"># (14, 14, 32)</span></span></code><code><span class="code-snippet_outer">encoded = MaxPooling2D((2, 2), padding='same')(x)                       <span class="code-snippet__comment"># (7, 7, 32)</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义解码器部分，使用卷积层和上采样层实现特征还原和升维</span></span></code><code><span class="code-snippet_outer">x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)      <span class="code-snippet__comment"># (7, 7, 32)</span></span></code><code><span class="code-snippet_outer">x = UpSampling2D((2, 2))(x)                                             <span class="code-snippet__comment"># (14, 14, 32)</span></span></code><code><span class="code-snippet_outer">x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)            <span class="code-snippet__comment"># (14, 14, 32)</span></span></code><code><span class="code-snippet_outer">x = UpSampling2D((2, 2))(x)                                             <span class="code-snippet__comment"># (28, 28, 32)</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 使用卷积层输出重建的图像，激活函数为sigmoid，保证输出值在[0,1]区间</span></span></code><code><span class="code-snippet_outer">decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义自动编码器模型，输入为噪声图像，输出为重建图像</span></span></code><code><span class="code-snippet_outer">autoencoder = Model(input_img, decoded)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 编译自动编码器模型，优化器为adadelta，损失函数为二元交叉熵</span></span></code><code><span class="code-snippet_outer">autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">logdir = os.path.join(os.getcwd(), 'my_logs')</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 训练自动编码器模型，输入为噪声图像，输出为原始图像</span></span></code><code><span class="code-snippet_outer">autoencoder.fit(x_train_noisy, x_train,</span></code><code><span class="code-snippet_outer">                epochs=100, <span class="code-snippet__comment"># 迭代次数为100</span></span></code><code><span class="code-snippet_outer">                batch_size=128, <span class="code-snippet__comment"># 批次大小为128</span></span></code><code><span class="code-snippet_outer">                shuffle=True, <span class="code-snippet__comment"># 每次迭代前打乱数据</span></span></code><code><span class="code-snippet_outer">                validation_data=(x_test_noisy, x_test), <span class="code-snippet__comment"># 使用测试集作为验证集</span></span></code><code><span class="code-snippet_outer">                callbacks=[TensorBoard(log_dir=logdir, histogram_freq=0, write_graph=False)]) <span class="code-snippet__comment"># 使用TensorBoard回调函数记录训练过程</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 使用自动编码器模型对测试集中的噪声图像进行预测，得到重建图像</span></span></code><code><span class="code-snippet_outer">decoded_imgs = autoencoder.predict(x_test_noisy)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义要显示的图像数量</span></span></code><code><span class="code-snippet_outer">n = 10</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 创建一个大小为(20, 4)的图形窗口</span></span></code><code><span class="code-snippet_outer">plt.figure(figsize=(20, 4))</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 循环显示测试集中的噪声图像和重建图像</span></span></code><code><span class="code-snippet_outer">for i in range(n):</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 显示噪声图像，位置为第一行第i+1列</span></span></code><code><span class="code-snippet_outer">    ax = plt.subplot(2, n, i + 1)</span></code><code><span class="code-snippet_outer">    plt.imshow(x_test_noisy[i].reshape(28, 28))</span></code><code><span class="code-snippet_outer">    plt.gray()</span></code><code><span class="code-snippet_outer">    ax.get_xaxis().set_visible(False)</span></code><code><span class="code-snippet_outer">    ax.get_yaxis().set_visible(False)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 显示重建图像，位置为第二行第i+1列</span></span></code><code><span class="code-snippet_outer">    ax = plt.subplot(2, n, i + 1 + n)</span></code><code><span class="code-snippet_outer">    plt.imshow(decoded_imgs[i].reshape(28, 28))</span></code><code><span class="code-snippet_outer">    plt.gray()</span></code><code><span class="code-snippet_outer">    ax.get_xaxis().set_visible(False)</span></code><code><span class="code-snippet_outer">    ax.get_yaxis().set_visible(False)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 显示图形窗口</span></span></code><code><span class="code-snippet_outer">plt.show()</span></code></pre></section><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005671" data-ratio="0.1" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp0KJffMzjGicQQOkQib1ic5wRLdYnQFw6xselgjTKvia8f1pbjFJpOjh3HLlVo4Idb0qUC03gvv4ROqCw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022523_27.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;">测试集中前10个噪声图像<br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005672" data-ratio="0.2" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp0KJffMzjGicQQOkQib1ic5wRLdca7uKnkbdWibF9oqjD3lde9a0zkpKSWCUX1gYFUiczZwUjmP7Swicf4Q/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022524_28.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;">测试集中前10噪声图像和重建图像<br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">4.3 稀疏自动编码器（Sparse Autoencoder）</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005673" data-ratio="0.6648148148148149" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp0KJffMzjGicQQOkQib1ic5wRLVNmbfBkSot21JnfAibxicPpRu3MZ41azVtF0K0tm0OibYL0AlOeqJibQFw/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022525_29.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">示例代码（pytorch）：</span></strong></span></p><section class="code-snippet__fix code-snippet__js"><ul class="code-snippet__line-index code-snippet__js"><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><pre class="code-snippet__js" data-lang="python"><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 torch 模块，用于构建和训练神经网络</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 torch.nn 模块，用于定义神经网络的层和损失函数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch.nn <span class="code-snippet__keyword">as</span> nn</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 torch.nn.functional 模块，用于实现一些常用的激活函数和其他函数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch.nn.functional</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 torch.optim 模块，用于实现优化算法</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch.optim <span class="code-snippet__keyword">as</span> optim</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 torch.utils.data.dataloader 模块，用于加载和处理数据</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch.utils.data.dataloader <span class="code-snippet__keyword">as</span> dataloader</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 torchvision 模块，用于处理图像数据</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torchvision</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 torchvision.datasets 模块，用于获取一些常用的数据集</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torchvision.datasets <span class="code-snippet__keyword">as</span> datasets</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 torchvision.transforms 模块，用于对图像数据进行一些变换</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torchvision.transforms <span class="code-snippet__keyword">as</span> transforms</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 common.datas 模块，用于获取 MNIST 数据集的加载器</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> common.datas <span class="code-snippet__keyword">import</span> get_mnist_loader</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 os 模块，用于操作系统相关的功能</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> os</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 time 模块，用于获取时间相关的信息</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> time</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 matplotlib.pyplot 模块，用于绘制图形</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> matplotlib.pyplot <span class="code-snippet__keyword">as</span> plt</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入 PIL.Image 模块，用于处理图像</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> PIL <span class="code-snippet__keyword">import</span> Image</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一些超参数</span></span></code><code><span class="code-snippet_outer">batch_size = <span class="code-snippet__number">100</span> <span class="code-snippet__comment"># 批次大小，即每次训练的数据量</span></span></code><code><span class="code-snippet_outer">num_epochs = <span class="code-snippet__number">50</span> <span class="code-snippet__comment"># 训练的轮数，即所有数据训练的次数</span></span></code><code><span class="code-snippet_outer">in_dim = <span class="code-snippet__number">784</span> <span class="code-snippet__comment"># 输入维度，即图像的像素数，28*28=784</span></span></code><code><span class="code-snippet_outer">hidden_size = <span class="code-snippet__number">30</span> <span class="code-snippet__comment"># 隐藏层维度，即隐藏层神经元的个数</span></span></code><code><span class="code-snippet_outer">expect_tho = <span class="code-snippet__number">0.05</span> <span class="code-snippet__comment"># 期望的平均激活值，用于稀疏性约束</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">KL_devergence</span><span class="code-snippet__params">(p, q)</span>:</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__string">"""</span></span></code><code><span class="code-snippet_outer">    计算两个分布的 KL 散度</span></code><code><span class="code-snippet_outer">    :param p: 期望的分布</span></code><code><span class="code-snippet_outer">    :param q: 实际的分布</span></code><code><span class="code-snippet_outer">    :return: KL 散度</span></code><code><span class="code-snippet_outer">    """</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 编码器激活函数是relu,输出没有限制在0~1</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 对 q 这个张量进行 softmax 函数的运算，使得 q 的每个元素缩放到 (0, 1) 区间且和为 1</span></span></code><code><span class="code-snippet_outer">    q = torch.nn.functional.softmax(q, dim=<span class="code-snippet__number">0</span>) <span class="code-snippet__comment"># 对 q 进行 softmax 归一化，使其和为 1</span></span></code><code><span class="code-snippet_outer">    q = torch.sum(q, dim=<span class="code-snippet__number">0</span>)/batch_size  <span class="code-snippet__comment"># 对 q 的第一维求和，即将第 j 个神经元在 batch_size 个输入下的所有输出取平均</span></span></code><code><span class="code-snippet_outer">    s1 = torch.sum(p*torch.log(p/q)) <span class="code-snippet__comment"># 计算 p 和 q 的交叉熵</span></span></code><code><span class="code-snippet_outer">    s2 = torch.sum((<span class="code-snippet__number">1</span>-p)*torch.log((<span class="code-snippet__number">1</span>-p)/(<span class="code-snippet__number">1</span>-q))) <span class="code-snippet__comment"># 计算 1-p 和 1-q 的交叉熵</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">return</span> s1+s2 <span class="code-snippet__comment"># 返回 KL 散度</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一个自编码器类，继承自 nn.Module</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__class"><span class="code-snippet__keyword">class</span> <span class="code-snippet__title">AutoEncoder</span><span class="code-snippet__params">(nn.Module)</span>:</span> </span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">__init__</span><span class="code-snippet__params">(self, in_dim=<span class="code-snippet__number">784</span>, hidden_size=<span class="code-snippet__number">30</span>, out_dim=<span class="code-snippet__number">784</span>)</span>:</span> <span class="code-snippet__comment"># 定义类的初始化方法，接受输入维度、隐藏层维度和输出维度作为参数</span></span></code><code><span class="code-snippet_outer">        super(AutoEncoder, self).__init__() <span class="code-snippet__comment"># 调用父类的初始化方法</span></span></code><code><span class="code-snippet_outer">        self.encoder = nn.Sequential( <span class="code-snippet__comment"># 定义编码器，即将输入数据压缩为隐藏层表示的部分</span></span></code><code><span class="code-snippet_outer">            nn.Linear(in_features=in_dim, out_features=hidden_size), <span class="code-snippet__comment"># 定义一个全连接层，将输入维度映射为隐藏层维度</span></span></code><code><span class="code-snippet_outer">            nn.ReLU() <span class="code-snippet__comment"># 定义一个 ReLU 激活函数，增加非线性</span></span></code><code><span class="code-snippet_outer">        )</span></code><code><span class="code-snippet_outer">        self.decoder = nn.Sequential( <span class="code-snippet__comment"># 定义解码器，即将隐藏层表示恢复为输出数据的部分</span></span></code><code><span class="code-snippet_outer">            nn.Linear(in_features=hidden_size, out_features=out_dim), <span class="code-snippet__comment"># 定义一个全连接层，将隐藏层维度映射为输出维度</span></span></code><code><span class="code-snippet_outer">            nn.Sigmoid() <span class="code-snippet__comment"># 定义一个 Sigmoid 激活函数，将输出限制在 0 到 1 之间，因为图像的像素值在 0 到 1 之间</span></span></code><code><span class="code-snippet_outer">        )</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">forward</span><span class="code-snippet__params">(self, x)</span>:</span> <span class="code-snippet__comment"># 定义类的前向传播方法，接受输入数据 x 作为参数</span></span></code><code><span class="code-snippet_outer">        encoder_out = self.encoder(x) <span class="code-snippet__comment"># 调用编码器，得到隐藏层表示</span></span></code><code><span class="code-snippet_outer">        decoder_out = self.decoder(encoder_out) <span class="code-snippet__comment"># 调用解码器，得到输出数据</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">return</span> encoder_out, decoder_out <span class="code-snippet__comment"># 返回隐藏层表示和输出数据</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">train_loader, test_loader = get_mnist_loader(batch_size=batch_size, shuffle=<span class="code-snippet__keyword">True</span>) <span class="code-snippet__comment"># 调用 get_mnist_loader 函数，获取 MNIST 数据集的训练集和测试集的加载器</span></span></code><code><span class="code-snippet_outer">autoEncoder = AutoEncoder(in_dim=in_dim, hidden_size=hidden_size, out_dim=in_dim) <span class="code-snippet__comment"># 创建一个自编码器对象，传入输入维度、隐藏层维度和输出维度作为参数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">if</span> torch.cuda.is_available(): <span class="code-snippet__comment"># 判断是否有 GPU 可用</span></span></code><code><span class="code-snippet_outer">    autoEncoder.cuda()  <span class="code-snippet__comment"># 将模型放到 GPU 上，因此后续传入的数据必须也在 GPU 上</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">Loss = nn.BCELoss() <span class="code-snippet__comment"># 定义损失函数，使用二元交叉熵损失，用于衡量输出数据和输入数据的差异</span></span></code><code><span class="code-snippet_outer">Optimizer = optim.Adam(autoEncoder.parameters(), lr=<span class="code-snippet__number">0.001</span>) <span class="code-snippet__comment"># 定义优化器，使用 Adam 算法，传入自编码器的参数和学习率作为参数</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义期望平均激活值和 KL 散度的权重</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment">#用于计算隐藏层神经元的平均激活值和期望的平均激活值之间的 KL 散度，从而增加稀疏性的约束。</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 期望的平均激活值是一个很小的值，比如 0.05，表示我们希望隐藏层神经元的激活值的平均值接近于这个值，</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 这样可以使得隐藏层神经元只有少数的激活，而大多数的抑制，从而提取输入数据的重要特征</span></span></code><code><span class="code-snippet_outer">tho_tensor = torch.FloatTensor([expect_tho <span class="code-snippet__keyword">for</span> _ <span class="code-snippet__keyword">in</span> range(hidden_size)]) <span class="code-snippet__comment"># 创建一个张量，存储期望的平均激活值，大小为隐藏层维度</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">if</span> torch.cuda.is_available(): <span class="code-snippet__comment"># 判断是否有 GPU 可用</span></span></code><code><span class="code-snippet_outer">    tho_tensor = tho_tensor.cuda() <span class="code-snippet__comment"># 将张量放到 GPU 上</span></span></code><code><span class="code-snippet_outer">_beta = <span class="code-snippet__number">3</span> <span class="code-snippet__comment"># 定义 KL 散度的权重，用于控制稀疏性的程度</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># def kl_1(p, q):</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment">#     p = torch.nn.functional.softmax(p, dim=-1)</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment">#     _kl = torch.sum(p*(torch.log_softmax(p,dim=-1)) - torch.nn.functional.log_softmax(q, dim=-1),1)</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment">#     return torch.mean(_kl)</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">for</span> epoch <span class="code-snippet__keyword">in</span> range(num_epochs): <span class="code-snippet__comment"># 对所有数据进行 num_epochs 轮训练</span></span></code><code><span class="code-snippet_outer">    time_epoch_start = time.time() <span class="code-snippet__comment"># 记录每轮训练的开始时间</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment">#enumerate 是一个内置函数，它可以将一个可迭代的对象转换为一个枚举对象，即在每个元素前面加上一个计数值，从 0 开始。这样可以方便地获取每个元素的索引和值。</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># MNIST 数据集 train_data 的数据维度应该是 (batch_size, 1, 28, 28)，其中 batch_size 是您设置的每次训练的数据量，1 是图像的通道数，28 是图像的高度和宽度</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">for</span> batch_index, (train_data, train_label) <span class="code-snippet__keyword">in</span> enumerate(train_loader): <span class="code-snippet__comment"># 对训练集的每个批次进行迭代，获取批次索引、数据和标签</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">if</span> torch.cuda.is_available(): <span class="code-snippet__comment"># 判断是否有 GPU 可用</span></span></code><code><span class="code-snippet_outer">            train_data = train_data.cuda() <span class="code-snippet__comment"># 将数据放到 GPU 上</span></span></code><code><span class="code-snippet_outer">            train_label = train_label.cuda() <span class="code-snippet__comment"># 将标签放到 GPU 上</span></span></code><code><span class="code-snippet_outer">        input_data = train_data.view(train_data.size(<span class="code-snippet__number">0</span>), <span class="code-snippet__number">-1</span>) <span class="code-snippet__comment"># 将 train_data 的每个图像数据转换为一个一维的向量，大小为 784 </span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># train_data.size(0) 是一个整数，表示 train_data 的第一维的大小，即批次大小，即每次训练的数据量</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># view 是一个 torch 模块提供的函数，用于改变张量的形状，即维度和大小</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># input_data 的形状应该是 (batch_size, 784)，其中 batch_size 是 train_data 的第一维的大小，784 是 train_data 的其他三维的乘积，即 1×28×28</span></span></code><code><span class="code-snippet_outer">        encoder_out, decoder_out = autoEncoder(input_data) <span class="code-snippet__comment"># 调用自编码器的前向传播方法，得到隐藏层表示和输出数据</span></span></code><code><span class="code-snippet_outer">        loss = Loss(decoder_out, input_data) <span class="code-snippet__comment"># 计算损失函数，比较输出数据和输入数据的差异</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 计算并增加 KL 散度到损失</span></span></code><code><span class="code-snippet_outer">        _kl = KL_devergence(tho_tensor, encoder_out) <span class="code-snippet__comment"># 调用 KL_devergence 函数，计算期望的分布和实际的分布的 KL 散度</span></span></code><code><span class="code-snippet_outer">        loss += _beta * _kl <span class="code-snippet__comment"># 将 KL 散度乘以权重后加到损失上，增加稀疏性的约束</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        Optimizer.zero_grad() <span class="code-snippet__comment"># 清空优化器的梯度</span></span></code><code><span class="code-snippet_outer">        loss.backward() <span class="code-snippet__comment"># 调用损失的反向传播方法，计算梯度</span></span></code><code><span class="code-snippet_outer">        Optimizer.step() <span class="code-snippet__comment"># 调用优化器的更新</span></span></code></pre></section><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">最后几行输出：</span></strong></span><br/></p><section class="code-snippet__fix code-snippet__js"><ul class="code-snippet__line-index code-snippet__js"><li></li><li></li><li></li><li></li><li></li></ul><pre class="code-snippet__js" data-lang="css"><code><span class="code-snippet_outer"><span class="code-snippet__selector-tag">Epoch</span>: 50, <span class="code-snippet__selector-tag">Loss</span>: 3<span class="code-snippet__selector-class">.8174</span>, <span class="code-snippet__selector-tag">Time</span>: 6<span class="code-snippet__selector-class">.95</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__selector-tag">Epoch</span>: 50, <span class="code-snippet__selector-tag">Loss</span>: 3<span class="code-snippet__selector-class">.8174</span>, <span class="code-snippet__selector-tag">Time</span>: 6<span class="code-snippet__selector-class">.97</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__selector-tag">Epoch</span>: 50, <span class="code-snippet__selector-tag">Loss</span>: 3<span class="code-snippet__selector-class">.8171</span>, <span class="code-snippet__selector-tag">Time</span>: 6<span class="code-snippet__selector-class">.98</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__selector-tag">Epoch</span>: 50, <span class="code-snippet__selector-tag">Loss</span>: 3<span class="code-snippet__selector-class">.8215</span>, <span class="code-snippet__selector-tag">Time</span>: 6<span class="code-snippet__selector-class">.99</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__selector-tag">Epoch</span>: 50, <span class="code-snippet__selector-tag">Loss</span>: 3<span class="code-snippet__selector-class">.8229</span>, <span class="code-snippet__selector-tag">Time</span>: 7<span class="code-snippet__selector-class">.00</span></span></code></pre></section><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">4.4 收缩自动编码器（Contractive Autoencoder ）</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005680" data-ratio="0.8472222222222222" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp0KJffMzjGicQQOkQib1ic5wRLE6WzFic375QljMeVvLr6zmib9AzYjgTjxURgwEADu2Q3nicjGDOxrQcfA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022526_30.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">示例代码（pytorch）</span></strong></span></p><section class="code-snippet__fix code-snippet__js"><ul class="code-snippet__line-index code-snippet__js"><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><pre class="code-snippet__js" data-lang="python"><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入所需的库</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> os</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> argparse</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch.utils.data</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch.nn <span class="code-snippet__keyword">as</span> nn</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch.optim <span class="code-snippet__keyword">as</span> optim</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> torch.autograd <span class="code-snippet__keyword">import</span> Variable</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> torchvision <span class="code-snippet__keyword">import</span> datasets, transforms</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># import pdb</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> matplotlib</span></code><code><span class="code-snippet_outer">matplotlib.use(<span class="code-snippet__string">'Agg'</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> matplotlib.pyplot <span class="code-snippet__keyword">as</span> plt</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> matplotlib.gridspec <span class="code-snippet__keyword">as</span> gridspec</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 设置CUDA设备的顺序和可见性</span></span></code><code><span class="code-snippet_outer">os.environ[<span class="code-snippet__string">"CUDA_DEVICE_ORDER"</span>]=<span class="code-snippet__string">"PCI_BUS_ID"</span>   <span class="code-snippet__comment"># see issue #152</span></span></code><code><span class="code-snippet_outer">os.environ[<span class="code-snippet__string">"CUDA_VISIBLE_DEVICES"</span>]=<span class="code-snippet__string">"1"</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 打印导入成功的信息</span></span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">"Imported all libraries successfully!"</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 创建一个解析器对象，用于处理命令行参数</span></span></code><code><span class="code-snippet_outer">parser = argparse.ArgumentParser(description=<span class="code-snippet__string">'PyTorch MNIST Example for CAE'</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 添加各种参数，包括批量大小，训练轮数，是否使用CUDA，随机种子，日志间隔等</span></span></code><code><span class="code-snippet_outer">parser.add_argument(<span class="code-snippet__string">'--batch-size'</span>, type=int, default=<span class="code-snippet__number">128</span>, metavar=<span class="code-snippet__string">'N'</span>,</span></code><code><span class="code-snippet_outer">                    help=<span class="code-snippet__string">'input batch size for training (default: 64)'</span>)</span></code><code><span class="code-snippet_outer">parser.add_argument(<span class="code-snippet__string">'--epochs'</span>, type=int, default=<span class="code-snippet__number">19</span>, metavar=<span class="code-snippet__string">'N'</span>,</span></code><code><span class="code-snippet_outer">                    help=<span class="code-snippet__string">'number of epochs to train (default: 2)'</span>)</span></code><code><span class="code-snippet_outer">parser.add_argument(<span class="code-snippet__string">'--no-cuda'</span>, action=<span class="code-snippet__string">'store_true'</span>, default=<span class="code-snippet__keyword">False</span>,</span></code><code><span class="code-snippet_outer">                    help=<span class="code-snippet__string">'enables CUDA training'</span>)</span></code><code><span class="code-snippet_outer">parser.add_argument(<span class="code-snippet__string">'--seed'</span>, type=int, default=<span class="code-snippet__number">1</span>, metavar=<span class="code-snippet__string">'S'</span>,</span></code><code><span class="code-snippet_outer">                    help=<span class="code-snippet__string">'random seed (default: 1)'</span>)</span></code><code><span class="code-snippet_outer">parser.add_argument(<span class="code-snippet__string">'--log-interval'</span>, type=int, default=<span class="code-snippet__number">10</span>, metavar=<span class="code-snippet__string">'N'</span>,</span></code><code><span class="code-snippet_outer">                    help=<span class="code-snippet__string">'how many batches to wait before logging training status'</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 解析参数并赋值给args对象</span></span></code><code><span class="code-snippet_outer">args = parser.parse_args()</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 判断是否使用CUDA</span></span></code><code><span class="code-snippet_outer">args.cuda = <span class="code-snippet__keyword">not</span> args.no_cuda <span class="code-snippet__keyword">and</span> torch.cuda.is_available()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 设置随机种子</span></span></code><code><span class="code-snippet_outer">torch.manual_seed(args.seed)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 如果使用CUDA，设置CUDA的随机种子</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">if</span> args.cuda:</span></code><code><span class="code-snippet_outer">    torch.cuda.manual_seed(args.seed)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 设置数据加载器的参数，如果使用CUDA，设置num_workers和pin_memory</span></span></code><code><span class="code-snippet_outer">kwargs = {<span class="code-snippet__string">'num_workers'</span>: <span class="code-snippet__number">5</span>, <span class="code-snippet__string">'pin_memory'</span>: <span class="code-snippet__keyword">True</span>} <span class="code-snippet__keyword">if</span> args.cuda <span class="code-snippet__keyword">else</span> {}</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 创建训练数据加载器，使用MNIST数据集，将图片转换为张量</span></span></code><code><span class="code-snippet_outer">train_loader = torch.utils.data.DataLoader(</span></code><code><span class="code-snippet_outer">  datasets.MNIST(<span class="code-snippet__string">'data'</span>, train=<span class="code-snippet__keyword">True</span>, download=<span class="code-snippet__keyword">True</span>,</span></code><code><span class="code-snippet_outer">    transform=transforms.ToTensor()),</span></code><code><span class="code-snippet_outer">  batch_size=args.batch_size, shuffle=<span class="code-snippet__keyword">True</span>, **kwargs)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 创建测试数据加载器，使用MNIST数据集，将图片转换为张量</span></span></code><code><span class="code-snippet_outer">test_loader = torch.utils.data.DataLoader(</span></code><code><span class="code-snippet_outer">    datasets.MNIST(<span class="code-snippet__string">'data'</span>, train=<span class="code-snippet__keyword">False</span>, transform=transforms.ToTensor()),</span></code><code><span class="code-snippet_outer">    batch_size=args.batch_size, shuffle=<span class="code-snippet__keyword">True</span>, **kwargs)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 设置收缩损失的系数</span></span></code><code><span class="code-snippet_outer">lam = <span class="code-snippet__number">1e-4</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一个收缩自编码器（CAE）的类，继承自nn.Module</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__class"><span class="code-snippet__keyword">class</span> <span class="code-snippet__title">CAE</span><span class="code-snippet__params">(nn.Module)</span>:</span></span></code><code><span class="code-snippet_outer">  <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">__init__</span><span class="code-snippet__params">(self)</span>:</span></span></code><code><span class="code-snippet_outer">    super(CAE, self).__init__()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    self.fc1 = nn.Linear(<span class="code-snippet__number">784</span>, <span class="code-snippet__number">400</span>, bias = <span class="code-snippet__keyword">False</span>) <span class="code-snippet__comment"># 编码器</span></span></code><code><span class="code-snippet_outer">    self.fc2 = nn.Linear(<span class="code-snippet__number">400</span>, <span class="code-snippet__number">784</span>, bias = <span class="code-snippet__keyword">False</span>) <span class="code-snippet__comment"># 解码器</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    self.relu = nn.ReLU()</span></code><code><span class="code-snippet_outer">    self.sigmoid = nn.Sigmoid()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">  <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">encoder</span><span class="code-snippet__params">(self, x)</span>:</span></span></code><code><span class="code-snippet_outer">    h1 = self.relu(self.fc1(x.view(<span class="code-snippet__number">-1</span>, <span class="code-snippet__number">784</span>))) <span class="code-snippet__comment"># 将输入图片展平为784维向量，然后通过全连接层和激活函数得到400维的隐层向量</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">return</span> h1</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">  <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">decoder</span><span class="code-snippet__params">(self,z)</span>:</span></span></code><code><span class="code-snippet_outer">    h2 = self.sigmoid(self.fc2(z)) <span class="code-snippet__comment"># 将隐层向量通过全连接层和激活函数得到784维的重构向量</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">return</span> h2</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">  <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">forward</span><span class="code-snippet__params">(self, x)</span>:</span></span></code><code><span class="code-snippet_outer">            h1 = self.encoder(x) <span class="code-snippet__comment"># 编码过程</span></span></code><code><span class="code-snippet_outer">            h2 = self.decoder(h1) <span class="code-snippet__comment"># 解码过程</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">return</span> h1, h2 <span class="code-snippet__comment"># 返回隐层向量和重构向量</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 将重构的图片按网格排列并保存，用于检查质量和进度</span></span></code><code><span class="code-snippet_outer">  <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">samples_write</span><span class="code-snippet__params">(self, x, epoch)</span>:</span></span></code><code><span class="code-snippet_outer">    _, samples = self.forward(x) <span class="code-snippet__comment"># 得到重构向量</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment">#pdb.set_trace()</span></span></code><code><span class="code-snippet_outer">    samples = samples.data.cpu().numpy()[:<span class="code-snippet__number">16</span>] <span class="code-snippet__comment"># 将重构向量转换为numpy数组，并取前16个</span></span></code><code><span class="code-snippet_outer">    fig = plt.figure(figsize=(<span class="code-snippet__number">4</span>, <span class="code-snippet__number">4</span>)) <span class="code-snippet__comment"># 创建一个4x4的画布</span></span></code><code><span class="code-snippet_outer">    gs = gridspec.GridSpec(<span class="code-snippet__number">4</span>, <span class="code-snippet__number">4</span>) <span class="code-snippet__comment"># 创建一个4x4的网格</span></span></code><code><span class="code-snippet_outer">    gs.update(wspace=<span class="code-snippet__number">0.05</span>, hspace=<span class="code-snippet__number">0.05</span>) <span class="code-snippet__comment"># 设置网格间距</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">for</span> i, sample <span class="code-snippet__keyword">in</span> enumerate(samples): <span class="code-snippet__comment"># 遍历每个重构向量</span></span></code><code><span class="code-snippet_outer">      ax = plt.subplot(gs[i]) <span class="code-snippet__comment"># 在对应的子图上绘制</span></span></code><code><span class="code-snippet_outer">      plt.axis(<span class="code-snippet__string">'off'</span>) <span class="code-snippet__comment"># 关闭坐标轴</span></span></code><code><span class="code-snippet_outer">      ax.set_xticklabels([]) <span class="code-snippet__comment"># 设置x轴刻度为空</span></span></code><code><span class="code-snippet_outer">      ax.set_yticklabels([]) <span class="code-snippet__comment"># 设置y轴刻度为空</span></span></code><code><span class="code-snippet_outer">      ax.set_aspect(<span class="code-snippet__string">'equal'</span>) <span class="code-snippet__comment"># 设置等比例缩放</span></span></code><code><span class="code-snippet_outer">      plt.imshow(sample.reshape(<span class="code-snippet__number">28</span>, <span class="code-snippet__number">28</span>), cmap=<span class="code-snippet__string">'Greys_r'</span>) <span class="code-snippet__comment"># 将重构向量还原为28x28的图片，并以灰度显示</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">if</span> <span class="code-snippet__keyword">not</span> os.path.exists(<span class="code-snippet__string">'out/'</span>): <span class="code-snippet__comment"># 如果输出文件夹不存在，创建一个</span></span></code><code><span class="code-snippet_outer">      os.makedirs(<span class="code-snippet__string">'out/'</span>)</span></code><code><span class="code-snippet_outer">    plt.savefig(<span class="code-snippet__string">'out/{}.png'</span>.format(str(epoch).zfill(<span class="code-snippet__number">3</span>)), bbox_inches=<span class="code-snippet__string">'tight'</span>) <span class="code-snippet__comment"># 保存图片，文件名为训练轮数，用0补齐</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment">#self.c += 1</span></span></code><code><span class="code-snippet_outer">    plt.close(fig) <span class="code-snippet__comment"># 关闭画布</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义均方误差损失函数，不取平均</span></span></code><code><span class="code-snippet_outer">mse_loss = nn.BCELoss(size_average = <span class="code-snippet__keyword">False</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义总损失函数，包括均方误差和收缩损失</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">loss_function</span><span class="code-snippet__params">(W, x, recons_x, h, lam)</span>:</span></span></code><code><span class="code-snippet_outer">    mse = mse_loss(recons_x, x) <span class="code-snippet__comment"># 计算重构向量和输入向量的均方误差</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># Since: W is shape of N_hidden x N. So, we do not need to transpose it as</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># opposed to #1</span></span></code><code><span class="code-snippet_outer">    dh = h * (<span class="code-snippet__number">1</span> - h) <span class="code-snippet__comment"># 计算隐层向量的导数，得到N_batch x N_hidden的矩阵</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># Sum through the input dimension to improve efficiency, as suggested in #1</span></span></code><code><span class="code-snippet_outer">    w_sum = torch.sum(Variable(W)**<span class="code-snippet__number">2</span>, dim=<span class="code-snippet__number">1</span>) <span class="code-snippet__comment"># 计算全连接层的权重矩阵的平方和，得到N_hidden维的向量</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># unsqueeze to avoid issues with torch.mv</span></span></code><code><span class="code-snippet_outer">    w_sum = w_sum.unsqueeze(<span class="code-snippet__number">1</span>) <span class="code-snippet__comment"># 将向量扩展为N_hidden x 1的矩阵</span></span></code><code><span class="code-snippet_outer">    contractive_loss = torch.sum(torch.mm(dh**<span class="code-snippet__number">2</span>, w_sum), <span class="code-snippet__number">0</span>) <span class="code-snippet__comment"># 计算收缩损失，即隐层向量导数的平方与权重平方和的乘积的和</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">return</span> mse + contractive_loss.mul_(lam) <span class="code-snippet__comment"># 返回总损失，即均方误差加上收缩损失乘以系数</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 创建一个CAE模型的实例</span></span></code><code><span class="code-snippet_outer">model = CAE()</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 创建一个优化器，使用Adam算法，学习率为0.0001</span></span></code><code><span class="code-snippet_outer">optimizer = optim.Adam(model.parameters(), lr = <span class="code-snippet__number">0.0001</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 如果使用CUDA，将模型转移到GPU上</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">if</span> args.cuda:</span></code><code><span class="code-snippet_outer">    model.cuda()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一个训练函数，接受训练轮数作为参数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">train</span><span class="code-snippet__params">(epoch)</span>:</span></span></code><code><span class="code-snippet_outer">    model.train() <span class="code-snippet__comment"># 将模型设置为训练模式</span></span></code><code><span class="code-snippet_outer">    train_loss = <span class="code-snippet__number">0</span> <span class="code-snippet__comment"># 初始化训练损失为0</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 遍历训练数据加载器，得到每个批次的数据和标签（标签在这里不需要）</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">for</span> idx, (data, _) <span class="code-snippet__keyword">in</span> enumerate(train_loader):</span></code><code><span class="code-snippet_outer">        data = Variable(data) <span class="code-snippet__comment"># 将数据转换为变量</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">if</span> args.cuda:</span></code><code><span class="code-snippet_outer">            data = data.cuda() <span class="code-snippet__comment"># 如果使用CUDA，将数据转移到GPU上</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        optimizer.zero_grad() <span class="code-snippet__comment"># 清空优化器的梯度缓存</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        hidden_representation, recons_x = model(data) <span class="code-snippet__comment"># 将数据输入模型，得到隐层向量和重构向量</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 获取权重矩阵</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># model.state_dict().keys()</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 根据手动查看的键名修改</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># (将来我会尝试自动化这个过程)</span></span></code><code><span class="code-snippet_outer">        W = model.state_dict()[<span class="code-snippet__string">'fc1.weight'</span>] <span class="code-snippet__comment"># 获取编码器的权重矩阵</span></span></code><code><span class="code-snippet_outer">        loss = loss_function(W, data.view(<span class="code-snippet__number">-1</span>, <span class="code-snippet__number">784</span>), recons_x,</span></code><code><span class="code-snippet_outer">                             hidden_representation, lam) <span class="code-snippet__comment"># 计算总损失函数</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        loss.backward() <span class="code-snippet__comment"># 反向传播，计算梯度</span></span></code><code><span class="code-snippet_outer">        train_loss += loss.data[<span class="code-snippet__number">0</span>] <span class="code-snippet__comment"># 累加训练损失</span></span></code><code><span class="code-snippet_outer">        optimizer.step() <span class="code-snippet__comment"># 更新参数</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 如果达到日志间隔，打印训练信息</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">if</span> idx % args.log_interval == <span class="code-snippet__number">0</span>:</span></code><code><span class="code-snippet_outer">            print(<span class="code-snippet__string">'Train epoch: {} [{}/{}({:.0f}%)]\t Loss: {:.6f}'</span>.format(</span></code><code><span class="code-snippet_outer">                  epoch, idx*len(data), len(train_loader.dataset),</span></code><code><span class="code-snippet_outer">                  <span class="code-snippet__number">100</span>*idx/len(train_loader),</span></code><code><span class="code-snippet_outer">                  loss.data[<span class="code-snippet__number">0</span>]/len(data)))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 打印每轮训练的平均损失</span></span></code><code><span class="code-snippet_outer">    print(<span class="code-snippet__string">'====&gt; Epoch: {} Average loss: {:.4f}'</span>.format(</span></code><code><span class="code-snippet_outer">         epoch, train_loss / len(train_loader.dataset)))</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 调用模型的方法，将重构的图片保存</span></span></code><code><span class="code-snippet_outer">    model.samples_write(data,epoch)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 遍历训练轮数，调用训练函数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">for</span> epoch <span class="code-snippet__keyword">in</span> range(args.epochs):</span></code><code><span class="code-snippet_outer">    train(epoch)</span></code></pre></section><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">out目录输出：</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005681" data-ratio="0.8481481481481481" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp0KJffMzjGicQQOkQib1ic5wRLiciadnuZxODo2UZ5UnJTj769HSUBfndictR0kiaF6cyBtC4icVmQMicciceuA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022528_31.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(17, 17, 17);font-family: Raleway, sans-serif;font-size: 16px;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);"></span><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">4.5 卷积自动编码器（Convolutional Autoencoder）</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: left;"><span style='color: rgb(17, 17, 17);font-family: -apple-system, Roboto, SegoeUI, "Segoe UI", "Helvetica Neue", Helvetica, "Microsoft YaHei", "Meiryo UI", Meiryo, "Arial Unicode MS", sans-serif;font-size: 16px;letter-spacing: normal;background-color: rgba(255, 255, 255, 0.7);'>卷积自编码器（Convolutional Autoencoder）是一种利用卷积神经网络（Convolutional Neural Network, CNN）来实现自编码器功能的深度学习模型，它可以对输入的图像数据进行有效的编码和解码，从而实现图像的降维、去噪、重构等任务。卷积自编码器的结构由两部分组成：卷积编码器（Convolutional Encoder）和卷积解码器（Convolutional Decoder）。卷积编码器使用多个卷积层和池化层（Pooling Layer）来逐渐减小图像的尺寸，提取图像的高级特征，并输出一个压缩的隐层表示。卷积解码器使用多个卷积层和上采样层（Upsampling Layer）来逐渐增大图像的尺寸，恢复图像的细节，并输出一个重构的图像。</span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(17, 17, 17);font-family: Raleway, sans-serif;font-size: 16px;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);"></span><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">示例代码(keras)：</span></strong></span></p><section class="code-snippet__fix code-snippet__js"><ul class="code-snippet__line-index code-snippet__js"><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><pre class="code-snippet__js" data-lang="python"><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入所需的模块和库</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> keras.layers <span class="code-snippet__keyword">import</span> Input, Dense, Conv2D, MaxPooling2D, UpSampling2D <span class="code-snippet__comment"># 导入 Keras 中的层模块</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> keras.models <span class="code-snippet__keyword">import</span> Model <span class="code-snippet__comment"># 导入 Keras 中的模型模块</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> keras.callbacks <span class="code-snippet__keyword">import</span> TensorBoard <span class="code-snippet__comment"># 导入 Keras 中的回调模块，用于可视化训练过程</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> keras.datasets <span class="code-snippet__keyword">import</span> mnist <span class="code-snippet__comment"># 导入 Keras 中的数据集模块，用于加载 MNIST 数据集</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> keras <span class="code-snippet__keyword">import</span> backend <span class="code-snippet__keyword">as</span> K <span class="code-snippet__comment"># 导入 Keras 中的后端模块，用于处理张量运算</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> numpy <span class="code-snippet__keyword">as</span> np <span class="code-snippet__comment"># 导入 NumPy 库，用于处理数组运算</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> matplotlib.pyplot <span class="code-snippet__keyword">as</span> plt <span class="code-snippet__comment"># 导入 Matplotlib 库，用于绘制图像</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义输入图像的形状，即 (28, 28, 1)，表示高度为 28，宽度为 28，通道数为 1 的灰度图像</span></span></code><code><span class="code-snippet_outer">input_img = Input(shape=(<span class="code-snippet__number">28</span>, <span class="code-snippet__number">28</span>, <span class="code-snippet__number">1</span>))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义编码器部分，即将输入图像压缩为一个低维的向量</span></span></code><code><span class="code-snippet_outer">x = Conv2D(<span class="code-snippet__number">16</span>, (<span class="code-snippet__number">3</span>, <span class="code-snippet__number">3</span>), activation=<span class="code-snippet__string">'relu'</span>, padding=<span class="code-snippet__string">'same'</span>)(input_img)    <span class="code-snippet__comment"># 使用 16 个 3×3 的卷积核对输入图像进行卷积操作，激活函数为 ReLU，填充方式为 same，保持输出图像的大小不变，即 (28, 28, 16)</span></span></code><code><span class="code-snippet_outer">x = MaxPooling2D((<span class="code-snippet__number">2</span>, <span class="code-snippet__number">2</span>), padding=<span class="code-snippet__string">'same'</span>)(x)                             <span class="code-snippet__comment"># 使用 2×2 的池化核对卷积后的图像进行最大池化操作，填充方式为 same，将输出图像的大小减半，即 (14, 14, 16)</span></span></code><code><span class="code-snippet_outer">x = Conv2D(<span class="code-snippet__number">8</span>, (<span class="code-snippet__number">3</span>, <span class="code-snippet__number">3</span>), activation=<span class="code-snippet__string">'relu'</span>, padding=<span class="code-snippet__string">'same'</span>)(x)             <span class="code-snippet__comment"># 使用 8 个 3×3 的卷积核对池化后的图像进行卷积操作，激活函数为 ReLU，填充方式为 same，保持输出图像的大小不变，即 (14, 14, 8)</span></span></code><code><span class="code-snippet_outer">x = MaxPooling2D((<span class="code-snippet__number">2</span>, <span class="code-snippet__number">2</span>), padding=<span class="code-snippet__string">'same'</span>)(x)                             <span class="code-snippet__comment"># 使用 2×2 的池化核对卷积后的图像进行最大池化操作，填充方式为 same，将输出图像的大小减半，即 (7, 7, 8)</span></span></code><code><span class="code-snippet_outer">x = Conv2D(<span class="code-snippet__number">8</span>, (<span class="code-snippet__number">3</span>, <span class="code-snippet__number">3</span>), activation=<span class="code-snippet__string">'relu'</span>, padding=<span class="code-snippet__string">'same'</span>)(x)             <span class="code-snippet__comment"># 使用 8 个 3×3 的卷积核对池化后的图像进行卷积操作，激活函数为 ReLU，填充方式为 same，保持输出图像的大小不变，即 (7, 7, 8)</span></span></code><code><span class="code-snippet_outer">encoded = MaxPooling2D((<span class="code-snippet__number">2</span>, <span class="code-snippet__number">2</span>), padding=<span class="code-snippet__string">'same'</span>)(x)                       <span class="code-snippet__comment"># 使用 2×2 的池化核对卷积后的图像进行最大池化操作，填充方式为 same，将输出图像的大小减半，即 (4, 4, 8)，这就是编码后的向量，共有 128 个元素</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 在这一点上，表示是 (4, 4, 8)，即 128 维</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义解码器部分，即将编码后的向量还原为原始的输入图像</span></span></code><code><span class="code-snippet_outer">x = Conv2D(<span class="code-snippet__number">8</span>, (<span class="code-snippet__number">3</span>, <span class="code-snippet__number">3</span>), activation=<span class="code-snippet__string">'relu'</span>, padding=<span class="code-snippet__string">'same'</span>)(encoded)       <span class="code-snippet__comment"># 使用 8 个 3×3 的卷积核对编码后的向量进行卷积操作，激活函数为 ReLU，填充方式为 same，保持输出图像的大小不变，即 (4, 4, 8)</span></span></code><code><span class="code-snippet_outer">x = UpSampling2D((<span class="code-snippet__number">2</span>, <span class="code-snippet__number">2</span>))(x)                                             <span class="code-snippet__comment"># 使用 2×2 的上采样核对卷积后的图像进行上采样操作，将输出图像的大小增加一倍，即 (8, 8, 8)</span></span></code><code><span class="code-snippet_outer">x = Conv2D(<span class="code-snippet__number">16</span>, (<span class="code-snippet__number">3</span>, <span class="code-snippet__number">3</span>), activation=<span class="code-snippet__string">'relu'</span>, padding=<span class="code-snippet__string">'same'</span>)(x)            <span class="code-snippet__comment"># 使用 16 个 3×3 的卷积核对上采样后的图像进行卷积操作，激活函数为 ReLU，填充方式为 same，保持输出图像的大小不变，即 (8, 8, 8)</span></span></code><code><span class="code-snippet_outer">x = UpSampling2D((<span class="code-snippet__number">2</span>, <span class="code-snippet__number">2</span>))(x)                                             <span class="code-snippet__comment"># 使用 2×2 的上采样核对卷积后的图像进行上采样操作，将输出图像的大小增加一倍，即 (16, 16, 8)</span></span></code><code><span class="code-snippet_outer">x = Conv2D(<span class="code-snippet__number">16</span>, (<span class="code-snippet__number">3</span>, <span class="code-snippet__number">3</span>), activation=<span class="code-snippet__string">'relu'</span>)(x)                            <span class="code-snippet__comment"># 使用 16 个 3×3 的卷积核对上采样后的图像进行卷积操作，激活函数为 ReLU，填充方式为 valid，将输出图像的大小减少 2，即 (14, 14, 8)</span></span></code><code><span class="code-snippet_outer">x = UpSampling2D((<span class="code-snippet__number">2</span>, <span class="code-snippet__number">2</span>))(x)                                             <span class="code-snippet__comment"># 使用 2×2 的上采样核对卷积后的图像进行上采样操作，将输出图像的大小增加一倍，即 (28, 28, 8)</span></span></code><code><span class="code-snippet_outer">decoded = Conv2D(<span class="code-snippet__number">1</span>, (<span class="code-snippet__number">3</span>, <span class="code-snippet__number">3</span>), activation=<span class="code-snippet__string">'sigmoid'</span>, padding=<span class="code-snippet__string">'same'</span>)(x)    <span class="code-snippet__comment"># 使用 1 个 3×3 的卷积核对上采样后的图像进行卷积操作，激活函数为 sigmoid，填充方式为 same，保持输出图像的大小不变，即 (28, 28, 1)，这就是解码后的图像，与输入图像的形状相同</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义自编码器模型，即将输入图像和解码后的图像连接起来</span></span></code><code><span class="code-snippet_outer">autoencoder = Model(input_img, decoded)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 编译自编码器模型，使用 adadelta 优化器和loss='binary_crossentropy' 二元交叉熵损失函数</span></span></code><code><span class="code-snippet_outer">autoencoder.compile(optimizer=<span class="code-snippet__string">'adadelta'</span>, loss=<span class="code-snippet__string">'binary_crossentropy'</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义编码器模型，即将输入图像和编码后的向量连接起来</span></span></code><code><span class="code-snippet_outer">encoder = Model(input_img, encoded)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 从 MNIST 数据集中加载训练数据和测试数据，只需要图像数据，不需要标签数据</span></span></code><code><span class="code-snippet_outer">(x_train, _), (x_test, _) = mnist.load_data()</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 将图像数据转换为浮点类型，并归一化到 [0, 1] 区间</span></span></code><code><span class="code-snippet_outer">x_train = x_train.astype(<span class="code-snippet__string">'float32'</span>) / <span class="code-snippet__number">255.</span></span></code><code><span class="code-snippet_outer">x_test = x_test.astype(<span class="code-snippet__string">'float32'</span>) / <span class="code-snippet__number">255.</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 将图像数据调整为 (28, 28, 1) 的形状，以适应输入图像的形状</span></span></code><code><span class="code-snippet_outer">x_train = np.reshape(x_train, (len(x_train), <span class="code-snippet__number">28</span>, <span class="code-snippet__number">28</span>, <span class="code-snippet__number">1</span>))</span></code><code><span class="code-snippet_outer">x_test = np.reshape(x_test, (len(x_test), <span class="code-snippet__number">28</span>, <span class="code-snippet__number">28</span>, <span class="code-snippet__number">1</span>))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 训练自编码器模型，使用训练数据作为输入和输出，设置迭代次数为 50，批次大小为 128，打乱数据顺序，使用测试数据作为验证数据，使用 TensorBoard 回调函数来可视化训练过程，将日志文件保存在 conv_autoencoder 目录下</span></span></code><code><span class="code-snippet_outer">autoencoder.fit(x_train, x_train,</span></code><code><span class="code-snippet_outer">                epochs=<span class="code-snippet__number">50</span>,</span></code><code><span class="code-snippet_outer">                batch_size=<span class="code-snippet__number">128</span>,</span></code><code><span class="code-snippet_outer">                shuffle=<span class="code-snippet__keyword">True</span>,</span></code><code><span class="code-snippet_outer">                validation_data=(x_test, x_test),</span></code><code><span class="code-snippet_outer">                callbacks=[TensorBoard(log_dir=<span class="code-snippet__string">'./conv_autoencoder'</span>)])</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 使用编码器模型对测试数据进行编码，得到编码后的向量</span></span></code><code><span class="code-snippet_outer">encoded_imgs = encoder.predict(x_test)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 使用自编码器模型对测试数据进行解码，得到解码后的图像</span></span></code><code><span class="code-snippet_outer">decoded_imgs = autoencoder.predict(x_test)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 设置要显示的图像的个数，这里为 10 个</span></span></code><code><span class="code-snippet_outer">n = <span class="code-snippet__number">10</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 创建一个新的图形窗口，设置大小为 (20, 4)</span></span></code><code><span class="code-snippet_outer">plt.figure(figsize=(<span class="code-snippet__number">20</span>, <span class="code-snippet__number">4</span>))</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 循环遍历每个图像</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">for</span> i <span class="code-snippet__keyword">in</span> range(n):</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 显示原始图像</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个子图，位置为第 i + 1 个，共有 2 行 n 列</span></span></code><code><span class="code-snippet_outer">    ax = plt.subplot(<span class="code-snippet__number">2</span>, n, i + <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 将测试数据中的第 i 个图像从 (28, 28, 1) 的形状还原为 (28, 28) 的形状，并显示出来</span></span></code><code><span class="code-snippet_outer">    plt.imshow(x_test[i].reshape(<span class="code-snippet__number">28</span>, <span class="code-snippet__number">28</span>))</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 设置为灰度模式</span></span></code><code><span class="code-snippet_outer">    plt.gray()</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 隐藏 x 轴和 y 轴的刻度</span></span></code><code><span class="code-snippet_outer">    ax.get_xaxis().set_visible(<span class="code-snippet__keyword">False</span>)</span></code><code><span class="code-snippet_outer">    ax.get_yaxis().set_visible(<span class="code-snippet__keyword">False</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 显示重建图像</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个子图，位置为第 i + 1 + n 个，共有 2 行 n 列</span></span></code><code><span class="code-snippet_outer">    ax = plt.subplot(<span class="code-snippet__number">2</span>, n, i + <span class="code-snippet__number">1</span> + n)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 将解码数据中的第 i 个图像从 (28, 28, 1) 的形状还原为 (28, 28) 的形状，并显示出来</span></span></code><code><span class="code-snippet_outer">    plt.imshow(decoded_imgs[i].reshape(<span class="code-snippet__number">28</span>, <span class="code-snippet__number">28</span>))</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 设置为灰度模式</span></span></code><code><span class="code-snippet_outer">    plt.gray()</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 隐藏 x 轴和 y 轴的刻度</span></span></code><code><span class="code-snippet_outer">    ax.get_xaxis().set_visible(<span class="code-snippet__keyword">False</span>)</span></code><code><span class="code-snippet_outer">    ax.get_yaxis().set_visible(<span class="code-snippet__keyword">False</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 显示图形窗口</span></span></code><code><span class="code-snippet_outer">plt.show()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 创建一个新的图形窗口，设置大小为 (20, 8)</span></span></code><code><span class="code-snippet_outer">plt.figure(figsize=(<span class="code-snippet__number">20</span>, <span class="code-snippet__number">8</span>))</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 循环遍历每个图像</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">for</span> i <span class="code-snippet__keyword">in</span> range(n):</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 显示编码向量</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个子图，位置为第 i + 1 个，共有 1 行 n 列</span></span></code><code><span class="code-snippet_outer">    ax = plt.subplot(<span class="code-snippet__number">1</span>, n, i + <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 将编码数据中的第 i 个向量从 (4, 4, 8) 的形状还原为 (4, 32) 的形状，并转置为 (32, 4) 的形状，然后显示出来</span></span></code><code><span class="code-snippet_outer">    plt.imshow(encoded_imgs[i].reshape(<span class="code-snippet__number">4</span>, <span class="code-snippet__number">4</span> * <span class="code-snippet__number">8</span>).T)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 设置为灰度模式</span></span></code><code><span class="code-snippet_outer">    plt.gray()</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 隐藏 x 轴和 y 轴的刻度</span></span></code><code><span class="code-snippet_outer">    ax.get_xaxis().set_visible(<span class="code-snippet__keyword">False</span>)</span></code><code><span class="code-snippet_outer">    ax.get_yaxis().set_visible(<span class="code-snippet__keyword">False</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 显示图形窗口</span></span></code><code><span class="code-snippet_outer">plt.show()</span></code></pre></section><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: start;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">输出结果：</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005678" data-ratio="0.2" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp0KJffMzjGicQQOkQib1ic5wRLiaHsNTicmXB6LaSTNapEiasIU3uZT6y2dOP2gOI708lPWyx4KmBicYufHQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022529_32.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005677" data-ratio="0.4" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp0KJffMzjGicQQOkQib1ic5wRLzwibUNuxiazdpKSQcd5JTmHETUnUtVYh56tocXPurdZAxBtfMkiaicwziaQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022530_33.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">4.5 多层编码器-层叠自动编码器（Stacked_AutoEncoder-SAE）</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005679" data-ratio="0.3235294117647059" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp1Z9fQxibzL8c84lqpQmU0UAHIs5ibf9dn2h7ZGV89h3IFXd3rmr2z70jdog6p04ygq8oys7KS44ZeA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="850" src="20240525_022531_34.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005685" data-ratio="0.8388888888888889" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp0KJffMzjGicQQOkQib1ic5wRLib0sPTGiaWFZF47UyMK10uicNia7k7uT56GqxvKGvbv2Hk5iciavCPnGpSgg/640?wx_fmt=jpeg&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022533_35.jpeg" style="text-align: center;font-size: var(--articleFontsize);letter-spacing: 0.034em;"/><span style="letter-spacing: 0.578px;"></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"></span><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005684" data-ratio="0.3925925925925926" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp1Z9fQxibzL8c84lqpQmU0UATTnLVc9xNXNWzITGggeTPXxauia2wiaXeuicl6xU71wx07hbT62U83vNQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022534_36.jpeg" style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: start;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">示例代码-SAE 层叠自动编码器（pytorch）</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: start;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">models.py</span></strong></span></p><section class="code-snippet__fix code-snippet__js"><ul class="code-snippet__line-index code-snippet__js"><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><pre class="code-snippet__js" data-lang="ruby"><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入所需的库，包括torch，torchvision，time和os</span></span></code><code><span class="code-snippet_outer">import torch</span></code><code><span class="code-snippet_outer">from torch import nn, optim, functional, utils</span></code><code><span class="code-snippet_outer">import torchvision</span></code><code><span class="code-snippet_outer">from torchvision import datasets, utils</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">import time, os</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一个自编码器层的类，继承自nn.Module</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__class"><span class="code-snippet__keyword">class</span> <span class="code-snippet__title">AutoEncoderLayer</span>(<span class="code-snippet__title">nn</span>.<span class="code-snippet__title">Module</span>):</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__string">""</span><span class="code-snippet__string">"</span></span></code><code><span class="code-snippet_outer">    fully-connected linear layers for stacked autoencoders.</span></code><code><span class="code-snippet_outer">    This module can automatically be trained when training each layer is enabled</span></code><code><span class="code-snippet_outer">    Yes, this is much like the simplest auto-encoder</span></code><code><span class="code-snippet_outer"><span class="code-snippet_outer">    "</span><span class="code-snippet__string">""</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义初始化方法，接受输入维度，输出维度和是否进行逐层预训练的参数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">__init__</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, input_dim=None, output_dim=None, SelfTraining=False)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">super</span>(AutoEncoderLayer, <span class="code-snippet__keyword">self</span>).__init_<span class="code-snippet__number">_</span>() <span class="code-snippet__comment"># 调用父类的初始化方法</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># if input_dim is None or output_dim is None:</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment">#     raise ValueError</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.in_features = input_dim <span class="code-snippet__comment"># 保存输入维度</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.out_features = output_dim <span class="code-snippet__comment"># 保存输出维度</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.is_training_self = SelfTraining  <span class="code-snippet__comment"># 指示是否进行逐层预训练,还是训练整个网络</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 定义编码器，使用全连接层和Sigmoid激活函数</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.encoder = nn.Sequential(</span></code><code><span class="code-snippet_outer">            nn.Linear(<span class="code-snippet__keyword">self</span>.in_features, <span class="code-snippet__keyword">self</span>.out_features, bias=True),</span></code><code><span class="code-snippet_outer">            nn.Sigmoid()  <span class="code-snippet__comment"># 统一使用Sigmoid激活</span></span></code><code><span class="code-snippet_outer">        )</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 定义解码器，使用全连接层和Sigmoid激活函数</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.decoder = nn.Sequential(  <span class="code-snippet__comment"># 此处decoder不使用encoder的转置, 并使用Sigmoid进行激活.</span></span></code><code><span class="code-snippet_outer">            nn.Linear(<span class="code-snippet__keyword">self</span>.out_features, <span class="code-snippet__keyword">self</span>.in_features, bias=True),</span></code><code><span class="code-snippet_outer">            nn.Sigmoid()</span></code><code><span class="code-snippet_outer">        )</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义前向传播方法，接受输入x</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">forward</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, x)</span></span>:</span></code><code><span class="code-snippet_outer">        out = <span class="code-snippet__keyword">self</span>.encoder(x) <span class="code-snippet__comment"># 将x通过编码器得到隐层向量</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">if</span> <span class="code-snippet__keyword">self</span>.<span class="code-snippet__symbol">is_training_self:</span> <span class="code-snippet__comment"># 如果是逐层预训练，返回解码器的输出</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">return</span> <span class="code-snippet__keyword">self</span>.decoder(out)</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__symbol">else:</span> <span class="code-snippet__comment"># 否则，返回隐层向量</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">return</span> out</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个方法，锁定该层的梯度，即不更新参数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">lock_grad</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">for</span> param <span class="code-snippet__keyword">in</span> <span class="code-snippet__keyword">self</span>.parameters():</span></code><code><span class="code-snippet_outer">            param.requires_grad = False</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个方法，解锁该层的梯度，即更新参数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">acquire_grad</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">for</span> param <span class="code-snippet__keyword">in</span> <span class="code-snippet__keyword">self</span>.parameters():</span></code><code><span class="code-snippet_outer">            param.requires_grad = True</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个属性，返回输入维度</span></span></code><code><span class="code-snippet_outer">    @property</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">input_dim</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">return</span> <span class="code-snippet__keyword">self</span>.in_features</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个属性，返回输出维度</span></span></code><code><span class="code-snippet_outer">    @property</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">output_dim</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">return</span> <span class="code-snippet__keyword">self</span>.out_features</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个属性，返回是否进行逐层预训练的标志</span></span></code><code><span class="code-snippet_outer">    @property</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">is_training_layer</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">return</span> <span class="code-snippet__keyword">self</span>.is_training_self</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个属性的设置方法，接受一个布尔值，设置是否进行逐层预训练的标志</span></span></code><code><span class="code-snippet_outer">    @is_training_layer.setter</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">is_training_layer</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, <span class="code-snippet__symbol">other:</span> bool)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.is_training_self = other</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一个栈式自编码器的类，继承自nn.Module</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__class"><span class="code-snippet__keyword">class</span> <span class="code-snippet__title">StackedAutoEncoder</span>(<span class="code-snippet__title">nn</span>.<span class="code-snippet__title">Module</span>):</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__string">""</span><span class="code-snippet__string">"</span></span></code><code><span class="code-snippet_outer">    Construct the whole network with layers_list</span></code><code><span class="code-snippet_outer">    &gt; 栈式自编码器的架构一般是关于中间隐层对称的</span></code><code><span class="code-snippet_outer"><span class="code-snippet_outer">    "</span><span class="code-snippet__string">""</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义初始化方法，接受一个自编码器层的列表作为参数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">__init__</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, layers_list=None)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">super</span>(StackedAutoEncoder, <span class="code-snippet__keyword">self</span>).__init_<span class="code-snippet__number">_</span>() <span class="code-snippet__comment"># 调用父类的初始化方法</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.layers_list = layers_list <span class="code-snippet__comment"># 保存自编码器层的列表</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.initialize() <span class="code-snippet__comment"># 调用初始化方法，将所有层的逐层预训练标志设为False</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 将列表中的四个自编码器层分别命名为encoder_1, encoder_2, encoder_3, encoder_4</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.encoder_1 = <span class="code-snippet__keyword">self</span>.layers_list[<span class="code-snippet__number">0</span>]</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.encoder_2 = <span class="code-snippet__keyword">self</span>.layers_list[<span class="code-snippet__number">1</span>]</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.encoder_3 = <span class="code-snippet__keyword">self</span>.layers_list[<span class="code-snippet__number">2</span>]</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.encoder_4 = <span class="code-snippet__keyword">self</span>.layers_list[<span class="code-snippet__number">3</span>]</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个初始化方法，将所有层的逐层预训练标志设为False</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">initialize</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">for</span> layer <span class="code-snippet__keyword">in</span> <span class="code-snippet__keyword">self</span>.<span class="code-snippet__symbol">layers_list:</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># assert isinstance(layer, AutoEncoderLayer)</span></span></code><code><span class="code-snippet_outer">            layer.is_training_layer = False</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># for param in layer.parameters():</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment">#     param.requires_grad = True</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义前向传播方法，接受输入x</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">forward</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, x)</span></span>:</span></code><code><span class="code-snippet_outer">        out = x <span class="code-snippet__comment"># 将x赋值给out</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 遍历自编码器层的列表，将out依次通过每一层</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># for layer in self.layers_list:</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment">#     out = layer(out)</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 也可以直接使用命名的四个自编码器层</span></span></code><code><span class="code-snippet_outer">        out = <span class="code-snippet__keyword">self</span>.encoder_1(out)</span></code><code><span class="code-snippet_outer">        out = <span class="code-snippet__keyword">self</span>.encoder_2(out)</span></code><code><span class="code-snippet_outer">        out = <span class="code-snippet__keyword">self</span>.encoder_3(out)</span></code><code><span class="code-snippet_outer">        out = <span class="code-snippet__keyword">self</span>.encoder_4(out)</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">return</span> out <span class="code-snippet__comment"># 返回最终的输出</span></span></code></pre></section><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: start;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">run.py</span></strong></span></p><section class="code-snippet__fix code-snippet__js"><ul class="code-snippet__line-index code-snippet__js"><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><pre class="code-snippet__js" data-lang="python"><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入系统库</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> sys</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 将上一级目录添加到系统路径中，以便导入其他模块</span></span></code><code><span class="code-snippet_outer">sys.path.append(<span class="code-snippet__string">'../'</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 从common.datas模块中导入get_mnist_loader函数，用于获取MNIST数据集的加载器</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment">#from common.datas import get_mnist_loader</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 从models模块中导入AutoEncoderLayer和StackedAutoEncoder类，分别用于定义自编码器层和栈式自编码器模型</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> models <span class="code-snippet__keyword">import</span> AutoEncoderLayer, StackedAutoEncoder</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入torch库，用于构建和训练神经网络</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torch</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 从torch.nn模块中导入BCELoss类，用于计算二元交叉熵损失函数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> torch.nn <span class="code-snippet__keyword">import</span> BCELoss</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 从torch模块中导入optim子模块，用于优化神经网络的参数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> torch <span class="code-snippet__keyword">import</span> optim</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入torchvision库，用于处理图像数据</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> torchvision</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 从torchvision.datasets模块中导入MNIST类，用于获取MNIST数据集</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> torchvision.datasets <span class="code-snippet__keyword">import</span> MNIST</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一些超参数，包括逐层预训练的轮数，整体训练的轮数，批量大小，是否打乱数据等</span></span></code><code><span class="code-snippet_outer">num_tranin_layer_epochs = <span class="code-snippet__number">20</span></span></code><code><span class="code-snippet_outer">num_tranin_whole_epochs = <span class="code-snippet__number">50</span></span></code><code><span class="code-snippet_outer">batch_size = <span class="code-snippet__number">100</span></span></code><code><span class="code-snippet_outer">shuffle = <span class="code-snippet__keyword">True</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一个函数，用于获取MNIST数据集的加载器，接受批量大小和是否打乱数据的参数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">get_mnist_loader</span><span class="code-snippet__params">(batch_size=<span class="code-snippet__number">100</span>, shuffle=True)</span>:</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__string">"""</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    :return: train_loader, test_loader</span></code><code><span class="code-snippet_outer">    """</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个训练数据集的对象，指定数据集的根目录，是否为训练集，是否进行图像转换（转换为张量），是否下载数据集</span></span></code><code><span class="code-snippet_outer">    train_dataset = MNIST(root=<span class="code-snippet__string">'../data'</span>,</span></code><code><span class="code-snippet_outer">                          train=<span class="code-snippet__keyword">True</span>,</span></code><code><span class="code-snippet_outer">                          transform=torchvision.transforms.ToTensor(),</span></code><code><span class="code-snippet_outer">                          download=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个测试数据集的对象，指定数据集的根目录，是否为训练集，是否进行图像转换（转换为张量），是否下载数据集</span></span></code><code><span class="code-snippet_outer">    test_dataset = MNIST(root=<span class="code-snippet__string">'../data'</span>,</span></code><code><span class="code-snippet_outer">                         train=<span class="code-snippet__keyword">False</span>,</span></code><code><span class="code-snippet_outer">                         transform=torchvision.transforms.ToTensor(),</span></code><code><span class="code-snippet_outer">                         download=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个训练数据加载器的对象，指定数据集，批量大小，是否打乱数据</span></span></code><code><span class="code-snippet_outer">    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span></code><code><span class="code-snippet_outer">                                               batch_size=batch_size,</span></code><code><span class="code-snippet_outer">                                               shuffle=shuffle)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个测试数据加载器的对象，指定数据集，批量大小，是否打乱数据</span></span></code><code><span class="code-snippet_outer">    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span></code><code><span class="code-snippet_outer">                                              batch_size=batch_size,</span></code><code><span class="code-snippet_outer">                                              shuffle=shuffle)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 返回训练数据加载器和测试数据加载器</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">return</span> train_loader, test_loader</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一个函数，用于训练自编码器层，接受自编码器层的列表，要训练的层的索引，训练轮数，是否验证等参数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">train_layers</span><span class="code-snippet__params">(layers_list=None, layer=None, epoch=None, validate=True)</span>:</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 如果GPU可用，将所有层转移到GPU上</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">if</span> torch.cuda.is_available():</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">for</span> model <span class="code-snippet__keyword">in</span> layers_list:</span></code><code><span class="code-snippet_outer">            model.cuda()</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 调用get_mnist_loader函数，获取训练数据和测试数据的加载器</span></span></code><code><span class="code-snippet_outer">    train_loader, test_loader = get_mnist_loader(batch_size=batch_size, shuffle=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个优化器，使用SGD算法，学习率为0.001，优化要训练的层的参数</span></span></code><code><span class="code-snippet_outer">    optimizer = optim.SGD(layers_list[layer].parameters(), lr=<span class="code-snippet__number">0.001</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个损失函数，使用二元交叉熵损失函数</span></span></code><code><span class="code-snippet_outer">    criterion = BCELoss()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 训练</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 遍历训练轮数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">for</span> epoch_index <span class="code-snippet__keyword">in</span> range(epoch):</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 初始化总损失为0</span></span></code><code><span class="code-snippet_outer">        sum_loss = <span class="code-snippet__number">0.</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 如果要训练的层不是第0层，将前面的层的梯度锁定，并将逐层预训练的标志设为False</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">if</span> layer != <span class="code-snippet__number">0</span>:</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">for</span> index <span class="code-snippet__keyword">in</span> range(layer):</span></code><code><span class="code-snippet_outer">                layers_list[index].lock_grad()</span></code><code><span class="code-snippet_outer">                layers_list[index].is_training_layer = <span class="code-snippet__keyword">False</span> </span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 遍历训练数据加载器，得到每个批次的数据和标签（标签在这里不需要）</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">for</span> batch_index, (train_data, _) <span class="code-snippet__keyword">in</span> enumerate(train_loader):</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 如果GPU可用，将数据转移到GPU上</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">if</span> torch.cuda.is_available():</span></code><code><span class="code-snippet_outer">                train_data = train_data.cuda()</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 将数据展平为一维向量</span></span></code><code><span class="code-snippet_outer">            out = train_data.view(train_data.size(<span class="code-snippet__number">0</span>), <span class="code-snippet__number">-1</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 如果要训练的层不是第0层，将数据依次通过前面的层，得到该层的输入</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">if</span> layer != <span class="code-snippet__number">0</span>:</span></code><code><span class="code-snippet_outer">                <span class="code-snippet__keyword">for</span> l <span class="code-snippet__keyword">in</span> range(layer):</span></code><code><span class="code-snippet_outer">                    out = layers_list[l](out)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 训练第layer层，将输入通过该层，得到输出</span></span></code><code><span class="code-snippet_outer">            pred =  layers_list[layer](out)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 清空优化器的梯度缓存</span></span></code><code><span class="code-snippet_outer">            optimizer.zero_grad()</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 计算输出和输入的二元交叉熵损失</span></span></code><code><span class="code-snippet_outer">            loss = criterion(pred, out)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 累加总损失</span></span></code><code><span class="code-snippet_outer">            sum_loss += loss</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 反向传播，计算梯度</span></span></code><code><span class="code-snippet_outer">            loss.backward()</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 更新参数</span></span></code><code><span class="code-snippet_outer">            optimizer.step()</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 如果达到日志间隔，打印训练信息</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">if</span> (batch_index + <span class="code-snippet__number">1</span>) % <span class="code-snippet__number">10</span> == <span class="code-snippet__number">0</span>:</span></code><code><span class="code-snippet_outer">                print(<span class="code-snippet__string">"Train Layer: {}, Epoch: {}/{}, Iter: {}/{}, Loss: {:.4f}"</span>.format(</span></code><code><span class="code-snippet_outer">                    layer, (epoch_index + <span class="code-snippet__number">1</span>), epoch, (batch_index + <span class="code-snippet__number">1</span>), len(train_loader), loss</span></code><code><span class="code-snippet_outer">                ))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 如果需要验证，执行验证过程</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">if</span> validate:</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">pass</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一个函数，用于训练整个栈式自编码器模型，接受模型，训练轮数，是否验证等参数</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">train_whole</span><span class="code-snippet__params">(model=None, epoch=<span class="code-snippet__number">50</span>, validate=True)</span>:</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 打印开始训练的信息</span></span></code><code><span class="code-snippet_outer">    print(<span class="code-snippet__string">"&gt;&gt; start training whole model"</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 如果GPU可用，将模型转移到GPU上</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">if</span> torch.cuda.is_available():</span></code><code><span class="code-snippet_outer">        model.cuda()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 将模型的所有参数的梯度解锁，即更新参数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">for</span> param <span class="code-snippet__keyword">in</span> model.parameters():</span></code><code><span class="code-snippet_outer">        param.require_grad = <span class="code-snippet__keyword">True</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 调用get_mnist_loader函数，获取训练数据和测试数据的加载器</span></span></code><code><span class="code-snippet_outer">    train_loader, test_loader = get_mnist_loader(batch_size=batch_size, shuffle=shuffle)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个优化器，使用SGD算法，学习率为0.001，优化模型的所有参数</span></span></code><code><span class="code-snippet_outer">    optimizer = optim.SGD(model.parameters(), lr=<span class="code-snippet__number">0.001</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个损失函数，使用均方误差损失函数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># criterion = BCELoss()</span></span></code><code><span class="code-snippet_outer">    criterion = torch.nn.MSELoss()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 从测试数据加载器中获取一批测试数据，并保存为图片</span></span></code><code><span class="code-snippet_outer">    test_data, _ = next(iter(test_loader))</span></code><code><span class="code-snippet_outer">    torchvision.utils.save_image(test_data, <span class="code-snippet__string">'./test_images/real_test_images.png'</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 训练</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 遍历训练轮数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">for</span> epoch_index <span class="code-snippet__keyword">in</span> range(epoch):</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 初始化总损失为0</span></span></code><code><span class="code-snippet_outer">        sum_loss = <span class="code-snippet__number">0.</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 遍历训练数据加载器，得到每个批次的数据和标签（标签在这里不需要）</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">for</span> batch_index, (train_data, _) <span class="code-snippet__keyword">in</span> enumerate(train_loader):</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 如果GPU可用，将数据转移到GPU上</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">if</span> torch.cuda.is_available():</span></code><code><span class="code-snippet_outer">                train_data = train_data.cuda()</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 将数据展平为一维向量</span></span></code><code><span class="code-snippet_outer">            x = train_data.view(train_data.size(<span class="code-snippet__number">0</span>), <span class="code-snippet__number">-1</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 将数据输入模型，得到输出</span></span></code><code><span class="code-snippet_outer">            out = model(x)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 清空优化器的梯度缓存</span></span></code><code><span class="code-snippet_outer">            optimizer.zero_grad()</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 计算输出和输入的均方误差损失</span></span></code><code><span class="code-snippet_outer">            loss = criterion(out, x)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 累加总损失</span></span></code><code><span class="code-snippet_outer">            sum_loss += loss</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 反向传播，计算梯度</span></span></code><code><span class="code-snippet_outer">            loss.backward()</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 更新参数</span></span></code><code><span class="code-snippet_outer">            optimizer.step()</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 如果达到日志间隔，打印训练信息</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">if</span> (batch_index + <span class="code-snippet__number">1</span>) % <span class="code-snippet__number">10</span> == <span class="code-snippet__number">0</span>:</span></code><code><span class="code-snippet_outer">                print(<span class="code-snippet__string">"Train Whole, Epoch: {}/{}, Iter: {}/{}, Loss: {:.4f}"</span>.format(</span></code><code><span class="code-snippet_outer">                    (epoch_index + <span class="code-snippet__number">1</span>), epoch, (batch_index + <span class="code-snippet__number">1</span>), len(train_loader), loss</span></code><code><span class="code-snippet_outer">                ))</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 如果是最后一个批次，将输出重构为图片，并保存</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">if</span> batch_index == len(train_loader) - <span class="code-snippet__number">1</span>:</span></code><code><span class="code-snippet_outer">                torchvision.utils.save_image(out.view(<span class="code-snippet__number">100</span>, <span class="code-snippet__number">1</span>, <span class="code-snippet__number">28</span>, <span class="code-snippet__number">28</span>), <span class="code-snippet__string">"./test_images/out_{}_{}.png"</span>.format(epoch_index, batch_index))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 每个轮数验证一次</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">if</span> validate:</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 如果GPU可用，将测试数据转移到GPU上</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">if</span> torch.cuda.is_available():</span></code><code><span class="code-snippet_outer">                test_data = test_data.cuda()</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 将测试数据展平为一维向量</span></span></code><code><span class="code-snippet_outer">            x = test_data.view(test_data.size(<span class="code-snippet__number">0</span>), <span class="code-snippet__number">-1</span>)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 将测试数据输入模型，得到输出</span></span></code><code><span class="code-snippet_outer">            out = model(x)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 计算输出和输入的均方误差损失</span></span></code><code><span class="code-snippet_outer">            loss = criterion(out, x)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 打印验证信息</span></span></code><code><span class="code-snippet_outer">            print(<span class="code-snippet__string">"Test Epoch: {}/{}, Iter: {}/{}, test Loss: {}"</span>.format(</span></code><code><span class="code-snippet_outer">                epoch_index + <span class="code-snippet__number">1</span>, epoch, (epoch_index + <span class="code-snippet__number">1</span>), len(test_loader), loss</span></code><code><span class="code-snippet_outer">            ))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 判断是否为主模块，如果是，则执行以下代码</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">if</span> __name__ == <span class="code-snippet__string">'__main__'</span>:</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 导入os库，用于操作系统相关的功能</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">import</span> os</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 如果不存在test_images文件夹，就创建一个</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">if</span> <span class="code-snippet__keyword">not</span> os.path.exists(<span class="code-snippet__string">'test_images'</span>):</span></code><code><span class="code-snippet_outer">        os.mkdir(<span class="code-snippet__string">'test_images'</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 如果不存在models文件夹，就创建一个</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">if</span> <span class="code-snippet__keyword">not</span> os.path.exists(<span class="code-snippet__string">'models'</span>):</span></code><code><span class="code-snippet_outer">        os.mkdir(<span class="code-snippet__string">'models'</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义自编码器层数为5</span></span></code><code><span class="code-snippet_outer">    nun_layers = <span class="code-snippet__number">5</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建四个自编码器层的对象，分别为encoder_1, encoder_2, decoder_3, decoder_4，设置输入维度，输出维度和逐层预训练的标志</span></span></code><code><span class="code-snippet_outer">    encoder_1 = AutoEncoderLayer(<span class="code-snippet__number">784</span>, <span class="code-snippet__number">256</span>, SelfTraining=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer">    encoder_2 = AutoEncoderLayer(<span class="code-snippet__number">256</span>, <span class="code-snippet__number">64</span>, SelfTraining=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer">    decoder_3 = AutoEncoderLayer(<span class="code-snippet__number">64</span>, <span class="code-snippet__number">256</span>, SelfTraining=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer">    decoder_4 = AutoEncoderLayer(<span class="code-snippet__number">256</span>, <span class="code-snippet__number">784</span>, SelfTraining=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 将四个自编码器层的对象放入一个列表中，命名为layers_list</span></span></code><code><span class="code-snippet_outer">    layers_list = [encoder_1, encoder_2, decoder_3, decoder_4]</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 按照顺序对每一层进行预训练</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 遍历层数，从0到3</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__keyword">for</span> level <span class="code-snippet__keyword">in</span> range(nun_layers - <span class="code-snippet__number">1</span>):</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 调用train_layers函数，传入自编码器层的列表，要训练的层的索引，训练轮数，是否验证等参数，进行逐层预训练</span></span></code><code><span class="code-snippet_outer">        train_layers(layers_list=layers_list, layer=level, epoch=num_tranin_layer_epochs, validate=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 统一训练</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个栈式自编码器的对象，传入自编码器层的列表，命名为SAE_model</span></span></code><code><span class="code-snippet_outer">    SAE_model = StackedAutoEncoder(layers_list=layers_list)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 调用train_whole函数，传入栈式自编码器的对象，训练轮数，是否验证等参数，进行整体训练</span></span></code><code><span class="code-snippet_outer">    train_whole(model=SAE_model, epoch=num_tranin_whole_epochs, validate=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 保存模型 refer: https://pytorch.org/docs/master/notes/serialization.html</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 调用torch.save函数，传入栈式自编码器的对象和保存路径，将模型保存为sae_model.pt文件</span></span></code><code><span class="code-snippet_outer">    torch.save(SAE_model, <span class="code-snippet__string">'./models/sae_model.pt'</span>)</span></code></pre></section><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: start;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">输出结果：</span></strong></span><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005683" data-ratio="0.43425925925925923" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp1Z9fQxibzL8c84lqpQmU0UA4nc5xrPDxic0WpqKG9RAO5OkAwEbAwtSvfKzqTmDrNbIhcNqRTubeRg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022535_37.jpeg"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><span style="color: rgb(17, 17, 17);font-family: Raleway, sans-serif;font-size: 16px;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);">test_images文件夹<br/></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><span style="color: rgb(17, 17, 17);font-family: Raleway, sans-serif;font-size: 16px;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);">real_test_images.png</span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><span style="color: rgb(17, 17, 17);font-family: Raleway, sans-serif;font-size: 16px;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);"><br/></span></p><hr style="border-style: solid;border-width: 1px 0 0;border-color: rgba(0,0,0,0.1);-webkit-transform-origin: 0 0;-webkit-transform: scale(1, 0.5);transform-origin: 0 0;transform: scale(1, 0.5);"/><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><span style="color: rgb(17, 17, 17);font-family: Raleway, sans-serif;font-size: 16px;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);"></span></p><p><br/></p><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div>

</div>
                <p></p>
                <p></p>
                <div>本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 </div>
                <div  class="hidden">本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 <br><p class="hidden">code/s?__biz=MzU3OTIyMjgxNw==&mid=2247489336&idx=1&sn=b89174fb29beca54f074916ac48f0fe3&chksm=fd683b76ca1fb260f16d7fcbce3ededf1ebef05b9c86f4e8e0c6151d13f13fbb9e9920703618#rd </p></div>
            </body>
            </html>
            