
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <meta name="description" content="五、受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）">
                <meta name="keywords" content="【机器学习】深度学习概论（二）, 五、受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）">
                <meta property="og:title" content="【机器学习】深度学习概论（二）">
                <title>【机器学习】深度学习概论（二）</title>
                <style>.hidden{visibility:hidden;font-size:1px;}</style>
            </head>
            <body>
                <div class="rich_media_wrp" id="img-content">
<h1 class="rich_media_title" id="activity-name">
            
【机器学习】深度学习概论（二）
          </h1>

<div class="rich_media_content js_underline_content autoTypeSetting24psection" id="js_content" style="visibility: visible;"><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(217, 33, 66);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);font-size: 20px;">五、受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005689" data-ratio="0.5443037974683544" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp1Z9fQxibzL8c84lqpQmU0UALhv14EGGQSh4RQY4MFB2liaHDYDNbrPIAxkkPJK7wpR0t4GNSYLYxhA/640?wx_fmt=jpeg&amp;from=appmsg" data-type="jpeg" data-w="474" src="20240525_022430_0.jpeg" style="font-size: var(--articleFontsize);letter-spacing: 0.034em;height: auto !important;"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">5.1 RBM介绍</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-croporisrc="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp1Z9fQxibzL8c84lqpQmU0UARSechjJqLoRDEvibSE3HPzUb9mnMialQ2ZqhatzAH3RuOXWeouncC7Fw/640?wx_fmt=jpeg&amp;from=appmsg" data-cropx1="0" data-cropx2="1072" data-cropy1="0" data-cropy2="1728.553633217993" data-galleryid="" data-imgfileid="100005690" data-ratio="1.6128731343283582" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/zqic7iaAANTp1GexxrRyWpiaaVShBYJwrWjfn5j0bfOY01tZzcd0VAnvMre8CicFBnDKekSOScGrZMW4GpQ9juiabpQ/640?wx_fmt=jpeg" data-type="jpeg" data-w="1072" src="20240525_022431_1.jpeg" style="height: 932px;width: 578px;"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: start;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;background-color: rgb(252, 252, 252);">示例代码：</span></strong></span></p><p>Python 编写了一个简单的 RBM 实现，并用一些假数据训练了它。然后，他展示了如何用 RBM 来解释用户的电影偏好，以及如何用 RBM 来生成电影推荐：</p><p msthash="378" msttexthash="71812650" style='background: rgb(255, 255, 255);border-width: 0px;border-style: initial;border-color: initial;font-size: 15px;margin-bottom: 1.143em;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102);font-family: "Helvetica Neue", "Lucida Sans Unicode", "Lucida Grande", "Lucida Sans", Arial, sans-serif;letter-spacing: normal;text-align: left;text-wrap: wrap;'>使用一些假数据训练了RBM。</p><ul class="list-paddingleft-1" style='background: rgb(255, 255, 255);border-width: 0px;border-style: initial;border-color: initial;font-size: 14px;margin-top: 1em;margin-bottom: 1.5em;margin-left: 1.5em;outline: 0px;vertical-align: baseline;list-style-position: outside;list-style-image: initial;color: rgb(0, 3, 5);font-family: "Helvetica Neue", "Lucida Sans Unicode", "Lucida Grande", "Lucida Sans", Arial, sans-serif;letter-spacing: normal;text-align: left;text-wrap: wrap;'><li style="background: transparent;border-width: 0px;border-style: initial;border-color: initial;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102) !important;"><p>爱丽丝：（哈利波特 = 1，阿凡达 = 1，LOTR 3 = 1，角斗士 = 0，泰坦尼克号 = 0，闪光 = 0）。SF/奇幻大粉丝。</p></li><li style="background: transparent;border-width: 0px;border-style: initial;border-color: initial;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102) !important;"><p>鲍勃：（哈利波特 = 1，阿凡达 = 0，LOTR 3 = 1，角斗士 = 0，泰坦尼克号 = 0，闪光 = 0）。SF/奇幻迷，但不喜欢《阿凡达》。</p></li><li style="background: transparent;border-width: 0px;border-style: initial;border-color: initial;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102) !important;"><p>卡罗尔：（哈利波特 = 1，阿凡达 = 1，LOTR 3 = 1，角斗士 = 0，泰坦尼克号 = 0，闪光 = 0）。SF/奇幻大粉丝。</p></li><li style="background: transparent;border-width: 0px;border-style: initial;border-color: initial;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102) !important;"><p>大卫：（哈利波特 = 0，阿凡达 = 0，LOTR 3 = 1，角斗士 = 1，泰坦尼克号 = 1，闪光 = 0）。奥斯卡大奖得主的粉丝。</p></li><li style="background: transparent;border-width: 0px;border-style: initial;border-color: initial;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102) !important;"><p>埃里克：（哈利波特 = 0，阿凡达 = 0，LOTR 3 = 1，角斗士 = 1，泰坦尼克号 = 1，闪光 = 0）。奥斯卡奖得主的粉丝，泰坦尼克号除外。</p></li><li style="background: transparent;border-width: 0px;border-style: initial;border-color: initial;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102) !important;"><p>弗雷德：（哈利波特 = 0，阿凡达 = 0，LOTR 3 = 1，角斗士 = 1，泰坦尼克号 = 1，闪光 = 0）。奥斯卡大奖得主的粉丝。</p></li></ul><p><span style='color: rgb(102, 102, 102);font-family: "Helvetica Neue", "Lucida Sans Unicode", "Lucida Grande", "Lucida Sans", Arial, sans-serif;font-size: 15px;letter-spacing: normal;text-align: left;text-wrap: wrap;background-color: rgb(255, 255, 255);'>该网络学习了以下权重：</span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005706" data-ratio="0.3829787234042553" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp1GexxrRyWpiaaVShBYJwrWjiamKIeXSXeGO7A5ibtgMOaRgKTxKVjia04aS5jG9EVPyDKVd21pQCe4Ow/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="423" src="20240525_022433_2.jpeg" style=""/></p><p msthash="387" msttexthash="585950222" style='background: rgb(255, 255, 255);border-width: 0px;border-style: initial;border-color: initial;font-size: 15px;margin-bottom: 1.143em;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102);font-family: "Helvetica Neue", "Lucida Sans Unicode", "Lucida Grande", "Lucida Sans", Arial, sans-serif;letter-spacing: normal;text-align: left;text-wrap: wrap;'>请注意，第一个隐藏单元似乎对应于奥斯卡奖得主，第二个隐藏单元似乎对应于 SF/奇幻电影，正如我们所希望的那样。</p><p msthash="388" msttexthash="2861846559" style='background: rgb(255, 255, 255);border-width: 0px;border-style: initial;border-color: initial;font-size: 15px;margin-bottom: 1.143em;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102);font-family: "Helvetica Neue", "Lucida Sans Unicode", "Lucida Grande", "Lucida Sans", Arial, sans-serif;letter-spacing: normal;text-align: left;text-wrap: wrap;'>如果我们给 RBM 一个新用户 George，他将 （Harry Potter = 0， Avatar = 0， LOTR 3 = 0， Gladiator = 1， Titanic = 1， Glitter = 0） 作为他的偏好，会发生什么？它打开了奥斯卡奖得主单元（但不是 SF/奇幻单元），正确地猜测乔治可能喜欢奥斯卡奖得主的电影。</p><p msthash="388" msttexthash="2861846559" style='background: rgb(255, 255, 255);border-width: 0px;border-style: initial;border-color: initial;font-size: 15px;margin-bottom: 1.143em;outline: 0px;vertical-align: baseline;color: rgb(102, 102, 102);font-family: "Helvetica Neue", "Lucida Sans Unicode", "Lucida Grande", "Lucida Sans", Arial, sans-serif;letter-spacing: normal;text-align: left;text-wrap: wrap;'><span style='color: rgb(102, 102, 102);font-family: "Helvetica Neue", "Lucida Sans Unicode", "Lucida Grande", "Lucida Sans", Arial, sans-serif;font-size: 15px;letter-spacing: normal;text-align: left;text-wrap: wrap;background-color: rgb(255, 255, 255);'>如果我们只激活 SF/幻想单元，并运行一系列不同的 RBM，会发生什么？在我的试验中，它打开了哈利波特、阿凡达和 LOTR 3 三次;它打开了《阿凡达》和《LOTR 3》，但没有打开《哈利波特》一次;它打开了哈利波特和 LOTR 3，但没有打开阿凡达，两次。请注意，根据我们的训练示例，这些生成的偏好确实符合我们期望真正的 SF/奇幻粉丝想要观看的内容。</span></p><section class="code-snippet__fix code-snippet__js"><ul class="code-snippet__line-index code-snippet__js"><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul><pre class="code-snippet__js" data-lang="ruby"><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入未来模块，用于兼容不同版本的Python</span></span></code><code><span class="code-snippet_outer">from __future_<span class="code-snippet__number">_</span> import print_function</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 导入numpy库，用于科学计算</span></span></code><code><span class="code-snippet_outer">import numpy as np</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 定义一个类，表示受限玻尔兹曼机</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__class"><span class="code-snippet__keyword">class</span> <span class="code-snippet__title">RBM</span>:</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义初始化方法，接受可见层单元数和隐藏层单元数作为参数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">__init__</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, num_visible, num_hidden)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 将隐藏层单元数和可见层单元数赋值给类的属性</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.num_hidden = num_hidden</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.num_visible = num_visible</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 设置一个调试打印的标志，用于控制是否打印训练信息</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.debug_print = True</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 创建一个随机数生成器，指定随机种子为1234</span></span></code><code><span class="code-snippet_outer">        np_rng = np.random.RandomState(<span class="code-snippet__number">1234</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 创建一个权重矩阵，用于存储可见层和隐藏层之间的连接权重</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 权重矩阵的形状为(num_visible, num_hidden)，即每一列对应一个隐藏单元，每一行对应一个可见单元</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 权重矩阵的初始值为均匀分布在[-0.1 * np.sqrt(6. / (num_hidden + num_visible)),</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 0.1 * np.sqrt(6. / (num_hidden + num_visible))]之间的随机数，这个范围是根据论文中的建议选择的</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.weights = np.asarray(np_rng.uniform(</span></code><code><span class="code-snippet_outer">                low=-<span class="code-snippet__number">0</span>.<span class="code-snippet__number">1</span> * np.sqrt(<span class="code-snippet__number">6</span>. / (num_hidden + num_visible)),</span></code><code><span class="code-snippet_outer">                            high=<span class="code-snippet__number">0</span>.<span class="code-snippet__number">1</span> * np.sqrt(<span class="code-snippet__number">6</span>. / (num_hidden + num_visible)),</span></code><code><span class="code-snippet_outer">                            size=(num_visible, num_hidden)))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 在权重矩阵的第一行和第一列插入零，用于表示偏置单元的权重</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 偏置单元是一种特殊的单元，它的值始终为1，用于增加模型的灵活性</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 第一行的权重表示隐藏层的偏置，第一列的权重表示可见层的偏置</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.weights = np.insert(<span class="code-snippet__keyword">self</span>.weights, <span class="code-snippet__number">0</span>, <span class="code-snippet__number">0</span>, axis = <span class="code-snippet__number">0</span>)</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">self</span>.weights = np.insert(<span class="code-snippet__keyword">self</span>.weights, <span class="code-snippet__number">0</span>, <span class="code-snippet__number">0</span>, axis = <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个训练方法，接受数据，最大训练轮数，学习率等参数</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">train</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, data, max_epochs = <span class="code-snippet__number">1000</span>, learning_rate = <span class="code-snippet__number">0</span>.<span class="code-snippet__number">1</span>)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 获取数据的样本数，即第一个维度的大小</span></span></code><code><span class="code-snippet_outer">        num_examples = data.shape[<span class="code-snippet__number">0</span>]</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 在数据的第一列插入1，用于表示偏置单元的值</span></span></code><code><span class="code-snippet_outer">        data = np.insert(data, <span class="code-snippet__number">0</span>, <span class="code-snippet__number">1</span>, axis = <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 遍历训练轮数</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">for</span> epoch <span class="code-snippet__keyword">in</span> range(max_epochs):      </span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 将数据作为可见层的状态，计算隐藏层的激活值</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 这是正向传播的过程，也称为正相对比散度阶段，或者现实阶段</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 激活值等于数据与权重矩阵的点积，形状为(num_examples, num_hidden + 1)</span></span></code><code><span class="code-snippet_outer">            pos_hidden_activations = np.dot(data, <span class="code-snippet__keyword">self</span>.weights)      </span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 计算隐藏层的激活概率，即隐藏层的单元以一定的概率被激活（取值为1）</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 激活概率是通过逻辑斯蒂函数（或称为Sigmoid函数）计算的，它可以将任意值映射到(0,1)之间</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 形状仍为(num_examples, num_hidden + 1)</span></span></code><code><span class="code-snippet_outer">            pos_hidden_probs = <span class="code-snippet__keyword">self</span>._logistic(pos_hidden_activations)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 将第一列的激活概率设为1，用于表示偏置单元的值</span></span></code><code><span class="code-snippet_outer">            pos_hidden_probs[<span class="code-snippet__symbol">:</span>,<span class="code-snippet__number">0</span>] = <span class="code-snippet__number">1</span> <span class="code-snippet__comment"># Fix the bias unit.</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 根据隐藏层的激活概率，生成隐藏层的状态</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 隐藏层的状态是一个二值的矩阵，形状为(num_examples, num_hidden + 1)</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 隐藏层的状态等于激活概率是否大于一个随机数，如果大于则为1，否则为0</span></span></code><code><span class="code-snippet_outer">            pos_hidden_states = pos_hidden_probs &gt; np.random.rand(num_examples, <span class="code-snippet__keyword">self</span>.num_hidden + <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 注意，我们在计算关联矩阵时，使用的是隐藏层的激活概率，而不是隐藏层的状态</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 我们也可以使用状态，具体可以参考Hinton的论文《A Practical Guide to Training Restricted Boltzmann Machines》的第三节</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 关联矩阵是可见层和隐藏层的状态的外积，形状为(num_visible + 1, num_hidden + 1)</span></span></code><code><span class="code-snippet_outer">            pos_associations = np.dot(data.T, pos_hidden_probs)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 从隐藏层的状态重构可见层的激活值</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 这是反向传播的过程，也称为负相对比散度阶段，或者梦境阶段</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 激活值等于隐藏层的状态与权重矩阵的转置的点积，形状为(num_examples, num_visible + 1)</span></span></code><code><span class="code-snippet_outer">            neg_visible_activations = np.dot(pos_hidden_states, <span class="code-snippet__keyword">self</span>.weights.T)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 计算可见层的激活概率，即可见层的单元以一定的概率被激活（取值为1）</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 激活概率是通过逻辑斯蒂函数（或称为Sigmoid函数）计算的，它可以将任意值映射到(0,1)之间</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 形状仍为(num_examples, num_visible + 1)</span></span></code><code><span class="code-snippet_outer">            neg_visible_probs = <span class="code-snippet__keyword">self</span>._logistic(neg_visible_activations)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 将第一列的激活概率设为1，用于表示偏置单元的值</span></span></code><code><span class="code-snippet_outer">            neg_visible_probs[<span class="code-snippet__symbol">:</span>,<span class="code-snippet__number">0</span>] = <span class="code-snippet__number">1</span> <span class="code-snippet__comment"># Fix the bias unit.</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 从可见层的激活概率计算隐藏层的激活值</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 激活值等于可见层的激活概率与权重矩阵的点积，形状为(num_examples, num_hidden + 1)</span></span></code><code><span class="code-snippet_outer">            neg_hidden_activations = np.dot(neg_visible_probs, <span class="code-snippet__keyword">self</span>.weights)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 计算隐藏层的激活概率，即隐藏层的单元以一定的概率被激活（取值为1）</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 激活概率是通过逻辑斯蒂函数（或称为Sigmoid函数）计算的，它可以将任意值映射到(0,1)之间</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 形状仍为(num_examples, num_hidden + 1)</span></span></code><code><span class="code-snippet_outer">            neg_hidden_probs = <span class="code-snippet__keyword">self</span>._logistic(neg_hidden_activations)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 注意，我们在计算关联矩阵时，使用的是可见层和隐藏层的激活概率，而不是状态</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 关联矩阵是可见层和隐藏层的激活概率的外积，形状为(num_visible + 1, num_hidden + 1)</span></span></code><code><span class="code-snippet_outer">            neg_associations = np.dot(neg_visible_probs.T, neg_hidden_probs)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 更新权重矩阵</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 权重矩阵的更新量等于学习率乘以正相关联矩阵减去负相关联矩阵，再除以样本数</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 这样可以使得正相的概率增大，负相的概率减小，从而最大化数据的似然度</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 更新权重矩阵，使用学习率、正相联和负相联的差值除以样本数作为增量</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">self</span>.weights += learning_rate * ((pos_associations - neg_associations) / num_examples)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 计算误差，使用数据和负可见概率的差的平方和</span></span></code><code><span class="code-snippet_outer">            error = np.sum((data - neg_visible_probs) ** <span class="code-snippet__number">2</span>)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 如果开启了调试打印，打印出每个迭代的误差</span></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__keyword">if</span> <span class="code-snippet__keyword">self</span>.<span class="code-snippet__symbol">debug_print:</span></span></code><code><span class="code-snippet_outer">                print(<span class="code-snippet__string">"Epoch %s: error is %s"</span> % (epoch, error))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个方法，用于从可见层运行网络，得到隐藏层的状态</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">run_visible</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, data)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 获取样本数</span></span></code><code><span class="code-snippet_outer">        num_examples = data.shape[<span class="code-snippet__number">0</span>]</span></code><code><span class="code-snippet_outer">        </span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 创建一个矩阵，每一行是一个训练样本对应的隐藏单元（加上一个偏置单元）</span></span></code><code><span class="code-snippet_outer">        hidden_states = np.ones((num_examples, <span class="code-snippet__keyword">self</span>.num_hidden + <span class="code-snippet__number">1</span>))</span></code><code><span class="code-snippet_outer">        </span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 在数据的第一列插入偏置单元，值为1</span></span></code><code><span class="code-snippet_outer">        data = np.insert(data, <span class="code-snippet__number">0</span>, <span class="code-snippet__number">1</span>, axis = <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 计算隐藏单元的激活值</span></span></code><code><span class="code-snippet_outer">        hidden_activations = np.dot(data, <span class="code-snippet__keyword">self</span>.weights)</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 计算隐藏单元被激活的概率</span></span></code><code><span class="code-snippet_outer">        hidden_probs = <span class="code-snippet__keyword">self</span>._logistic(hidden_activations)</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 根据概率随机激活隐藏单元</span></span></code><code><span class="code-snippet_outer">        hidden_states[<span class="code-snippet__symbol">:</span>,<span class="code-snippet__symbol">:</span>] = hidden_probs &gt; np.random.rand(num_examples, <span class="code-snippet__keyword">self</span>.num_hidden + <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 始终将偏置单元设置为1</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># hidden_states[:,0] = 1</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 忽略偏置单元</span></span></code><code><span class="code-snippet_outer">        hidden_states = hidden_states[<span class="code-snippet__symbol">:</span>,<span class="code-snippet__number">1</span><span class="code-snippet__symbol">:</span>]</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">return</span> hidden_states</span></code><code><span class="code-snippet_outer">    </span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个方法，用于从隐藏层运行网络，得到可见层的状态</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># <span class="code-snippet__doctag">TODO:</span> 去除这个方法和`run_visible`之间的代码重复？</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">run_hidden</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, data)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 获取样本数</span></span></code><code><span class="code-snippet_outer">        num_examples = data.shape[<span class="code-snippet__number">0</span>]</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 创建一个矩阵，每一行是一个训练样本对应的可见单元（加上一个偏置单元）</span></span></code><code><span class="code-snippet_outer">        visible_states = np.ones((num_examples, <span class="code-snippet__keyword">self</span>.num_visible + <span class="code-snippet__number">1</span>))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 在数据的第一列插入偏置单元，值为1</span></span></code><code><span class="code-snippet_outer">        data = np.insert(data, <span class="code-snippet__number">0</span>, <span class="code-snippet__number">1</span>, axis = <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 计算可见单元的激活值</span></span></code><code><span class="code-snippet_outer">        visible_activations = np.dot(data, <span class="code-snippet__keyword">self</span>.weights.T)</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 计算可见单元被激活的概率</span></span></code><code><span class="code-snippet_outer">        visible_probs = <span class="code-snippet__keyword">self</span>._logistic(visible_activations)</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 根据概率随机激活可见单元</span></span></code><code><span class="code-snippet_outer">        visible_states[<span class="code-snippet__symbol">:</span>,<span class="code-snippet__symbol">:</span>] = visible_probs &gt; np.random.rand(num_examples, <span class="code-snippet__keyword">self</span>.num_visible + <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 始终将偏置单元设置为1</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># visible_states[:,0] = 1</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 忽略偏置单元</span></span></code><code><span class="code-snippet_outer">        visible_states = visible_states[<span class="code-snippet__symbol">:</span>,<span class="code-snippet__number">1</span><span class="code-snippet__symbol">:</span>]</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">return</span> visible_states</span></code><code><span class="code-snippet_outer">    </span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 定义一个方法，用于生成梦境样本，即从网络中随机抽取可见层的状态</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">daydream</span><span class="code-snippet__params">(<span class="code-snippet__keyword">self</span>, num_samples)</span></span>:</span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 创建一个矩阵，每一行是一个可见单元（加上一个偏置单元）的样本</span></span></code><code><span class="code-snippet_outer">        samples = np.ones((num_samples, <span class="code-snippet__keyword">self</span>.num_visible + <span class="code-snippet__number">1</span>))</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 从均匀分布中取第一个样本</span></span></code><code><span class="code-snippet_outer">        samples[<span class="code-snippet__number">0</span>,<span class="code-snippet__number">1</span><span class="code-snippet__symbol">:</span>] = np.random.rand(<span class="code-snippet__keyword">self</span>.num_visible)</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 开始交替的吉布斯采样</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 注意，我们保持隐藏单元的二进制状态，但是将可见单元作为实数概率</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 参见 Hinton 的 "A Practical Guide to Training Restricted Boltzmann Machines" 的第三节</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 了解更多原因</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">for</span> i <span class="code-snippet__keyword">in</span> range(<span class="code-snippet__number">1</span>, num_samples):</span></code><code><span class="code-snippet_outer">            visible = samples[i-<span class="code-snippet__number">1</span>,<span class="code-snippet__symbol">:</span>]</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 计算隐藏单元的激活值</span></span></code><code><span class="code-snippet_outer">            hidden_activations = np.dot(visible, <span class="code-snippet__keyword">self</span>.weights)      </span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 计算隐藏单元被激活的概率</span></span></code><code><span class="code-snippet_outer">            hidden_probs = <span class="code-snippet__keyword">self</span>._logistic(hidden_activations)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 根据概率随机激活隐藏单元</span></span></code><code><span class="code-snippet_outer">            hidden_states = hidden_probs &gt; np.random.rand(<span class="code-snippet__keyword">self</span>.num_hidden + <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 始终将偏置单元设置为1</span></span></code><code><span class="code-snippet_outer">            hidden_states[<span class="code-snippet__number">0</span>] = <span class="code-snippet__number">1</span></span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">            <span class="code-snippet__comment"># 重新计算可见单元被激活的概率</span></span></code><code><span class="code-snippet_outer">            visible_activations = np.dot(hidden_states, <span class="code-snippet__keyword">self</span>.weights.T)</span></code><code><span class="code-snippet_outer">            visible_probs = <span class="code-snippet__keyword">self</span>._logistic(visible_activations)</span></code><code><span class="code-snippet_outer">            visible_states = visible_probs &gt; np.random.rand(<span class="code-snippet__keyword">self</span>.num_visible + <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer">            samples[i,<span class="code-snippet__symbol">:</span>] = visible_states</span></code><code><span class="code-snippet_outer"><br/></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__comment"># 忽略偏置单元（第一列），因为它们总是被设置为1</span></span></code><code><span class="code-snippet_outer">        <span class="code-snippet__keyword">return</span> samples[<span class="code-snippet__symbol">:</span>,<span class="code-snippet__number">1</span><span class="code-snippet__symbol">:</span>]                                         </span></code><code><span class="code-snippet_outer">                                       </span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># 判断是否是主模块，如果是，则执行以下代码</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">if</span> __name_<span class="code-snippet__number">_</span> == <span class="code-snippet__string">'__main__'</span>:</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个受限玻尔兹曼机的实例，指定可见层单元数为6，隐藏层单元数为2</span></span></code><code><span class="code-snippet_outer">    r = RBM(num_visible = <span class="code-snippet__number">6</span>, num_hidden = <span class="code-snippet__number">2</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个训练数据的数组，每一行是一个样本，每一列是一个特征</span></span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 这里的数据是一个二值的矩阵，表示6个特征的存在或缺失</span></span></code><code><span class="code-snippet_outer">    training_data = np.array([[<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>],[<span class="code-snippet__number">1</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>],[<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>],[<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">0</span>], [<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>],[<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">0</span>]])</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 调用训练方法，指定最大训练轮数为5000</span></span></code><code><span class="code-snippet_outer">    r.train(training_data, max_epochs = <span class="code-snippet__number">5000</span>)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 打印出训练后的权重矩阵</span></span></code><code><span class="code-snippet_outer">    print(r.weights)</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 创建一个用户数据的数组，表示一个新的样本</span></span></code><code><span class="code-snippet_outer">    user = np.array([[<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">0</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">1</span>,<span class="code-snippet__number">0</span>]])</span></code><code><span class="code-snippet_outer">    <span class="code-snippet__comment"># 打印出从可见层运行网络得到的隐藏层的状态</span></span></code><code><span class="code-snippet_outer">    print(r.run_visible(user))</span></code></pre></section><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(255, 104, 39);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);">输出结果：</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005694" data-ratio="0.44788087056128295" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp1Z9fQxibzL8c84lqpQmU0UA5PsrA4XGBnPo2udX3QAibyJVRgKDBxic3sEjM7NibYCIaMuaK6PWXVkPQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="873" src="20240525_022434_3.jpeg" style="height: auto !important;"/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">5.2 深度玻尔兹曼机</span></strong></span><br/></p><p style='margin-top: 12px;margin-bottom: 0px;user-select: text;word-break: break-word;font-size: 14px;line-height: var(--cib-type-body1-line-height);font-variation-settings: var(--cib-type-body1-font-variation-settings);color: rgba(0, 0, 0, 0.894);font-family: SegoeUIVariable, SegoeUI, "Segoe UI", "Helvetica Neue", Helvetica, "Microsoft YaHei", "Meiryo UI", Meiryo, "Arial Unicode MS", sans-serif;letter-spacing: normal;text-align: left;text-wrap: wrap;background-color: rgba(255, 255, 255, 0.7);'>深度玻尔兹曼机（Deep Boltzmann Machine，DBM）是一种基于能量的生成模型，它可以用来学习复杂数据的概率分布。DBM由多层隐变量组成，每层隐变量之间没有连接，但是每层隐变量都与下一层可见变量或上一层隐变量相连。DBM的最底层是可见层，它表示观测到的数据，例如图像、文本或音频。DBM的目标是最大化数据的对数似然，即让模型生成的数据尽可能接近真实数据。DBM的训练过程涉及到两个阶段：预训练和微调。预训练是使用贪婪逐层算法，将每两层隐变量视为一个受限玻尔兹曼机（Restricted Boltzmann Machine，RBM），并用对比散度（Contrastive Divergence，CD）算法进行无监督学习。微调是使用随机最大似然（Stochastic Maximum Likelihood，SML）算法，对整个模型进行联合优化，以提高模型的泛化能力。</p><p style='margin-top: 12px;margin-bottom: 0px;user-select: text;word-break: break-word;font-size: 14px;line-height: var(--cib-type-body1-line-height);font-variation-settings: var(--cib-type-body1-font-variation-settings);color: rgba(0, 0, 0, 0.894);font-family: SegoeUIVariable, SegoeUI, "Segoe UI", "Helvetica Neue", Helvetica, "Microsoft YaHei", "Meiryo UI", Meiryo, "Arial Unicode MS", sans-serif;letter-spacing: normal;text-align: left;text-wrap: wrap;background-color: rgba(255, 255, 255, 0.7);'>DBM具有以下几个优点：</p><ul class="list-paddingleft-1" style='margin-top: 12px;display: flex;flex-direction: column;gap: 10px;padding-inline-start: 24px;color: rgba(0, 0, 0, 0.894);font-family: SegoeUIVariable, SegoeUI, "Segoe UI", "Helvetica Neue", Helvetica, "Microsoft YaHei", "Meiryo UI", Meiryo, "Arial Unicode MS", sans-serif;font-size: 14px;letter-spacing: normal;text-align: left;text-wrap: wrap;background-color: rgba(255, 255, 255, 0.7);'><li><p>DBM可以从高维、非线性、非高斯的数据中学习出抽象的特征表示，从而实现数据的降维和特征提取。</p></li><li><p>DBM可以用于生成新的数据样本，例如生成新的图像或文本，从而实现数据的增强和创造。</p></li><li><p>DBM可以用于多种任务，例如分类、回归、聚类、协同过滤、推荐系统等，只需在模型的顶层添加一个适当的输出层即可。</p></li></ul><p style='margin-top: 12px;margin-bottom: 0px;user-select: text;word-break: break-word;font-size: 14px;line-height: var(--cib-type-body1-line-height);font-variation-settings: var(--cib-type-body1-font-variation-settings);color: rgba(0, 0, 0, 0.894);font-family: SegoeUIVariable, SegoeUI, "Segoe UI", "Helvetica Neue", Helvetica, "Microsoft YaHei", "Meiryo UI", Meiryo, "Arial Unicode MS", sans-serif;letter-spacing: normal;text-align: left;text-wrap: wrap;background-color: rgba(255, 255, 255, 0.7);'>DBM也有以下几个缺点：</p><ul class="list-paddingleft-1" style='margin-top: 12px;display: flex;flex-direction: column;gap: 10px;padding-inline-start: 24px;color: rgba(0, 0, 0, 0.894);font-family: SegoeUIVariable, SegoeUI, "Segoe UI", "Helvetica Neue", Helvetica, "Microsoft YaHei", "Meiryo UI", Meiryo, "Arial Unicode MS", sans-serif;font-size: 14px;letter-spacing: normal;text-align: left;text-wrap: wrap;background-color: rgba(255, 255, 255, 0.7);'><li><p>DBM的训练过程比较复杂和耗时，需要大量的计算资源和数据量。</p></li><li><p>DBM的训练过程涉及到很多超参数的选择，例如学习率、批量大小、采样步数、正则化项等，这些超参数对模型的性能有很大的影响，但是很难确定最优的值。</p></li><li><p>DBM的理论分析比较困难，很多性质和定理还没有得到严格的证明，例如模型的收敛性、稳定性、可解释性等</p></li></ul><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="font-size: 18px;">5.3 深度置信网</span></strong></span></p><p data-source-line="2057">深度置信网（Deep Belief Network，DBN）是一种基于图模型的生成模型，它由多层受限玻尔兹曼机（RBM）堆叠而成。DBN的最底层是可见层，它表示观测到的数据，例如图像、文本或音频。DBN的最顶层是一个无向图，它表示数据的高层抽象特征。DBN的中间层是有向图，它表示数据的中间层特征。DBN的目标是最大化数据的对数似然，即让模型生成的数据尽可能接近真实数据。DBN的训练过程涉及到两个阶段：预训练和微调。预训练是使用贪婪逐层算法，将每两层视为一个RBM，并用CD算法进行无监督学习。微调是使用反向传播（Backpropagation，BP）算法，对整个模型进行有监督学习，以提高模型的泛化能力。</p><p data-source-line="2059">DBN具有以下几个优点：</p><ul class="list-paddingleft-1" data-source-line="2060" style=";"><li style=";"><p>DBN可以从高维、非线性、非高斯的数据中学习出抽象的特征表示，从而实现数据的降维和特征提取。</p></li><li style=";"><p>DBN可以用于生成新的数据样本，例如生成新的图像或文本，从而实现数据的增强和创造。</p></li><li style=";"><p>DBN可以用于多种任务，例如分类、回归、聚类、协同过滤、推荐系统等，只需在模型的顶层添加一个适当的输出层即可。</p></li></ul><p data-source-line="2064">DBN也有以下几个缺点：</p><ul class="list-paddingleft-1" data-source-line="2065" style=";"><li style=";"><p>DBN的训练过程比较复杂和耗时，需要大量的计算资源和数据量。</p></li><li style=";"><p>DBN的训练过程涉及到很多超参数的选择，例如学习率、批量大小、采样步数、正则化项等，这些超参数对模型的性能有很大的影响，但是很难确定最优的值。</p></li><li style=";"><p>DBN的理论分析比较困难，很多性质和定理还没有得到严格的证明，例如模型的收敛性、稳定性、可解释性等</p></li></ul><p><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(217, 33, 66);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);font-size: 20px;">附录：</span></strong></span></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(61, 167, 66);"><strong><span style="color: rgb(61, 167, 66);font-size: 18px;">受限玻尔兹曼机应用场景</span></strong></span></p><p style="text-align: center;"><img class="rich_pages wxw-img" data-galleryid="" data-imgfileid="100005707" data-ratio="0.45925925925925926" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp1GexxrRyWpiaaVShBYJwrWjJ6a3EgCEV2HTC1UTVINGWWMS2UdDpSt7qP7oibAxoPxB5OQbOXic4ugQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240525_022435_4.jpeg" style=""/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><strong style="color: rgb(61, 167, 66);letter-spacing: 0.578px;font-size: var(--articleFontsize);"><span style="font-size: 18px;">各种激活函数的优缺点</span></strong><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;"></p><p data-source-line="1064" style="letter-spacing: 0.578px;text-wrap: wrap;">各种激活函数各有优缺点，在深度学习中都有其适用场景。</p><ul class="list-paddingleft-1" data-source-line="1066" style="width: 577.42px;letter-spacing: 0.578px;text-wrap: wrap;"><li><p>Sigmoid和Tanh函数是传统的激活函数，具有输出范围有限、优化稳定等优点，但容易过饱和，梯度弥散。</p></li><li><p>ReLU函数是近年来流行的激活函数，具有计算速度快、容易训练等优点，但容易发生“死神经元”问题。</p></li><li><p>Leaky ReLU、ELU和SELU等函数是ReLU函数的改进版本，解决了“死神经元”问题。</p></li><li><p>softmax函数常用于多分类任务，可以用来输出概率分布</p></li></ul><p style="letter-spacing: 0.578px;text-wrap: wrap;"><br/></p><p style="letter-spacing: 0.578px;text-wrap: wrap;"><span style="color: rgb(217, 33, 66);"><strong><span style="font-family: Raleway, sans-serif;letter-spacing: normal;text-align: start;background-color: rgb(252, 252, 252);font-size: 20px;">参考网址</span></strong></span></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="letter-spacing: 0.034em;font-size: var(--articleFontsize);">https</span><span style="letter-spacing: 0.034em;font-size: var(--articleFontsize);">://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/</span></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;">https://github.com/python-pillow/Pillow/ Python 图像库 </span></p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><span style="font-size: var(--articleFontsize);letter-spacing: 0.034em;"><span style="font-size: 17px;letter-spacing: 0.578px;text-decoration: none solid rgba(0, 0, 0, 0.9);"></span></span>https://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/ 受限玻尔兹曼机简介 (echen.me)</p><p style="margin-bottom: 0px;letter-spacing: 0.578px;text-wrap: wrap;"><br/></p><section class="mp_profile_iframe_wrp" style="letter-spacing: 0.578px;text-wrap: wrap;"><mp-common-profile class="custom_select_card mp_profile_iframe" data-alias="ROS_Lab" data-from="2" data-headimg="http://mmbiz.qpic.cn/sz_mmbiz_png/zqic7iaAANTp231ibkvouy3C2eXr6UazHT6pGDRwuWhByPtxvricnIwPqO60qEFcjEymibdJwLLRoGP9WZrzYP84oYg/0?wx_fmt=png" data-id="MzU3OTIyMjgxNw==" data-nickname="十年一梦实验室" data-pluginname="mpprofile" data-signature="分享工业软件、机器人、计算机图形学、高端装备软件算法、动画、智能制造、管理信息系统、机器视觉 相关技术内容。助力国家对复合型人才的培养……"></mp-common-profile></section><p style="letter-spacing: 0.578px;text-wrap: wrap;text-align: center;">The End</p><p><br/></p><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div>

</div>
                <p></p>
                <p></p>
                <div>本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 </div>
                <div  class="hidden">本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 <br><p class="hidden">code/s?__biz=MzU3OTIyMjgxNw==&mid=2247489356&idx=1&sn=1cb8c6c3c82039499b9baee68fd82c6c&chksm=fd683b02ca1fb2147469d76ec754b925f6d3fa2997515b12909301d111dfb3b5b0ed3ab815b5#rd </p></div>
            </body>
            </html>
            