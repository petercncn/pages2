


            
AI的道德与偏见
          




Andrew Ng在AI for Everyone第四周的课程里，开始讲AI的社会与道德问题，他讲了一个技术人员相对容易理解的例子，来解释为什么AI会出现偏见问题。比如，让AI学习并推断：如果Man（男人）是Father（父亲），那么Woman（女人）可以是什么？AI的答案是：Mother（母亲），同样的，如果Man（男人）是King（国王），那么Woman（女人）可以是什么？AI的答案是：Queen（女皇）。这时，一切看起来还蛮正常的。但是，当你问AI，如果Man（男人）是Computer Programmer（计算机程序员），那么女人可以是什么？AI的答案是：Homemaker（全职家庭主妇）。这答案就政治不正确了！但是，AI的答案是从互联网语料库中学习来的。一个简化的模型就是像下面这张图一样，在AI的知识库里，每个词都有一个坐标（这里简化成二维坐标，实际上是更多维度的）。AI预测如果男人是计算机程序员，那么女人可以是什么时，大致上的行为就：是先找到「男人」这个词的坐标（1，1）到计算机程序员（3，2）这个词的路径：向右两步，向上一步。找到「女人」这个词的坐标（2，3）。按照第一步找到的路径，从「女人」这个词的坐标（2，3）出发，向右两步，向上一步。走到（4，4）。看看（4，4）这个坐标的词是什么——全职家庭主妇，于是得出结论。人类社会存在的偏见，以及由于这些偏见积累的数据，造成了AI的偏见。但是，AI并不知道这是偏见。于是，需要人工干预、或者修改模型来纠正这些偏见。甚至于什么是「偏见」，同样的也得人来教他，训练他。就好像小孩一样，在什么环境长大，就学成什么样子，所谓耳濡目染。目前的AI，看起来也是这么回事。其实，从计算机视角看，这些概念的「对」与「错」更简单直接，没有那么多道德意义。无非三点：这过去的经验数据。过去的数据可能是片面、不完整的。所以，结论只是概率，而且也可能是错误的概率。于是，Open AI在ChatGPT的界面上，始终摆着这条提示：反而是人类社会，目前似乎特别纠结「政治正确」这件事儿，我甚至听说学校的老师，因为班上有坚定的、极端的非二元性别论者，在讲生理卫生时百般纠结，小心措辞，还依然要面料被攻击的风险。想只讲科学，都未见得容易。呜呼～不过，话又说回来，科学也曾论证过：这世界本来就是由那些「极端主义者」们推动的——无论这极端观点指向的，是正确方向还是相反。不管怎么说，人类依旧在积极地帮助AI建立「道德」。虽然最初，许多AI生图应用都是在创作成人图片领域「蓬勃发展」。但随着被大众关注，MidJourney 也好，Open AI 的DALL•E 也好，显然都增加了许多「人类社会的道德考量」。当然，从效果上来说，其实做得非常「糙」，体验糟透了。比如，上次我为病毒那篇文章配图时，GPT一开始为我生成的是这样一张图片：太灰暗了，我本意是想要一些明快的水彩画，于是乎让他重新来了一张：明快色彩、梦幻风格的水彩画，宽屏，主题是病毒与人类。表达乐观的态图GPT这才为我生成了之前那张明快的水彩题图：但是，当我想要继续这风格，让他给内容再多配一张下面的图片时：明快色彩风格的水彩肖像画。捧着纸巾盒，感冒了，但是还是很快乐的男人和小孩却报错了：可以自作主张给我画了「病怏怏一家人」，让她画「感冒」的情景，却以内容不「健康向上」为由，被直接被拒绝，实在是难以理解。所以，上次试着画漫画时，GPT死活要让主角穿着衣服泡澡、淋浴，也就情有可原了。想来，这只是AI进化过程中的必然吧！




