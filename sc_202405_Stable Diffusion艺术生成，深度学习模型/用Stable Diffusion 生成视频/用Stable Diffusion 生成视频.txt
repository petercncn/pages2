


            
用Stable Diffusion 生成视频
          




生成耗时：三个小时电脑配置：M1 ProStable Diffusion web UI版本：•  version: v1.8.0  •  python: 3.10.11  •  torch: 2.1.0  •  xformers: N/A  •  gradio: 3.41.2  •  checkpoint: 9c03252bea所需插件：AnimateDiff  、ADetailer运行模型：braBeautifulRealistic_v40.safetensors [9c03252bea]参数配置：night scene, dark, a stunning photo with beautiful saturation by Emily Soto, pretty korean idol woman, (mature:1.3), best quality, sultry, seductive, wet <lora:SD15_noise_offset:1>Negative prompt: black and white, monochrome, (impassive:1.4), featureless, colorless, characterless, teen, young, grotesque, 3d max, desaturated, paintings, sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, skin spots, acnes, skin blemishes, age spotSteps: 15, Size: 512x768, Seed: 3240157613, Model: BraV4, Sampler: DPM++ 2M Karras, CFG scale: 7, Model hash: 9c03252bea, Hires steps: 15, Hires upscale: 1.25, Hires upscaler: ESRGAN-4x, Denoising strength: 0.6Stable Diffusion Web UIStable Diffusion Web UI是一个基于网络浏览器的用户界面，用于展示和演示Stable Diffusion生成式模型的工作原理和效果。这个Web UI提供了一种交互式的方式，让用户可以轻松地与模型进行互动，并观察模型生成的图像结果。功能特点：1. 模型演示：用户可以通过Web UI查看Stable Diffusion模型生成的图像样本，包括图像修复、增强等任务的展示。2. 参数调节：用户可以在Web UI上调整模型的参数，如噪声水平、迭代次数等，以探索不同设置对生成结果的影响。3. 实时预览：用户可以即时查看模型处理图像的过程和结果，在网页上实现动态展示。4. 交互式操作：用户可以与模型进行交互，例如选择输入图像、调整处理方式等，以获得定制化的生成结果。应用场景：* 教学示范：Stable Diffusion Web UI可用于教育和培训目的，向用户展示生成模型的工作原理和应用场景。* 艺术创作：艺术家和设计师可以利用这个UI来生成艺术作品，探索不同风格和效果。* 研究实验：研究人员可以使用此UI进行实验和数据收集，以改进模型性能和探索新领域。通过Stable Diffusion Web UI，用户可以更直观地了解和体验生成式模型的功能，从而促进模型技术的推广和应用。Stable DiffusionStable Diffusion是一种生成式模型，用于处理图像生成和修复任务。它通过多次迭代将图像逐渐模糊，并在每个阶段引入一些噪声，从而产生更加真实和高质量的图像。这种模型结合了扩散过程的思想，利用渐进性的迭代步骤来提高图像生成的质量和稳定性。AnimateDiffAnimateDiff是一个用于视频动画生成的模型，它基于Stable Diffusion框架，但专注于处理动态内容的生成。通过在时间上进行扩散和噪声注入，AnimateDiff能够生成具有连续动态效果的视频内容。该模型可以用于创作艺术动画、视频特效等应用场景。ADetailerADetailer是一个基于图像转换技术的模型，旨在帮助图像细节增强和编辑。通过类似Stable Diffusion的渐进式处理方法，ADetailer可以有效地改善图像质量，增强细节信息，并使图像看起来更加清晰和逼真。这个模型对于图像增强、风格转换等任务非常有用。




