


            
教孩子AI编程003（T5大模型文本摘要）
          




题目利用T5大模型读一段文章，自动生成一段摘要。一、预备环境本次测试环境为windows10环境，python版本为3.8.7。1、安装pytorchpip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cpu/torch_stable.html2、安装transformers库pip install transformerspip install sentencepiece二、编写程序1，在notepad++中编写代码t5.pyimport torchfrom transformers import T5Tokenizer, T5ForConditionalGenerationdef summarize_text(text, max_length=50, min_length=25):    # 加载预训练的T5模型和分词器    model = T5ForConditionalGeneration.from_pretrained("t5-small")    tokenizer = T5Tokenizer.from_pretrained("t5-small")    # 对输入文本进行编码    input_text = tokenizer.encode("summarize: " + text, return_tensors="pt")    # 生成摘要    with torch.no_grad():        summary_ids = model.generate(            input_text,            max_length=max_length,            min_length=min_length,            num_return_sequences=1,            no_repeat_ngram_size=3        )    # 将摘要ID解码为文本    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)    return summary_textif __name__ == "__main__":    text = "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world."    summary = summarize_text(text)    print(f"生成的摘要：{summary}")2、运行程序D:\>python t5.py3、查看结果生成的摘要：the tower was designed by the engineer Gustave Eiffel. it was initially criticized by some of France's leading artists and intellectuals.三、补充说明1、T5大模型T5（Text-to-Text Transfer Transformer）是一种大型神经网络模型，由Google Research的团队开发。T5 是基于 Transformer 架构的一种自然语言处理（NLP）模型。它采用了预训练-微调的两阶段过程，利用大量文本数据进行无监督训练，学习到丰富的语言表示。然后，在具体任务上进行微调，从而实现对各种 NLP 任务的高效处理。T5 的一个关键特性是将所有 NLP 任务统一为一个“文本到文本”的框架。这意味着，对于各种 NLP 任务，输入和输出都被视为简单的文本序列。例如，翻译任务中，输入是源语言文本，输出是目标语言文本；情感分析任务中，输入是文本，输出是表示情感的标签，如 "positive" 或 "negative"。通过这种方法，T5 可以很容易地适应多种不同类型的 NLP 任务，而无需对模型架构进行太多更改。T5 的一个重要贡献是其“大小”：它有多个版本，从较小的 T5-Small 到非常大的 T5-XXL，包含的参数数量不同。大型模型通常具有更多的表示能力，可以在各种 NLP 任务上实现更好的性能，但也需要更多的计算资源和存储空间。T5 在许多自然语言处理任务上取得了显著的成功，例如文本摘要、机器翻译、问答、文本分类等。然而，它的计算需求和训练成本较高，这也限制了其在某些场景下的应用。2、T5大模型和gpt3大模型的区别Chatbot GPT（例如 GPT-3）和 T5 都是基于 Transformer 架构的大型自然语言处理模型。它们在许多自然语言处理任务上都表现出色。然而，它们之间存在一些关键区别：任务框架：T5 将所有 NLP 任务统一为一个“文本到文本”的框架，这意味着输入和输出都被视为简单的文本序列。这使得 T5 能够在不修改模型架构的情况下处理各种不同类型的 NLP 任务。而 GPT 系列模型主要关注生成式任务，使用自回归的方式生成文本，通常需要根据具体任务进行一定的修改。预训练目标：T5 使用的是“降噪自编码”任务来进行预训练，这意味着模型需要从输入中删除或重新排序一些文本片段，然后在输出中恢复这些片段。这有助于 T5 学习到更丰富的语言表示。GPT 使用的是单向自回归预训练任务，从左到右生成文本，只能看到之前的上下文。方向性：GPT 是一个单向模型，它在生成文本时只能从左到右查看上下文。这意味着 GPT 在生成某个词时，不能访问该词右侧的上下文信息。T5 是一个双向模型，它可以同时访问左侧和右侧的上下文信息，从而更好地理解句子中的关系和依赖。模型大小和版本：T5 和 GPT 都有多个版本，从较小的模型到非常大的模型。大型模型通常具有更多的表示能力，可以在各种 NLP 任务上实现更好的性能。然而，它们的计算需求和训练成本也相应增加。尽管这些模型在结构和预训练方法上有所不同，但它们在实际应用中的性能差异可能取决于具体任务和数据。在实际使用中，可能需要根据任务需求和计算资源来选择最适合的模型。3、T5大模型和gpt3大模型的相同点T5 大模型和 GPT-3 大模型之间有一些共同点：基于 Transformer：两者都基于 Transformer 架构，这是一种非常强大的自注意力机制，用于处理自然语言任务。Transformer 架构使得这两种模型能够捕捉长距离依赖关系和语义信息。大型预训练模型：它们都是通过在大量无标签文本数据上进行预训练来学习丰富的语言表示。这使得它们在许多自然语言处理任务上具有很高的性能。知识蒸馏：大型模型通常有很高的计算需求，可能不适用于实际部署。因此，通常通过知识蒸馏技术将这些大型模型的知识压缩到较小的模型中，以便在实际应用中使用。微调：虽然 T5 和 GPT-3 在预训练时学习了丰富的语言表示，但它们通常需要针对特定任务进行微调。微调时，模型在有标签数据上进行有监督学习，以便在特定任务上获得最佳性能。生成能力：T5 和 GPT-3 都具有生成自然语言文本的能力，它们可以生成连贯且看似人类编写的文本。这使得这两个模型在聊天机器人、摘要、翻译等生成任务上表现出色。




