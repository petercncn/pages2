


            
小语言模型SLM是可以单机部署脱网使用的语义语言模型
          




​在过去的一年中，大语言模型，也就是LLM，取得突破性的发展。而目前的大语言模型，本质上是一个基于统计的语用模型。具体的技术路线就是，根据之前的上文，给出下文的一个词，并递归循环这个操作。在很多时候，语用正确，语义可能就是正确的。但有时候，语义可能错误的。基于语义的关系生成文本，是一个更好的目标。但目前的研究和技术路线，还没有找到合适的方法和路径。而基于语义的语言模型，其核心，并不需要大语言模型的庞大规模。也就是可以通过小模型实现。现实中，人类也更需要小语言模型。因为小语言模型才可以单机或者单芯片存储运行。可以在脱离网络的情况下，成为机器人和自动驾驶的大脑。因此，在大语言模型继续发展探索甚至走向更大规模这条道里之外，小语言模型，一定会成为另一个重要的道理和目标。小语言模型，英文缩写应该是SLM。看到这个缩写，我忽然开心。这正是我的名字的缩写。而小语言模型，也正是我基于语义语言模型和针对大语言模型的质疑提出的目标和概念。SLM。嗯。一个巧合。但小语言模型却是一种必然。机器人和自动驾驶以及军事单位，必然需要单机部署了脱网工作的语言模型构成的机器大脑。而能够将大语言模型规模压缩的一个重要的思路和路径就是，训练语义模型。而非目前的语用模型。语用，关心的是一个词语被如何使用。在如何的情况下被如何的使用。目前的大语言模型上下文接龙，实际上就是一个御用问题。通过统计学，形成的语用概率向量。大语言模型的幻觉，捏造的不实信息，和逻辑推理能力的缺陷，都是因为这个语用基础的固有问题。语用部分等效于语义。但语义要更复杂得多。而语义模型，不需要大语言模型那样的庞大参数。因为上下文是一个一维的数学关系。语义本身是一个高纬度的网络关系。用高纬度的网络指征高纬度的语义网络，是高效的。我非常乐意用SLM命名这个小参数的语义网络或者说语言模型。




