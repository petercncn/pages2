


            
#107 CT重建的3D深度学习技术——深度学习图像重建算法分类
          




日更105天Lotus周六，周日，周一更新放射学前沿全文字数8843字，大量图片和概念，阅读需要15min。了解当前CT扫描重建中三维深度学习的最新方法对研究人员和从业人员都至关重要。2023年发表在Tomography的文章通过广泛的文献调查，为研究人员、临床医生以及对深度学习与医学影像交叉领域感兴趣的任何人提供了宝贵的资源。Lotus将会为各位读者带来含有Lotus查阅了大量资料的，关于这篇CT重建算法的综述的系列文献解读：#098 有趣的深度学习基础#099 如何进行3D深度学习相关文献的检索#100 60篇深度学习重建算法文章总结#104 CT重建算法基础讲解#105 迭代重建算法今天我们来介绍深度学习重建算法的分类。Part1深度学习方法学深度学习使用的算法的特点是使用多层神经网络。这种网络是一种独特的人工神经网络（artificial neural network，ANN），类似于人类的多层认知系统。人工神经网络由数百个基本计算单元组成，将输入信号和所需的输出决定成对呈现，采用反向传播等学习等算法，以训练网络的权重。这模仿了大脑利用外部感官刺激来学习完成特定任务的过程。人工智能（artificial intelligence，AI）在医学影像领域的蓬勃发展表明了人们对这一课题的浓厚兴趣。因此，我们需要了解有关人工智能和深度学习（deep learning，DL）原理的背景知识。与 DLIR 相关的原理包括特征工程节点层前向和反向传播权重调整训练、验证和测试我们分别来看。1特征工程如果把整个人工智能想象成做饭的话，在深度学习领域中，原始数据就像一堆杂乱的食材，不能直接下锅。因此，特征工程（feature engineering，FE）就是对这些数据食材进行预处理，把杂乱的数据转化成计算机能够理解的资料的过程。特征工程是模型与数据之间的桥梁。和厨师处理食材一样，所有的下锅之前的步骤都属于预处理的范畴，在深度学习领域内，所有的数据喂进模型之前的动作都属于特征工程。典型的特征工程的动作有：探索性分析和做菜一样，在动手之前需要先观察自己的食材。探索性分析（exploratory data analysis，EDA）本质上就是刚拿到数据，对数据的整体规模、结构、数据类型、数据分布、数据之间的关系等还不清楚，需要进行简单的理解。在CT重建算法的构建当中，当我们拿到手了一大堆原始数据之后，我们需要先了解一下我们拿到的都是些啥，比如说我们需要看一下：原始数据是什么格式的？有多大？原始数据的头文件里存储的都是些什么变量？这些变量都是什么类型的？是整数？小数？类型？进一步的，为了更好的了解我们的数据，我们可以简单的做一下统计我们拿到的这批数据里面的变量是怎么分布的？是均匀分布？是正态分布？变量和变量之间是不是有什么关系？或者绘制一下图来看一下：散点图看一下不同组的变量是否有扎堆儿的情况？不同的组绘制直方图了解一下数据组和组之间的分布是否均匀？等等。数据清洗和做菜一样，在下锅之前我们需要清理食材。为了方便大家理解，Lotus这里用大家更熟悉的影像组学的数据处理来为大家解释。在深度学习领域的数据清洗（data cleaning）主要包括这样5类：缺失值处理比如说某一位病人的基础信息忘了记录了，这样的话可能的处理方式有排除掉这位病人（删除这一行），或者用群体病人的均值或者中位数来补全这位病人的信息。重复值处理同一个样本被记录了多次，通常的处理方式为删掉重复的条目，只留下一条。异常值处理比如说这里记录病人的年龄，出现一个120，那就要去看一下心率或者是血压，很有可能填错地方了。数据格式转换对于病人的性别来说，“男”和“女”是计算机不认得的，需要将男女这种类别转化为男=0，女=1来进行处理才可以，这就是一种数据格式的转换。常见的还有整数和小数之间的格式转换等。规范化数据比如说记录病人的体重，有可能记录的单位不是kg，而是斤，那么就需要进行除以2来得到国际单位制的体重。当然了，这里还涉及到特征的归一化等，比如常见的能谱参数图，为了更好地进行分析会使用标准化碘浓度作为一个有用的分析特征。数据增强在对数据进行分析的时候，很多时候现实的数据分布具有长尾性，就比如说在深度学习当中，我们希望深度学习模型着力解决的是平常FBP重建解决不了的“有缺陷”的图像。但是常规采集来说，通常遇到的都是FBP没问题的“健康”数据，FBP缺陷图像的占比非常少，而且，“幸福的家庭总是相似的，不幸福的家庭各有各的不幸。”“健康”数据总是相似的，“有缺陷”的图像，大家的问题都不太一样。如果按照数据的分类做直方图的话大概是这样：可以看到，不同类别的数据量分布非常不均衡，这样的数据分布就叫具有长尾性，这时候我们需要想办法在数据集中放大这种“有缺陷”，这么一个放大的过程就叫数据增强（data augmentation）。对于图像来说，常用的数据增强方式包括，拉伸，裁切，旋转，镜像，增加噪声等。数据表征就像做菜一样，当准备好了菜之后，我们需要把菜进行规范化的处理，该切丝切丝，该切块切块，方便后面能够放在锅里炒。在数据表征（Representation）这里，也是一样的，需要将数据处理为计算机能够处理的特征（feature）。我们来看wiki百科上对深度学习当中的数据表征（Representation）的定义，网络表征学习（Representation Learning on Network），一般说的就是向量化（Embedding）技术，简单来说，就是将网络中的结构（节点、边或者子图），通过一系列过程，变成一个多维向量，通过这样一层转化，能够将复杂的网络信息变成结构化的多维特征，从而利用机器学习方法实现更方便的算法应用。想详细了解数据表征（Representation）的概念的同学，详见#098相关部分。特征组合在一些工作做的很细的工程师这里，还会根据自己的经验对特征进行一些组合处理。比如说如果我们有了病人的身高和体重，我们在模型中可能还会再根据病人的身高和体重计算一个BMI作为一维特征，BMI就是身高和体重这两个特征通过特征组合（feature crosses）得到的新的特征。通常情况下，机器学习算法模型，如决策树，线性回归等比较依赖特征组合，深度学习建模中很少进行特征组合的处理。特征降维在深度学习模型当中，通常遇到的更常见的问题是特征太多了，所以对于这种特征过多，需要的算力过于庞大的情况往往需要进行特征降维。比如说我们的CT图，CT图每一个像素点的取值范围为[-3000, +3000]，如果给每一幅图像的每一个像素点都准备这么大的空间来存，可想而知对于计算机的硬件是个巨大的挑战。因此，对于一些希望用“小马拉大车”的实验环境的模型开发来说，会将CT图的像素取值范围缩放到普通图像的[0,255]，进行特征降维。对于能够处理的特征更少的机器学习而言，如果希望能处理非常大的维度的数据的话，可以使用组成成分分析（principal component analysis，PCA），线性判别分析法（linear discriminate analysis，LDA）等常见的降维方法选出最主要具代表性的特征进行建模。2节点和层首先让我们来看一个典型的人工神经网络，如下图所示：层（Layer）可以看到这是一个5层的神经网络：第一层：输入层（Input Layer），即A[0]层。最后一层：输出层（Ouput Layer），即A[4]层。中间3层，隐藏层（Hidden Layer），即A[1]，A[2]，A[3]层如果把神经网络看成一家公司的话，输入成代表一线小兵，输出成代表董事长，那么在纯干活儿（输入层）的和董事长（输出层）之间的这个汇报层级关系就叫做神经网络的层（Layer）。目前随着深度学习网络的架构越来越复杂，出现了各种各样的模块（module）和区块（block），很多时候一个模块（module）/区块（block）中内嵌了许多层神经元，但是为了方便也当作一个层。比如说母公司旗下有很多负责具体事项的一连串子公司，子公司里有自己的人事汇报关系。但是为了简便形式，可以把一个子公司称作为为一层。节点（Cell）图片上的每一个圆圈被称为一个节点，因为人工神经网络模拟的是人类大脑的结构，所以节点也叫神经元（cell）。在人体中，典型的神经细胞结构如下：蓝色的5只小爪爪叫做突触，树突可以收到来自上一个神经元或者环境给予的神经刺激作为输入。中间的蓝色五边形部分为神经元的细胞体，负责综合树突上的神经刺激。接着神经刺激进入红色像香肠一样的轴突，如果超过了轴突的传导阈值，则这个神经刺激会接着往中枢神经的方向进行传递。比如说这是一个疼痛的信号，如果只是自动铅笔扎手了一下，你的轴突会判断，多大点事啊，没超过你轴突的传递阈值，疼痛的信号就不往大脑送了。顶多传到脊柱，让你抬手别手欠往笔上戳就完事了。如果是大头钉扎穿了，这个刺激一下给的很猛，轴突不敢半路截胡，一路飞快传递到大脑。人工神经网络中的神经元/节点（cell），类似于高等动物的神经系统中的神经元，长这样：与人的神经元工作模式差不多，神经元也接受前面细胞或者输入层传过来的输入信号，并对不同来源的信息配置权重之后进行【加和】处理，赋权加和之后传给轴突——激活函数（activation function），如果超过激活函数的阈值则向下传递，如果没有超过激活函数的阈值则不传递。Lotus在【加和】的地方打括号的意思是最普通的神经元这里进行的是加和操作，更复杂的神经元这里会对输入进行更复杂的操作，如卷积等。不同模式的神经元（cell）和不同模式的激活函数（activation function）是目前数学家们，准确点来说是统计学家们，重点研究的领域之一。卷积神经核在神经网络的不断发展当中，这个神经元有不同的模式，就好像人类的神经细胞也有非常多不同的样子一样，比如说我们人类的眼睛里面的神经元是这样式的：其中不同颜色的视锥细胞，对不同颜色的光敏感度不同。比如说图上画的蓝色的为对短波长（蓝色）敏感的S型视锥细胞，对中波长（绿色）敏感的M型视锥细胞，对长波长（红色）敏感的L型视锥细胞。可以看到这三种视觉细胞都很单一，都是单通道进入，单通道向后传递的。研究表明，人类的视觉细胞能够化繁为简，高度抽象结构。在深度学习当中，为了尽可能地模拟人类视觉神经细胞的这一特点，出现了能够把复杂的如同摊开一把绿豆一样的图像，像小手一样抓起来的——卷积神经核。关于卷积神经网络的通俗解释详见#076卷积神经核的工作方式如下图所示：递归神经核除了模拟人类视觉的神经元之外，还有模拟人类记忆的深度学习特种神经元，——递归神经网络（recurrent neural network，RNN），也叫循环神经网络。典型的递归神经网络架构如下所示。可以看到每一个神经元除了向上汇报之外，还向后汇报。这样的“往前传话”的结构使得后面的神经元具有了前面神经元传递过来的“记忆”。下面是典型的RNN的经典神经元——长短时记忆（long short time memory）神经元。可以看到相比于常规神经元来说，在RNN神经元当中多了一个遗忘门（Forget gate），和人的记忆一样，遗忘门对前面一个神经元传递过来的信息进行判断，如果觉得重要，则汇总汇总接着往后传，如果觉得“这啥破玩意”，则会把这个信息抛之脑后。就比如说如果你昨天大婚，你的大脑判断一下觉得这个信息很重要，于是今天的你还会记得你昨天大婚，明年的你则会记得你今年大婚。在比如你今天早晨吃的早餐，你判断了一下，觉得记这破玩意干啥，于是现在估计你就已经忘了你早晨吃了啥了。你上午的神经元就没有把这个信息顺着时间往后传递。关于递归神经网络的通俗解释详见#076前向传播，反向传播和权重调整如果把神经网络当成一家公司的话，前向传播（Forward Propagation）的过程就是一个计算公式总盈利的过程，每个一线小兵把自己今年的收入，乘以上自己的权重，逐级汇报往上，最终聚集到大老板那里，计算得到一个总的输出。反向传播（Backward Propagation）的过程就是一个甩锅的过程，大老板先接一个甲方给的大锅，这个大锅对应到深度学习当中叫做损失（loss），和现实生活中一样，损失 等于 目标与实际 的差值。在深度学习当中，锅这个玩意，同样秉承一个不上称四两夜没有，上称了千金也打不住的思想。有一伙数学家，专门研究怎么制定合理的损失函数（loss function），不管怎么样吧，按照某种规则，把合适大家背着的锅弄出来了，然后锅通过高层，中层层层往下去找，去摊派，最终传递到一线小兵这里，才能让小兵积极调整，完成所谓的“变革”，真正的把高层的“优化”战略落地。在这个制定“优化”策略的动态调整过程中，可以发现对于人事的任命一定是有架构上的调整的，通常是，小兵（输入层）和大老板保（输出层）持不动，中间的中层高层领导（隐藏层），你方唱罢我登场，这个不断优化，不断调整隐藏层神经元权力结构的过程，就叫做权重调整。这些在网络里能够自动通过利益的汇总和权利的分配调整的权重（weight），加上每一层还会有一个随着模型调整而自动调整的常量，也叫偏置（bias），在这Lotus多说一嘴，参数的关键词为：自动。强调这个是因为等下我们还要提一个超参数的概念。神经网络中的权重（weight）+每一层的偏置（bias） = 神经网络的参数（parameter）。大家在各个厂家的广告中看到的，海量，百亿级参数，说的是这个东东。3如何实现深深的网络在权重调整过程中Lotus给大家介绍几个概念，在组织当中，现在公司都流行扁平化管理，原因就是你会发现如果中间的管理层太长了，不利于“分锅”。有在组织非常臃肿的地方上班的同学应该能感受到，这可能都不光是不利于“分锅”了，你一个链路上有10个人，一句完整的话，从第一个人传到第10个人，很可能第一个人说的是“贾宝玉大观园寻花问柳。”传到最后一个人变成了“川普大帝许愿越喝越有”传话都成问题了，就别说传锅了。在深度学习当中，也是一样的，隐藏层的层数越多，理论上能够分析问题的复杂度越高，然而隐藏层的层数如果过多，会导致梯度消失和梯度爆炸的问题。解决这个现象的方法也不复杂，在组织中，只要通过让高层和低层绕开中层直接对话就避免了越传越离谱的问题了，在深度学习算法模型当中，也是通过这样的by pass方案来实现越来越深的网络架构。这样高层大boss与低层小兵直接对话的算法架构，被称为残差块（residual block），残差块（residual block）的结构如下：当然了我们要对这种高层和低层直接对话的形式活学活用，比如说我们历朝历代逐渐形成的特使，东厂西厂，锦衣卫等等。在深度学习当中，高层领导和我等p民直接喊话可以是这样的（上图来自于Google的InceptionNet）还可以是这样的当然了，有了合适的组织架构，公司的重点是要变，是要把上面的命令（锅）有效的传导下去，让子弹飞里面说的特别好，“我以为，酒要一口一口地喝，路要一步一步走，步子迈得太大，会扯着蛋。”在权力结构调整的过程中，要是公司初期还好，要是组织已经运行个千八百年了，各个利益集团已经根深蒂固了。削藩或者赋权都需要少量多次，不能步子太大了，左一下，右一下的。当然了步子不能太大，步子也不能太小，太小了等于你改革了半天还在原地墨迹。这个权利结构调整的步幅在深度学习里被称作为学习率（learning rate），目前有一大票数学家研究怎么搞合适的学习率出来。总的原则和成立公司一样，都是一个先扯开步子改，再小碎步慢慢调整的思路。具体调整多少轮在深度学习中被叫做迭代次数（epoch）。像上述所说的管理层的层数（layer）和每层的人数（cell），包括改革的步频（learning rate）和轮数（epoch），包括上面提及的那口往下分配的锅（loss）里面的一些参数，都是董事会依据过往经验拍脑门定的，这种需要各位工程师拍脑门定的参数，在深度学习里，区别于自动调整的参数，叫做超参数（hyper parameter）。4训练、验证和测试训练集，验证集和测试集的比较说明如下图所示：标注的数据集（labeled data）深度学习作为有监督学习的一种，有监督学习是啥请详见#099的相关内容，数据集的构造非常重要。用于有监督学习的数据集由两部分组成，一部分为数据本身，一部分为数据的标签。对于CT重建算法来说，用于训练深度学习CT图像重建算法的数据通常由：待提升的数据（有可能是原始投影域sinogram数据，也有可能是图像域image数据）原始投影域sinogram数据和图像域image数据是啥请见#104相关内容解读。和待提升的数据配着对的高质量图像域image数据。组成。就像儿童的老师很重要一样，对于有监督学习，如深度学习模型，来说，精心构造的数据集非常重要！一方面，和教育小孩一样，我们希望小孩见多识广，因此构造的数据集要尽可能的庞杂，这是因为只有数据集尽可能地覆盖模型在实际应用中遇到的各种情况，才能使得模型尽可能地学习到完整的潜藏在数据内部的隐藏模式和规律。另一方面，和教育小孩一样，上梁不正下梁歪，因为无论我们使用多大的数据集，被用于构建模型的数据总是真实世界数据的一部分。因此，用于训练的数据集的标签要尽可能的精确、完善。不然就会像小孩上学一样，教的时候就是错的，指望着小孩自己悟道，那就太难为孩子了。在后续的模型开发中，会将上述数据分成独立的三份（如果数据量足够大的话）。标注的数据集（labeled data） = 训练集（training data）+ 验证集（validation data）+ 测试集（test data）训练集（training data）训练集（Training Set）指用于训练模型的数据集。相当于小孩上课的时候，老师拿来讲解的题目，训练集（Training Set）用于模型的训练阶段使用，通过大量的前向传播，反向传播，自动进行模型内的权重（weight）以及偏置（bias）的调整。“训练”的目标是得到在训练集上loss最小的时候的模型参数（parameters）。如果数据量巨大（million），通常会留2w的验证和测试用数据，其余的数据均用于训练。，如果数据量足够大（w），通常训练集占的60%。如果数据量不够大，那么训练集+验证集占总数据集的80%，采用多折交叉验证的方式构造验证集。在训练的过程中，耗费的时间和算力通常比较高。比如说打败李世石的AlphaGo在48个TPU（一种高性能计算芯片，计算能力相当于30-80倍的CPU）训练了小半年。总共自己跟自己左右互搏，下了三千多万局。为了让大家有个直观对比，Lotus查了一下，一盘围棋大概需要1-2h，取中间1.5h。人类完成这样的“训练”大概需要不吃不喝不眠不休5137年。差不多从盘古开天辟地那会儿一直下棋下到现在。验证集（validation data）验证集（validation data）通常用于验证超参数（hyper parameter）。超参数（hyper parameter）详见上面如何得到深深的网络一节。如果数据量巨大（million），通常会留1w数据用作验证集，1w数据用作测试集，其余的数据均用于训练。，如果数据量足够大（w），通常训练集占的60%，测试集和验证集各占20%。如果数据量不够大，那么训练集+验证集占总数据集的80%，采用多折交叉验证的方式构造验证集。下图为5折交叉验证的流程：测试集（test data）如果构建了不同的模型，则测试集（test data）用于对比不同模型之间的表现。如果只是构建了单一模型，则使用测试集来评估模型的泛化性能。在训练流程上，训练集，验证集和测试集的关系如下所示：Part2CT重建算法中的深度学习网络结构如下图所示可以看到，按照深度学习方法学分类使用不同技术发表的文献如下图所示。发表的篇目详见#100 60篇深度学习重建算法研究。5卷积神经网络，CNN在广泛的计算机视觉应用中，关于计算机视觉任务是啥详见#076卷积神经网络（ convolutional neural network，CNN）已成为一种主要的深度学习模型。鉴于其从 CT 图像中学习分层特征的效率，CNN 在计算机断层扫描重建中发挥着至关重要的作用。CNN 通过应用多个可训练的滤波器，即卷积核（卷积核是啥详见上面的节点一节所述），对输入数据的结构维度进行卷积，从而分析计算断层成像重建中的输入数据。通过这种卷积过程，可从计算断层成像数据中提取局部模式和特征 。通过堆叠多个卷积层（如何大量堆叠请见上面的如何实现深深的网络一节），CNN 可以学习特别适合新增断层成像重建任务的表征，从而从输入数据中捕捉到越来越复杂和抽象的方面。CNN 的分层结构使其能够学习不同抽象层的特征。如下图，可以看到深度学习模型关注到的区域（b中蓝色的热力图部分），以及从关注区域抽取出的抽象特征（c图中的关注的地方的结构）以及后续的，对分类（e图中不同的颜色代表计算机认出来的不同类别的物体）的决策依据的图像重点（d图代表在判断图片里具体有啥的时候，卷积神经网络聚焦图像上的内容）这里比较有意思的是图二的汽车，可以看到对于计算机来说，和咱们人类差不多，上来关注的是车上的贴纸（图b和图c）。但是看到这张图片的分类为车轮，当决策这张图片的分类的时候，看到计算机的关注重点挪到了右侧的车轮上（图d）CNN的高层可以学习更复杂、更有意义的表征，而低层则倾向于捕捉更简单的元素，如边缘和纹理。由于具有提取分层特征的能力，CNN 非常适合用于计算机断层扫描重建，因为CNN可以识别容积 CT 图像中的重要结构和模式。63D卷积神经网络，3D CNN三维卷积神经网络（3D convolutional neural network，3D CNN）可用于三维计算机断层扫描等体积数据，是传统 CNN 的有力扩展。与单独分析二维图片的典型 CNN 相比，3D CNN 通过考虑容积数据的整个三维环境，可以有效捕捉计算断层扫描中存在的空间相互依存性和联系。3D CNN 增强了把握不同区域和结构之间复杂互动的能力，从而提高了重建方法的精度和鲁棒性。有关三维深度学习在计算机断层扫描重建中的应用的研究论文经常将3D CNN 作为最先进的模型架构进行研究。这些研究显示了使用3D CNN 进行重建的好处，以及它们识别 CT 图像中复杂模式和特征的能力。3D CNN 通过使用数据的容积部分来包含上下文联系和空间信息，从而提高了重建的准确性。由于数据的多维性，3D CNN 可以识别和理解相邻切片之间的联系，从而提高完整容积重建的质量。当感兴趣的结构跨越多个切片或具有复杂的空间排列时，这种能力尤为重要。7其他深度学习算法除了CNN之外，在已经公开的方法学文章中还使用了基于CNN或者3DCNN的复杂变体，如：变异自动编码器（variational autoencoders，VAE）VAE的网络架构如下图所示，可以看到整体网络由一个encoder和一个decoder组成，关于encoder和decoder是啥详见#99的相关解读。生成对抗网络（generative adversarial networks，GAN）一些医学领域用的GAN架构如下图所示，可以看到整体网络由一个Generator和一个Discriminator组成，关于Generator和Discriminator是啥详见#99的相关解读。递归神经网络（recurrent neural networks，RNN）典型的单向的RNN网络如下图所示：其中每一个神经元的结构都为递归神经元（详见本文节点一节）先行的双向RNN网络结构如下图所示：目前处理时序数据的大语言模型架构如图所示：Trm代表Transformer Layer，Transformer结构的讲解详见#100相关内容。Lstm为长短时记忆（long short time memory）神经元，（详见本文节点一节）上述复杂的基于CNN或者3DCNN的复杂变体可以应对医学影像挑战，提高医学影像分析的准确性、效率和质量，展示了该领域的动态性和创新性。商用的深度学习图像重建算法（Deep Learning Image Reconstruction，DLIR）是一类多用途方法，它整合了各种方法，包括卷积神经网络（CNN）、3D卷积神经网络（3D CNN）、生成对抗网络（GAN）、变异自动编码器（VAE）和递归神经网络（RNN）。许多综述论文证明了，目前出现的商用的深度学习图像重建算法（Deep Learning Image Reconstruction，DLIR）作为计算机断层扫描重建中三维深度学习的综合框架，很好地将多种深度学习算法模型中的框架统一起来，以提高图像质量、减少辐射剂量，并能够改善整体重建的效率和性能。参考资料：R. Clough, A.I. Kirkland, in Advances in Imaging and Electron Physics, 2016《The Fundamentals of MTF, Wiener Spectra, and DQE》Robert M NishikawaSEERAM E. Dose Optimization in Digital Radiography and Computed Tomography: An Essential Guide[M/OL]. Cham: Springer International Publishing, 2023.Jerry C. Whitaker (27 April 2005). The Electronics Handbook, Second Edition. CRC Press. p. 636. ISBN 978-1-4200-3666-4.Rahman H, Khan AR, Sadiq T, Farooqi AH, Khan IU, Lim WH. A Systematic Literature Review of 3D Deep Learning Techniques in Computed Tomography Reconstruction. Tomography. 2023; 9(6):2158-2189.EULER A, SOLOMON J, MARIN D, et al. A Third-Generation Adaptive Statistical Iterative Reconstruction Technique: Phantom Study of Image Noise, Spatial Resolution, Lesion Detectability, and Dose Reduction Potential[J]. American Journal of Roentgenology, 2018, 210(6): 1301-1308.KOETZIER L R, MASTRODICASA D, SZCZYKUTOWICZ T P, et al. Deep Learning Image Reconstruction for CT: Technical Principles and Clinical Prospects[J]. Radiology, 2023, 306(3): e221257.欢迎转载和抄袭，赞赏一下，喜欢就说是你写的。如果喜欢就点点点赞，转发，在看或者在下面留言吧。创作不易，请Lotus喝盏茶～




