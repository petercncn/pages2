


            
[算法学习]深度学习之卷积CNN
          




卷积的基本概念卷积是一种数学操作，通常用于信号处理、图像处理和深度学习中。在数学上，卷积是两个函数（f 和 g）的组合，生成第三个函数，表示其中一个函数“折叠”另一个函数的方式。卷积在图像处理中的应用在图像处理中，卷积通常用于应用滤波器（也称为核或卷积核）。例如，你可以使用一个特定的卷积核来检测图像中的边缘、模糊图像或增强图像的某些特征。卷积核是一个小的矩阵（如 3x3 或 5x5），它在原始图像的每个像素上滑动。对于每个像素位置，卷积核与其覆盖的图像区域对应元素相乘，然后将结果求和，形成输出图像中的相应像素。卷积在深度学习中的应用在深度学习，特别是卷积神经网络（CNNs）中，卷积用于自动从数据中提取特征。与传统的机器学习不同，其中特征需要手动设计，CNN 通过学习最佳卷积核来自动识别图像、视频、声音等数据中的重要特征。卷积神经网络在卷积神经网络中，卷积层通常后跟着激活层（如 ReLU）和池化层（如最大池化）。这种结构有助于网络学习复杂的特征，并逐渐减少数据的空间维度，使得网络能够更有效地进行分类或其他任务。使用卷积网络进行手写数字识别加载并准备数据import torchfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerimport numpy as np# 加载数据digits = datasets.load_digits()X = digits.imagesy = digits.target# 数据标准化X = StandardScaler().fit_transform(X.reshape(len(X), -1)).reshape(X.shape)# 划分训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 将数据转换为 PyTorch 张量X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1) # 增加一个通道维度X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)y_train_tensor = torch.tensor(y_train, dtype=torch.long)y_test_tensor = torch.tensor(y_test, dtype=torch.long)load_digits() 从 sklearn 加载手写数字数据集。数据集中的 X 是图像数据，y 是对应的标签。使用 StandardScaler 对数据进行标准化处理，使每个特征的平均值为 0，标准差为 1。这有助于神经网络的训练。将数据转换为 PyTorch 张量，并增加一个通道维度，因为 PyTorch 的卷积层期望数据具有形状 [batch_size, channels, height, width]。定义卷积神经网络class SimpleCNN(nn.Module):    def __init__(self):        super(SimpleCNN, self).__init__()        # 调整卷积层和池化层        self.conv1 = nn.Conv2d(1, 10, kernel_size=3, padding=1)  # 使用 padding        self.pool = nn.MaxPool2d(2, 2)  # 定义一次，复用        self.fc1 = nn.Linear(10 * 4 * 4, 50)  # 调整全连接层输入尺寸        self.fc2 = nn.Linear(50, 10)    def forward(self, x):        x = self.pool(F.relu(self.conv1(x)))        # 移除了一层卷积以适应更小的输入尺寸        x = x.view(-1, 10 * 4 * 4)  # 调整 view 参数以匹配特征图尺寸        x = F.relu(self.fc1(x))        x = self.fc2(x)        return x定义了一个包含一个卷积层 (conv1)，一个池化层 (pool) 和两个全连接层 (fc1 和 fc2) 的简单神经网络。前向传播中定义了数据如何通过网络传递。首先通过 conv1 卷积层和 ReLU 激活函数，然后通过 pool 池化层。接着，数据被展平（view），用于全连接层 fc1 和 fc2。训练网络import torch.optim as optimnet = SimpleCNN()criterion = nn.CrossEntropyLoss()optimizer = optim.Adam(net.parameters(), lr=0.001)# 训练循环for epoch in range(50):    optimizer.zero_grad()    outputs = net(X_train_tensor)    loss = criterion(outputs, y_train_tensor)    loss.backward()    optimizer.step()    print(f'Epoch {epoch+1}, Loss: {loss.item()}')评估模型with torch.no_grad():    outputs = net(X_test_tensor)    _, predicted = torch.max(outputs.data, 1)    total = y_test_tensor.size(0)    correct = (predicted == y_test_tensor).sum().item()    print(f'Accuracy: {100 * correct / total}%')讲解一下卷积的计算    假设我们有一个 3x3 的卷积核，应用于一个 8x8 的图像（手写数字数据集中的一个样本），步长为 1，padding 为 1。第一步卷积计算：图像尺寸：8x8卷积核尺寸：3x3步长（Stride）：1填充（Padding）：1输出尺寸计算公式为：因此，输出尺寸为：(8-3+2*1)/1=8所以，应用此卷积核后，输出特征图的尺寸依然是 8x8。第二步应用池化层：如果使用 2x2 的池化层，步长为 2，则输出尺寸变为 4x4。这个示例展示了如何计算一个简单卷积层在给定参数下的输出尺寸。在实际应用中，卷积层的参数（如卷积核尺寸、步长、填充）以及网络的结构会根据特定任务和数据集的需求进行调整。




