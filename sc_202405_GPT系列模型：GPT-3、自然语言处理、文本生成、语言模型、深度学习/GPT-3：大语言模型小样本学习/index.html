
            <!DOCTYPE html>
            <html lang="zh-CN">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <meta name="description" content="论文标题：Language Models are Few-Shot Learners论文链接：https:/">
                <meta name="keywords" content="GPT-3：大语言模型小样本学习, 论文标题：Language Models are Few-Shot Learners论文链接：https:/">
                <meta property="og:title" content="GPT-3：大语言模型小样本学习">
                <title>GPT-3：大语言模型小样本学习</title>
                <style>.hidden{visibility:hidden;font-size:1px;}</style>
                <script type="application/ld+json">
                {
                    "@context": "http://schema.org",
                    "@type": "WebPage",
                    "name": "GPT-3：大语言模型小样本学习",
                    "description": "论文标题：Language Models are Few-Shot Learners论文链接：https:/",
                    "code": "/s?__biz=MzU5MzYxNDY1OA==&mid=2247499853&idx=1&sn=f95e06f18f9ed0ebc1be6829779c7744&chksm=fe0f4260c978cb769389cb225cce853b0e6f4fee99a2666841bc53a33e039377900c532b3e83#rd"
                }
                </script>
            </head>
            <body>
                <div class="rich_media_wrp" id="img-content">
<h1 class="rich_media_title" id="activity-name">
            
GPT-3：大语言模型小样本学习
          </h1>

<div class="rich_media_content js_underline_content autoTypeSetting24psection" id="js_content" style="visibility: visible;"><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com" style='font-size: 16px;padding-right: 10px;padding-left: 10px;word-break: break-word;overflow-wrap: break-word;text-align: left;line-height: 1.25;color: rgb(43, 43, 43);letter-spacing: 2px;background-image: linear-gradient(90deg, rgba(50, 0, 0, 0.04) 3%, rgba(0, 0, 0, 0) 3%), linear-gradient(360deg, rgba(50, 0, 0, 0.04) 3%, rgba(0, 0, 0, 0) 3%);background-size: 20px 20px;background-position: center center;font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;'><p style="padding-top: 8px;padding-bottom: 8px;margin: 10px 0px 0px;font-size: 14px;word-spacing: 2px;line-height: normal;">论文标题：Language Models are Few-Shot Learners</p><p style="padding-top: 8px;padding-bottom: 8px;margin: 10px 0px 0px;font-size: 14px;word-spacing: 2px;line-height: normal;">论文链接：https://arxiv.org/abs/2005.14165</p><p style="padding-top: 8px;padding-bottom: 8px;margin: 10px 0px 0px;font-size: 14px;word-spacing: 2px;line-height: normal;">论文来源：OpenAI</p><h1 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;color: black;font-size: 25px;"><span style="color: rgb(64, 184, 250);display: none;"></span><span style="display: inline-block;color: rgb(64, 184, 250);">一、概述</span><span style="display: inline-block;color: rgb(64, 184, 250);"></span></h1><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">自然语言处理已经从学习特定任务的表示和设计特定任务的架构转变为使用任务无关的预训练和任务无关的架构。这种转变导致了许多具有挑战性的NLP任务的实质性进展，如阅读理解、问题回答、文本蕴涵等。虽然目前模型架构和初始表示是任务无关的，但最终仍然有一个特定于任务的步骤，也就是在一个较大的标注数据集上微调以使得预训练模型能够执行特定的任务。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">GPT-2的研究表明这个最终的步骤或许不是必要的。GPT-2能够以zero-shot的设置来迁移执行标准的自然语言处理任务，并不需要在一个数据集上进行微调。虽然这项工作看似很有前景，但最好的情况下其性能只与单个数据集上的一些有监督baseline相当。在大多数任务中，GPT-2的性能甚至远未达到简单的监督baseline的水平。不过GPT-2也展示出一个潜在的前进方向。这项研究展示了在模型规模跨域一个数量级时模型在迁移任务和语言模型损失上表现出的相对一致的log线性趋势。《Scaling Laws for Neural Language Models》这篇文章则以更加严谨的研究确认了这一趋势。在本文的研究中，我们测试了更高的数量级上是否仍然符合这个趋势。我们训练了一个最高1750亿规模参数量的自回归语言模型，即GPT-3，并且度量了其迁移学习的能力。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">在本文的研究中，我们也明确和系统化了GPT-2中的方法。虽然GPT-2将他们的工作描述为“zero-shot任务迁移”，但他们有时会在上下文中提供相关任务的示例。由于使用的是有效的训练示例，这些情况更好地描述为“one-shot”或“few-shot”转移。本文调研了语言模型在one-shot、few-shot和zero-shot的设置下的表现。研究结果如下图所示。可以观察到one-shot和few-shot的性能优于真正的zero-shot设置的性能，基于这一想象，作者认为语言模型可以被理解为是一个元学习器（meta-learner），它将慢速的外循环基于梯度下降的学习与快速的“上下文内”学习相结合。这里的“外循环梯度下降学习”指的是模型在训练过程中通过大量数据进行梯度下降优化的过程，也就是模型在训练时学习到的知识和概念。而“上下文内学习”是指模型在执行任务时，通过给定的上下文信息（例如，任务描述和示例）快速调整其预测的过程。在zero-shot学习中，模型在没有任何特定任务示例的情况下，需要直接根据给定的输入来解决问题。而在one-shot和few-shot学习中，模型会在输入中获得一个或几个任务示例，这使得模型可以更好地理解和适应任务要求，从而实现更高的性能。将语言模型视为元学习器有助于解释它们在one-shot和few-shot学习中的表现。这种观点强调了语言模型在利用给定的任务示例和上下文信息中的快速学习能力，这有望推动未来的研究和应用。</p><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img alt="GPT-3：大语言模型小样本学习" class="rich_pages wxw-img" data-ratio="0.6294227188081937" data-src="https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKPj7RJ8LibTbLYuuBWib0iaRZvwfvibyHia8do6XVzbsxK9MGQJp9OWJw12Q/640?wx_fmt=png" data-type="png" data-w="1074" src="20240525_125441_0.jpeg" style="border-radius: 6px;display: block;margin: 20px auto;object-fit: contain;box-shadow: rgb(153, 153, 153) 2px 4px 7px;" title="GPT-3：大语言模型小样本学习"/><figcaption style="margin-top: 5px;text-align: center;font-size: 13px;"><span style='background-image: url("https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKG0H5WoSglSJQIYmYnmiaBx6pRtXFic9oicXmTylcHJXF9W6BqGZ5TmC0w/640?wx_fmt=png");display: inline-block;width: 18px;height: 18px;background-size: 18px;background-repeat: no-repeat;background-position: center center;margin-right: 5px;margin-bottom: -5px;'></span>对比结果</figcaption></figure><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">如上面第三个折线图所示，本文还训练了一系列更小的模型（从1.25亿到130亿），在one-shot、few-shot和zero-shot的设置下对比他们的性能。在这三种设置下，模型在大多数任务上的性能与模型的容量之间保持平滑的递增关系。另外one-shot、few-shot和zero-shot的性能<strong style="color: rgb(53, 148, 247);">「差距」</strong>都会随模型容量的增大而递增，这或许表明更大的模型是更好的元学习器。</p><h1 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;color: black;font-size: 25px;"><span style="color: rgb(64, 184, 250);display: none;"></span><span style="display: inline-block;color: rgb(64, 184, 250);">二、方法</span><span style="display: inline-block;color: rgb(64, 184, 250);"></span></h1><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">GPT-3的模型、数据和训练过程都类似于GPT-2，不同的是模型的大小、数据的规模和多样性以及训练的长度。GPT-3在特定任务上的学习（in-context learning）也类似于GPT-2，只不过GPT-3更系统化地探索了不同的设置，包括：</p><p style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;margin-top: 0px;margin-bottom: 0px;line-height: 1.5em;"><strong style="color: rgb(53, 148, 247);">「微调（Fine-Tuning, FT）」</strong>：通过在成百上千的特定任务的有监督数据上训练以更新预训练模型的参数。微调方法的主要优点是在多个benchmark上的强大性能，主要的缺点是需要特定于任务的额外大数据集、可能在分布之外（out-of-distribution）的数据上泛化能力较差、可能导致模型利用训练数据中的伪相关特征从而影响模型的实际性能。GPT-3主要关注语言模型任务无关的性能，没有进行微调实验。</p><section style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;margin-top: 0px;margin-bottom: 0px;line-height: 1.5em;"><strong style="color: rgb(53, 148, 247);">「小样本（Few-Shot, FS）」</strong>：模型在推断阶段被给予一些相关任务的示例，但不会更新模型参数。一个典型的示例包括上下文和期望的补全（例如，一个英文句子和它的法文翻译）。少数示例学习通过提供K个上下文和补全的示例，然后给出一个最终的上下文示例，模型需要提供相应的补全。作者将<span style="cursor:pointer;"><span data-formula="K" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: 0px;width: 2.011ex;height: 1.545ex;" viewbox="0 -683 889 683" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z" data-c="4B"></path></g></g></g></svg></span></span>设置在<span style="cursor:pointer;"><span data-formula="10" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: -0.05ex;width: 2.262ex;height: 1.557ex;" viewbox="0 -666 1000 688" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" data-c="31"></path><path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" data-c="30" transform="translate(500, 0)"></path></g></g></g></svg></span></span>到<span style="cursor:pointer;"><span data-formula="100" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: -0.05ex;width: 3.394ex;height: 1.557ex;" viewbox="0 -666 1500 688" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" data-c="31"></path><path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" data-c="30" transform="translate(500, 0)"></path><path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" data-c="30" transform="translate(1000, 0)"></path></g></g></g></svg></span></span>的范围内，因为模型的上下文窗口（<span style="cursor:pointer;"><span data-formula="nctx=2048" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: -0.186ex;width: 11.99ex;height: 1.717ex;" viewbox="0 -677 5299.6 759" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" data-c="6E"></path></g><g data-mml-node="mi" transform="translate(600, 0)"><path d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" data-c="63"></path></g><g data-mml-node="mi" transform="translate(1033, 0)"><path d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" data-c="74"></path></g><g data-mml-node="mi" transform="translate(1394, 0)"><path d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" data-c="78"></path></g><g data-mml-node="mo" transform="translate(2243.8, 0)"><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" data-c="3D"></path></g><g data-mml-node="mn" transform="translate(3299.6, 0)"><path d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" data-c="32"></path><path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" data-c="30" transform="translate(500, 0)"></path><path d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" data-c="34" transform="translate(1000, 0)"></path><path d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z" data-c="38" transform="translate(1500, 0)"></path></g></g></g></svg></span></span>）可以容纳这么多示例。小样本设置的优点是大大减少了对特定任务数据的需求，缺点是迄今为止，这种方法的结果要比最先进的微调模型差很多，另外仍然需要少量的任务特定数据。这里的小样本学习与其他机器学习领域的小样本学习是相关的，它们都涉及基于广泛任务分布的学习，然后快速适应新任务。</section><section style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;margin-top: 0px;margin-bottom: 0px;line-height: 1.5em;"><strong style="color: rgb(53, 148, 247);">「单样本（One-Shot, 1S）」</strong>：与小样本类似但是<span style="cursor:pointer;"><span data-formula="K=1" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: -0.186ex;width: 6.16ex;height: 1.731ex;" viewbox="0 -683 2722.6 765" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z" data-c="4B"></path></g><g data-mml-node="mo" transform="translate(1166.8, 0)"><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" data-c="3D"></path></g><g data-mml-node="mn" transform="translate(2222.6, 0)"><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" data-c="31"></path></g></g></g></svg></span></span>。</section><section style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;margin-top: 0px;margin-bottom: 0px;line-height: 1.5em;"><strong style="color: rgb(53, 148, 247);">「零样本（Zero-Shot, 0S）」</strong>：与小样本类似但是不提供任何示例而是提供任务的自然语言描述。</section><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">GPT-3主要探究语言模型在one-shot、few-shot和zero-shot的设置下的表现。</p><ol class="list-paddingleft-1" data-tool="mdnice编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;font-size: 15px;color: rgb(89, 89, 89);"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;font-size: 14px;">模型架构</section></li></ol><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">GPT-3沿用了GPT-2的模型结构和架构，包括修改后的初始化、预归一化和可逆的分词方法。不同之处在于GPT-3在Transformer的层中采用了交替的密集（dense）和局部带状稀疏（locally banded sparse）注意力模式，这与Sparse Transformer类似。了研究机器学习性能与模型大小之间的依赖关系，作者训练了8种不同大小的模型，从1.25亿个参数到1750亿个参数。最大的模型被称为GPT-3。通过这个参数范围，作者可以测试《Scaling Laws for Neural Language Models》论文中引入的规模定律。</p><ol class="list-paddingleft-1" data-tool="mdnice编辑器" start="2" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;font-size: 15px;color: rgb(89, 89, 89);"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;font-size: 14px;">训练数据集</section></li></ol><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">本文按照以下方式创建了GPT-3的训练数据集： </p><p style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;margin-top: 0px;margin-bottom: 0px;line-height: 1.5em;">①下载和筛选：作者下载了一个CommonCrawl版本，并基于与一系列高质量参考语料库的相似性进行筛选。CommonCrawl是一个包含了大量网页抓取数据的数据集。 </p><p style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;margin-top: 0px;margin-bottom: 0px;line-height: 1.5em;">②模糊去重：为了避免冗余并保持验证集作为过拟合准确度量的完整性，作者在数据集内部和跨数据集之间对文档级别进行了模糊去重。 </p><p style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;margin-top: 0px;margin-bottom: 0px;line-height: 1.5em;">③添加高质量参考语料库：为了提高训练数据的质量和多样性，作者将已知的高质量参考语料库添加到了训练数据中。这些参考语料库包括：1). 扩展版的WebText数据集：通过在更长时间内抓取链接收集到的数据，该数据集首次出现在《Scaling Laws for Neural Language Models》论文中。2). 两个基于互联网的图书语料库（Books1和Books2）。3). 英语维基百科。</p><ol class="list-paddingleft-1" data-tool="mdnice编辑器" start="3" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;font-size: 15px;color: rgb(89, 89, 89);"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;font-size: 14px;">训练过程</section></li></ol><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">研究表明更大的模型通常可以使用更大的batch size，但需要较小的学习率。作者在训练过程中测量梯度噪声尺度，并用它指导批量大小的选择。为了在不耗尽内存的情况下训练更大的模型，作者采用了混合的模型并行方式，包括在每个矩阵乘法中的模型并行和在网络层之间的模型并行。所有模型都在V100 GPU上进行训练，这些GPU是高带宽集群的一部分。</p><ol class="list-paddingleft-1" data-tool="mdnice编辑器" start="4" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;font-size: 15px;color: rgb(89, 89, 89);"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;font-size: 14px;">评估</section></li></ol><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">对于few-shot设置，评估时，从任务训练集中随机抽取<span style="cursor:pointer;"><span data-formula="K" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: 0px;width: 2.011ex;height: 1.545ex;" viewbox="0 -683 889 683" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z" data-c="4B"></path></g></g></g></svg></span></span>个示例作为条件，根据任务需要，使用<span style="cursor:pointer;"><span data-formula="1" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: 0px;width: 1.131ex;height: 1.507ex;" viewbox="0 -666 500 666" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" data-c="31"></path></g></g></g></svg></span></span>或<span style="cursor:pointer;"><span data-formula="2" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: 0px;width: 1.131ex;height: 1.507ex;" viewbox="0 -666 500 666" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" data-c="32"></path></g></g></g></svg></span></span>个换行符分隔。LAMBADA和Storycloze没有监督训练集，因此从开发集中抽取条件示例，并在测试集上进行评估。有时，除了（或者在<span style="cursor:pointer;"><span data-formula="K=0" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: -0.186ex;width: 6.16ex;height: 1.731ex;" viewbox="0 -683 2722.6 765" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z" data-c="4B"></path></g><g data-mml-node="mo" transform="translate(1166.8, 0)"><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" data-c="3D"></path></g><g data-mml-node="mn" transform="translate(2222.6, 0)"><path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" data-c="30"></path></g></g></g></svg></span></span>时，代替）示例，作者还使用自然语言prompt。在自由形式补全任务上，作者使用与eam search，beam width为<span style="cursor:pointer;"><span data-formula="4" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: 0px;width: 1.131ex;height: 1.532ex;" viewbox="0 -677 500 677" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" data-c="34"></path></g></g></g></svg></span></span>，长度惩罚因子<span style="cursor:pointer;"><span data-formula="α=0.6" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: -0.452ex;width: 8.051ex;height: 2.149ex;" viewbox="0 -750 3558.7 950" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mjx-texclass="ORD" data-mml-node="TeXAtom"><g data-mml-node="mo"><text data-variant="normal" font-family="serif" font-size="947.1px" transform="matrix(1 0 0 -1 0 0)">α</text></g></g><g data-mml-node="mo" transform="translate(1224.9, 0)"><path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" data-c="3D"></path></g><g data-mml-node="mn" transform="translate(2280.7, 0)"><path d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" data-c="30"></path><path d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" data-c="2E" transform="translate(500, 0)"></path><path d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z" data-c="36" transform="translate(778, 0)"></path></g></g></g></svg></span></span>。当测试集公开时，作者报告每个模型大小和学习设置（one-shot、few-shot和zero-shot）在测试集上的结果。当测试集为私有时，由于模型通常太大而无法放在测试服务器上，作者报告在开发集上的结果。</p><h1 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;color: black;font-size: 25px;"><span style="color: rgb(64, 184, 250);display: none;"></span><span style="display: inline-block;color: rgb(64, 184, 250);">三、实验</span><span style="display: inline-block;color: rgb(64, 184, 250);"></span></h1><ol class="list-paddingleft-1" data-tool="mdnice编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;font-size: 15px;color: rgb(89, 89, 89);"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;font-size: 14px;">语言建模、完形填空和补全任务</section></li></ol><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img alt="GPT-3：大语言模型小样本学习" class="rich_pages wxw-img" data-ratio="0.32592592592592595" data-src="https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKA7IactadbycBysOia8kF5g61oKVfy37NfRGNPotNVDtUXcCMbaPnStQ/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_125442_1.jpeg" style="border-radius: 6px;display: block;margin: 20px auto;object-fit: contain;box-shadow: rgb(153, 153, 153) 2px 4px 7px;" title="GPT-3：大语言模型小样本学习"/><figcaption style="margin-top: 5px;text-align: center;font-size: 13px;"><span style='background-image: url("https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKG0H5WoSglSJQIYmYnmiaBx6pRtXFic9oicXmTylcHJXF9W6BqGZ5TmC0w/640?wx_fmt=png");display: inline-block;width: 18px;height: 18px;background-size: 18px;background-repeat: no-repeat;background-position: center center;margin-right: 5px;margin-bottom: -5px;'></span>实验</figcaption></figure><ol class="list-paddingleft-1" data-tool="mdnice编辑器" start="2" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;font-size: 15px;color: rgb(89, 89, 89);"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;font-size: 14px;">问答任务</section></li></ol><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img alt="GPT-3：大语言模型小样本学习" class="rich_pages wxw-img" data-ratio="0.3351851851851852" data-src="https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZK6nv9msePQjv41nIG2TpsC8WkekxYYsicszo14adDGu5lBjknv74aHJg/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_125443_2.jpeg" style="border-radius: 6px;display: block;margin: 20px auto;object-fit: contain;box-shadow: rgb(153, 153, 153) 2px 4px 7px;" title="GPT-3：大语言模型小样本学习"/><figcaption style="margin-top: 5px;text-align: center;font-size: 13px;"><span style='background-image: url("https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKG0H5WoSglSJQIYmYnmiaBx6pRtXFic9oicXmTylcHJXF9W6BqGZ5TmC0w/640?wx_fmt=png");display: inline-block;width: 18px;height: 18px;background-size: 18px;background-repeat: no-repeat;background-position: center center;margin-right: 5px;margin-bottom: -5px;'></span>实验</figcaption></figure><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img alt="GPT-3：大语言模型小样本学习" class="rich_pages wxw-img" data-ratio="0.28425925925925927" data-src="https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKflj0mTicxqT4kJEll0iaaZAu2Qzyo3oksTlibYdScDzn9zwMySA2zpnrg/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_125444_3.jpeg" style="border-radius: 6px;display: block;margin: 20px auto;object-fit: contain;box-shadow: rgb(153, 153, 153) 2px 4px 7px;" title="GPT-3：大语言模型小样本学习"/><figcaption style="margin-top: 5px;text-align: center;font-size: 13px;"><span style='background-image: url("https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKG0H5WoSglSJQIYmYnmiaBx6pRtXFic9oicXmTylcHJXF9W6BqGZ5TmC0w/640?wx_fmt=png");display: inline-block;width: 18px;height: 18px;background-size: 18px;background-repeat: no-repeat;background-position: center center;margin-right: 5px;margin-bottom: -5px;'></span>实验</figcaption></figure><ol class="list-paddingleft-1" data-tool="mdnice编辑器" start="3" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;font-size: 15px;color: rgb(89, 89, 89);"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;font-size: 14px;">翻译</section></li></ol><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img alt="GPT-3：大语言模型小样本学习" class="rich_pages wxw-img" data-ratio="0.5212962962962963" data-src="https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKOX1Svyj7VU3ARKERmm2CLiaEEFh0jU2uKkQE8OuIUicPcicCSFWQsTbKQ/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_125446_4.jpeg" style="border-radius: 6px;display: block;margin: 20px auto;object-fit: contain;box-shadow: rgb(153, 153, 153) 2px 4px 7px;" title="GPT-3：大语言模型小样本学习"/><figcaption style="margin-top: 5px;text-align: center;font-size: 13px;"><span style='background-image: url("https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKG0H5WoSglSJQIYmYnmiaBx6pRtXFic9oicXmTylcHJXF9W6BqGZ5TmC0w/640?wx_fmt=png");display: inline-block;width: 18px;height: 18px;background-size: 18px;background-repeat: no-repeat;background-position: center center;margin-right: 5px;margin-bottom: -5px;'></span>实验</figcaption></figure><ol class="list-paddingleft-1" data-tool="mdnice编辑器" start="4" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;font-size: 15px;color: rgb(89, 89, 89);"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;font-size: 14px;">SuperGLUE</section></li></ol><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><figcaption style="margin-top: 5px;text-align: center;font-size: 13px;"><span style='background-image: url("https://mmbiz.qpic.cn/mmbiz_png/YUNEEzwiaBceW18FucFFzH4ShBEOW7BZKG0H5WoSglSJQIYmYnmiaBx6pRtXFic9oicXmTylcHJXF9W6BqGZ5TmC0w/640?wx_fmt=png");display: inline-block;width: 18px;height: 18px;background-size: 18px;background-repeat: no-repeat;background-position: center center;margin-right: 5px;margin-bottom: -5px;'></span>实验</figcaption></figure><h1 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;color: black;font-size: 25px;"><span style="color: rgb(64, 184, 250);display: none;"></span><span style="display: inline-block;color: rgb(64, 184, 250);">四、局限性</span><span style="display: inline-block;color: rgb(64, 184, 250);"></span></h1><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;margin-top: 10px;margin-bottom: 10px;font-size: 14px;word-spacing: 2px;">本文讨论了GPT-3的五个主要局限性： </p><section style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;line-height: 1.5em;margin-bottom: 0px;margin-top: 0px;">①文本生成的问题：GPT-3生成的文本样本有时在文档层面上语义重复，较长篇幅的文本可能会失去连贯性，自相矛盾，或偶尔包含无关紧要的句子或段落。 </section><section style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;line-height: 1.5em;margin-bottom: 0px;margin-top: 0px;">②实验范围局限：作者的实验没有涉及双向架构（bidirectional architectures）或其他训练目标，如去噪（denoising）。这种设计决策可能导致在那些从双向性中获益的任务上性能较差，例如填空任务、需要回顾并比较两部分内容的任务（ANLI，WIC）或需要重新阅读或仔细思考长篇幅后生成简短答案的任务（QuAC，RACE）。 </section><section style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;line-height: 1.5em;margin-bottom: 0px;margin-top: 0px;">③目标函数的局限性：GPT-3的目标函数将每个token视为同等重要，缺乏区分重要性的概念。有研究表明，针对感兴趣的实体定制预测可能有益。此外，对于自监督目标，任务规范依赖于将期望的任务强制转化为预测问题。然而，有用的语言系统（如虚拟助手）可能更适合执行目标导向的行为，而不仅仅是预测。最后，大型预训练语言模型没有在其他领域（如视频或真实世界的物理互动）中建立基础，因此缺乏大量关于世界的上下文信息。 </section><section style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;line-height: 1.5em;margin-bottom: 0px;margin-top: 0px;">④纯自监督预测的限制：由于上述原因，纯自监督预测的扩展可能会遇到瓶颈，需要采用不同的方法进行补充。有希望的未来方向可能包括从人类那里学习目标函数、通过强化学习进行微调，或将图像等其他模态添加到模型中，以便提供基础和更好的世界模型。 </section><section style="padding-top: 8px;padding-bottom: 8px;font-size: 14px;word-spacing: 2px;line-height: 1.5em;margin-bottom: 0px;margin-top: 0px;">⑤模型的部署：GPT-3的尺寸使得部署变得具有挑战性。在这种规模下，任务特定的蒸馏（Task-specific distillation）值得探讨。</section></section><p><br/></p><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div>

</div>
                <p></p>
                <p><a href="../index.html">返回：GPT-3：大语言模型小样本学习</a></p>
                <div>本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 </div>
                <div  class="hidden">本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 <br><p class="hidden">code/s?__biz=MzU5MzYxNDY1OA==&mid=2247499853&idx=1&sn=f95e06f18f9ed0ebc1be6829779c7744&chksm=fe0f4260c978cb769389cb225cce853b0e6f4fee99a2666841bc53a33e039377900c532b3e83#rd </p></div>
            </body>
            </html>
            