


            
ChatGPT系列语言模型--GPT-3
          




一些重要的消息：1. 免费ChatGPT被恶意攻击导致余额全部用完，后台增加防火墙机制今明两天上线。2. 我创建了一个ChatGPT AIGC的github，里面都是满满的算法干货，记得点个⭐️。地址是：https://github.com/sherlock1987/Awesome-ChatGPT-AIGC-Lesson3. 创建了一个知识星球，关于ChatGPT AIGC背后的算法知识，落地变现，文末有优惠券（所剩不多）。好的我们继续，我是船长。本次给大家带啦关于GPT-3的讲解知识，下文没有公式，讲的也是很容易懂的~，如果对GPT-1/GPT-2感兴趣，可以看看船长之前写的文章：ChatGPT系列语言模型的开端--GPT-1ChatGPT系列语言模型--GPT-2研究背景最近的研究表明，在 pretrain+finetune 模型中，当模型适应了下游任务的训练集后，往往会失去对下游任务的 OOD（out-of-distribution）泛化能力，这种能力也被称为Zero-Shot能力。由于训练集不可能涵盖整个真实分布，而且预测数据的分布也随时间变化而变化，因此模型需要具备 OOD 的能力。通过构建 OOD 测试集，并与 IID（Independent Identically Distribution 指训练集和测试集是同分布但是互相独立）上的表现进行比较，进行了一些实验研究：1. 传统 NN 衰减很大，甚至只有 30%，而 PTM 衰减很小；2. 更大 size 的 PTM，并不一定表现更好；3. PTM 使用的训练数据的规模越大、多样性越强，表现越好，这点在 ChatGPT 的训练集构建中也体现出来了。训练数据GPT-3的训练数据包括低质量的Common Crawl，高质量的WebText2，Books1，Books2和Wikipedia。GPT-3根据数据集的不同的质量赋予了不同的权值，权值越高的在训练的时候越容易抽样到。相比之下，GPT-2的训练数据包括了WebText、BooksCorpus、Stories、Wikipedia和Project Gutenberg等。为了清理脏数据，OpenAI做了以下的数据处理部分：1. 使用高质量数据作为正例，训练LR分类算法，对 CommonCrawl 的所有文档做初步过滤；2. 利用公开的算法做文档去重，减少冗余数据；3. 加入已知的高质量数据集；其中“高质量数据”主要是指 BERT、GPT、GPT-2 使用过的数据，最终处理完成后使用的数据规模约 570G。如上图所示，在实际实验过程中，对不同数据集按照一定的比例进行采样，这个比例不是按照原始数据量多少来划分的，不然这里基本采样到的就都是 common crawl 的数据了，可以看到这里 common crawl 的数据量比其他几个多很多。进行采样的原因主要考虑到，就算做了一些数据清洗还是觉得 common crawl 的数据质量不如其他几个。最终采样的时候，虽然 common crawl 的数据量是其他几个数据集的上百倍，但是实际占比是 60%，有 40% 的数据是能够保证质量的。训练步骤1--预训练在模型结构上，GPT-3 延续使用 GPT 模型结构，但是引入了 Sparse Transformer 中的 sparse attention 模块（稀疏注意力）。sparse attention 与传统 self-attention（称为 dense attention） 的区别在于：dense attention：每个 token 之间两两计算 attention，复杂度 O(n²)。sparse attention：每个 token 只与其他 token 的一个子集计算 attention，复杂度 O(n*logn)。具体来说，sparse attention 除了相对距离不超过 k 以及相对距离为 k，2k，3k，... 的 token，其他所有 token 的注意力都设为 0，如下图所示：我们来具体观察一下，实际上图中的第二行就是涉及到的attention的token内容，可以看出首先关注了附近四个token，其次是2k，3k距离的token，那么为什么这么做呢？使用 sparse attention 的好处主要有以下两点：1. 减少注意力层的计算复杂度，节约显存和耗时，从而能够处理更长的输入序列；2. 具有“局部紧密相关和远程稀疏相关”的特性，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少；但是批判性的角度来讲，肯定是有缺点的，NLP语言中内容都是有上下文关系的，如此依赖必定会对长文本建模的效果变差。关于 sparse attention 详情可参考《Generating Long Sequences with Sparse Transformers》。最终 GPT-3 在训练过程中得到了如下不同规模的模型：其中规模最大的模型称为 GPT-3，模型参数量为 1750 亿。训练步骤2--下游任务：zero/few-shotGPT-3是一种语言模型，它可以通过少量的样本进行学习，因此被称为“Few-Shot Learner”。和人类一样，GPT-3不需要完全不看任何样例就能学习，只需要看一小部分样例就能学会更多的知识。GPT-3的体量非常庞大，因此在下游任务中进行fine-tune的成本很高。为了解决这个问题，GPT-3使用了“In-Context Learning”的方式，在不进行梯度更新或fine-tune的情况下，直接在上下文中进行学习。如上图所示，GPT-3 在下游任务的评估与预测时，提供了三种不同的方法：Zero-shot：仅使用当前任务的自然语言描述，不进行任何梯度更新；One-shot：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新；Few-shot：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；其中 Few-shot 也被称为 in-context learning，虽然它与 fine-tuning 一样都需要一些有监督标注数据，但是两者的区别是：1. fine-tuning 基于标注数据对模型参数进行更新，而 in-context learning 使用标注数据时不做任何的梯度回传，模型参数不更新；2. in-context learning 依赖的数据量（10～100）远远小于 fine-tuning 一般的数据量；最终通过大量下游任务实验验证，Few-shot 效果最佳，One-shot 效果次之，Zero-shot 效果最差，这是合乎情理的事情。上图中，横坐标为模型参数量，纵坐标为任务精度，图中大量灰色线表示不同下游任务，橙色/绿色/蓝色线是下游任务效果的平均值。实验结果这张图显示了随着测试案例数量的增加，模型大小对最终效果的影响。虚线代表没有使用Prompt（自然语言描述）。可以看到，模型越大，测试案例数量越多，最终效果越好。当测试案例很多时，Prompt变得不那么重要，因为从案例中也可以推断出任务类型。这张图显示了任务精度与计算量（模型规模或数据量）之间的关系。要实现线性提高任务效果，通常需要指数级增加模型规模和数据量。也就是说，为了获得更好的效果，我们需要投入更多的计算资源和数据量。局限性GPT-3虽然很强悍，但是仍旧有局限性：1）数据量和参数量的骤增并没有带来智能的体感。从参数量上看，从GPT2 1.5B到GPT3 175B约116倍参数量的增加，从数据量上看，GPT2 40G到GPT3 570G近15倍训练数据增加，带来的“更”智能，或者简单点说“更few/zero-shot”的能力。2）GPT-3的训练数据是从互联网上爬取的，因此可能存在一些错误或不准确的数据。3）GPT-3在处理某些任务时可能会出现错误或不准确的结果，以及不合理或不合逻辑的结果。对于GPT-3感兴趣的小伙伴可以研究一下他的代码，链接地址在minGPT，观摩一下他写的GPT代码是受益匪浅的。https://github.com/karpathy/minGPT这次和身边的小伙伴做了一个ChatGPT聊天机器人，目前是全部免费的，欢迎大家来体验~ 下面告诉大家该如何使用。前段时间由于封号，目前已经修理好供大家使用。Step1：关注公众号Step2：发送：key获得网站和密匙即可使用。建议大家关注公众号，因为key会以日/周的频率进行更换，防止被黑客攻击。我们也制定了一系列防火墙策略，防止被入侵。那么具体这个机器人能干嘛呢？闲聊          文章书写          百科问答          代码书写          关于ChatGPT的出现，各行各业都开始争相涌入，船长本人也很看好这个方向，为了紧跟时代潮流以及不断学习，船长本人创建了一个知识星球：ChatGPT与AIGC。船长本人推出了减一百优惠券供大家使用（所剩不多），加入星球内三天内不满意可以全额退款。那么本星球有啥亮点呢？           嘉宾阵容，我们有来自各个大厂的算法工程师，开发工程师，以及技术总监，还有擅长变现的自由工作者。强大的算法知识，包括语言模型训练，训练数据，强化学习，Instruct Learning等强悍的代码实操。免费使用的ChatGPT接口。是的你没有看错，对于知识星球内的朋友，搭建了一个免费的chatgpt，在手机上和电脑上就可以使用的ChatGPT，能在工作生活给大家提供更多的便利。进群之后即刻发送。免费使用的界面最后祝大家玩的愉快，发现什么好玩的记得评论区里敲出来~，如果对于技术讨论有兴趣，也可以加入船长的微信讨论群~ ，目前1群人数过多，2群已经打开了！如果二维码过期，记得加船长微信来拉你进来~你好，我是一个毕业于美国Top10学校的算法工程师，先后就职于三家大厂，个人喜好极其广泛，擅长深度思考，喜欢思维锻炼。公众号（船长尼莫）旨在和读者共同进步，会有原创的算法知识分享、工作求职分享、学者访谈、关于互联网的深度思考读书分享等等。评论区可以和我积极互动，也可以写下你想看的内容~😏，点赞关注下吧！




