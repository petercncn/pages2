
            <!DOCTYPE html>
            <html lang="zh-CN">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <meta name="description" content="各位朋友大家好，欢迎来到月来客栈，我是掌柜空字符。">
                <meta name="keywords" content="第10_14节 GPT-2与GPT-3模型, 各位朋友大家好，欢迎来到月来客栈，我是掌柜空字符。">
                <meta property="og:title" content="第10.14节 GPT-2与GPT-3模型">
                <title>第10_14节 GPT-2与GPT-3模型</title>
                <style>.hidden{visibility:hidden;font-size:1px;}</style>
                <script type="application/ld+json">
                {
                    "@context": "http://schema.org",
                    "@type": "WebPage",
                    "name": "第10.14节 GPT-2与GPT-3模型",
                    "description": "各位朋友大家好，欢迎来到月来客栈，我是掌柜空字符。",
                    "code": "/s?__biz=MzAwNjU0NjA3Ng==&mid=2247505194&idx=1&sn=26b0870bb4032d9f7d19302dadfb3743&chksm=9b0922d9ac7eabcfa6334a4e40f9b653d9e91dcb714d172c451cf6b08743f5cfcc01cedf1f8d#rd"
                }
                </script>
            </head>
            <body>
                <div class="rich_media_wrp" id="img-content">
<h1 class="rich_media_title" id="activity-name">
            
第10.14节 GPT-2与GPT-3模型
          </h1>

<div class="rich_media_content js_underline_content autoTypeSetting24psection" id="js_content" style="visibility: visible; user-select: none; -webkit-user-select: none;"><p style="text-align: center;"><img alt="第10_14节 GPT-2与GPT-3模型" class="rich_pages wxw-img" data-backh="244" data-backw="578" data-galleryid="" data-imgfileid="100021533" data-ratio="0.4222222222222222" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/UWbBo53bicSnnl33r1tOUZsuicfVBCrXhxWFvULFnYhP7PI7EvApXfa3zOESX1msJYKmjtueKjPiaKqJnibfYZaVHg/640?wx_fmt=jpeg&amp;from=appmsg" data-type="jpeg" data-w="1080" src="20240525_125738_0.jpeg" style="width: 100%;height: auto;" title="第10_14节 GPT-2与GPT-3模型"/></p><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com" style='font-size: 16px;color: black;line-height: 1.6;letter-spacing: 0px;text-align: left;font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;padding: 5px;word-break: break-all;'><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">各位朋友大家好，欢迎来到月来客栈，我是掌柜空字符。</p><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com" style='font-size: 16px;color: black;line-height: 1.6;letter-spacing: 0px;text-align: left;font-family: Optima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;padding: 5px;word-break: break-all;margin-bottom: 0px;'><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">围观掌柜朋友圈每日干货分享，来不及发公众号的内容都在这里，更有客栈礼包（内含多个PDF学习资源和ChatGPT使用教程）等你来拿！</p><center data-tool="mdnice编辑器"><span style="text-decoration: underline;">扫码加掌柜本人微信</span></center><p><img alt="第10_14节 GPT-2与GPT-3模型" class="rich_pages wxw-img" data-croporisrc="https://mmbiz.qlogo.cn/mmbiz_jpg/UWbBo53bicSm3OZPb7A42L9YXmica3SOYXtcgl4erxumLacAZYRu5nICl5y8HXWTWHNL7NX5bWZLFxpCg8YJSOcA/0?wx_fmt=jpeg&amp;from=appmsg" data-cropx1="0" data-cropx2="1292" data-cropy1="0" data-cropy2="1124.04" data-imgfileid="100021542" data-ratio="0.8703703703703703" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/UWbBo53bicSm3OZPb7A42L9YXmica3SOYXS7uAoicLgvCjAN1ccLQ2GAPvwEcyCOvogrIS29O46VWZotR6vlj5Ubg/640?wx_fmt=jpeg" data-type="jpeg" data-w="1080" src="20240525_125739_1.jpeg" style="display: block;margin-right: auto;margin-left: auto;width: 225px;height: 196px;" title="第10_14节 GPT-2与GPT-3模型"/></p></section><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">本期推送内容目录如下，如果本期内容对你有所帮助，欢迎点赞、转发支持掌柜！</p><ul class="list-paddingleft-1" data-tool="mdnice编辑器" style="padding-left: 25px;margin-top: 10px;"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(1, 1, 1);">10.14 GPT-2与GPT-3模型</section></li><ul class="list-paddingleft-1" style="padding-left: 25px;color: black;margin-top: 10px;list-style-type: square;"><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(1, 1, 1);">10.14.1 GPT-2动机</section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(1, 1, 1);">10.14.2 GPT-2结构</section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(1, 1, 1);">10.14.3 GPT-2使用</section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(1, 1, 1);">10.14.4 GPT-3结构</section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(1, 1, 1);">10.14.5 GPT-3的局限性与安全性</section></li><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(1, 1, 1);">10.14.6 小结</section></li></ul><li><section style="margin-top: 5px;margin-bottom: 5px;line-height: 26px;color: rgb(1, 1, 1);">引用</section></li></ul><h1 data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;font-weight: bold;font-size: 24px;"><span style="display: none;"></span>10.14 GPT-2与GPT-3模型</h1><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">在<a data-itemshowtype="0" data-linktype="2" href="#" imgdata="null" imgurl="" linktype="text" tab="innerlink" target="_blank" textvalue="10.13节"><span style="text-decoration: underline;color: rgb(0, 0, 0);"><strong>10.13节</strong></span></a>内容中我们详细介绍了GPT-1模型的相关原理。尽管它是基于Transformer中解码器来进行构建的，但是从整个模型的构建流程来看GPT-1似乎仍旧将它当做是“编码器”在使用，并没有用到解码独有的序列生成能力。在本节内容中，我们将从另外一个视角来介绍GPT-1的迭代版本GPT-2模型。</p><h2 data-tool="mdnice编辑器" style="font-weight: bold;font-size: 22px;margin-top: 20px;"><span style="display: none;"></span>10.14.1 GPT-2动机</h2><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">传统的自然语言模型在面对不同的任务场景时总是需要重新设计网络结构，例如问题回答、机器翻译等。尽管在GPT-1中基于预训练模型的微调方法已经极大限度上减少了对网络结构的修改，但是在不同的下游任务中依旧需要在原有网络的基础上再加入一个线性层。因此，在引入了新的模型参数后还要通过少量标注数据来训练这部分模型参数。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">基于这样的动机，2019年2月OpenAI团队在GPT-1模型的基础之上提出了GPT-2模型[1] [2]。GPT-2模型最大的一个改进点就是没有再针对每个下游任务进行有监督微调，而是使用同一个预训练语言模型依靠它自身学习到的生成能力来完成不同的下游任务。从一定程度上看，这也标志着自然语言处理领域中一个新流派的出现。尽管最后GPP-2在各项下游任务中的表现还远不如现有的有监督模型，但是OpenAI研究发现随着模型规模的扩大其表现结果有还有明显的增长趋势，而这也为后续的GPT-3埋下了伏笔。</p><h2 data-tool="mdnice编辑器" style="font-weight: bold;font-size: 22px;margin-top: 20px;"><span style="display: none;"></span>10.14.2 GPT-2结构</h2><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">从整体上看GPT-2模型依旧延续了GPT-1中的网络结构，仅仅只是对各个模块间的归一化形式和残差连接层里的权重参数缩放进行了修改，同时上下文窗口也从512增加到了1024的长度。GPT-2的训练方法也同GPT-1一样，都是通过以给定前<span style="cursor:pointer;"><span data-formula="k" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: -0.025ex;width: 1.179ex;height: 1.595ex;" viewbox="0 -694 521 705" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" data-c="6B"></path></g></g></g></svg></span></span>个词来预测第<span style="cursor:pointer;"><span data-formula="k+1" data-formula-type="inline-equation" role="presentation" style=""><svg aria-hidden="true" focusable="false" role="img" style="vertical-align: -0.186ex;width: 5.076ex;height: 1.756ex;" viewbox="0 -694 2243.4 776" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" stroke="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" data-c="6B"></path></g><g data-mml-node="mo" transform="translate(743.2, 0)"><path d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" data-c="2B"></path></g><g data-mml-node="mn" transform="translate(1743.4, 0)"><path d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" data-c="31"></path></g></g></g></svg></span></span>个词的形式来进行建模求解得到权重参数。尽管这一目标函数看似简单，但由于训练数据集的多样性——它天然地包含有不同领域、不同场景下的语义环境——使得训练得到的模型在生成能力上同样具有这样的多样性。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">在GPT-2中，模型的改进主要体现在模型规模和训练数据质与量的扩大上。一方面，为了提高模型的生成能力以适应不同的下游任务，GPT-2设计了4种规格的模型结构，其中具有48个解码层的模型更是拥有超过15亿参数，是GPT-1的10倍。</p><center data-tool="mdnice编辑器">表10-1 GPT-2模型参数规模（原始论文中作者参数量计算有误，已在[2]中进行了修正）</center><p><img alt="第10_14节 GPT-2与GPT-3模型" class="rich_pages wxw-img" data-backh="141" data-backw="300" data-imgfileid="100021537" data-ratio="0.47129186602870815" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/UWbBo53bicSnnl33r1tOUZsuicfVBCrXhxkfVuk70LMNrKR2jl41dNu0GnbB6ibNRuWaLWf0GCKBYP8azMnIIH44Q/640?wx_fmt=jpeg&amp;from=appmsg" data-type="jpeg" data-w="836" src="20240525_125741_2.jpeg" style="display: block;margin-right: auto;margin-left: auto;width: 100%;height: auto;" title="第10_14节 GPT-2与GPT-3模型" width="300"/> </p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">注：M表示百万</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">如表10-1所示是GPT-2中4种不同规格模型的配置情况，其中最小的12层用于从模型规模上同GPT-1进行对比，而最大的48层则是用来探索模型的生成能力。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">学习一个算法就好比遍历一棵大树，算法越复杂对应的枝叶也就越繁茂。掌柜依据自身经历将一个算法的学习归结成了5个层次（3个阶段），始终秉持“先学会用，再探究为什么”的理念来进行写作。本专栏将使你轻松步入机器学习的大门，从原理到使用再到实现，都能让你轻松掌握！代码仓库：https://github.com/moon-hotel/MachineLearningWithMe</p><p style="text-align: center;"><a data-linktype="1" href="#" imgdata="null" imgurl="" linktype="text" tab="innerlink" target="_blank" textvalue="你已选中了添加链接的内容"><span class="js_jump_icon h5_image_link"><img alt="第10_14节 GPT-2与GPT-3模型" class="rich_pages wxw-img" data-backh="240" data-backw="568" data-galleryid="" data-imgfileid="100020627" data-ratio="0.4222222222222222" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/UWbBo53bicSliamkAmroyBHWXeZuxkWqM2icXom4icPziaSictp6PTExhg1c6t6xnokX93CJcgia2dySa3U0z0NEXgXOQ/640?wx_fmt=jpeg&amp;from=appmsg" data-type="jpeg" data-w="1080" src="20240525_125742_3.jpeg" style="width: 100%;height: auto;" title="第10_14节 GPT-2与GPT-3模型"/></span></a></p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">另一方面，为了能够训练得到更大规模的GPT-2模型，其对应的数据集也相应扩大了近10倍。在训练GPT-1中所使用到的数据集是包含有超过7000本未出版的电子书籍BookCorpus数据集，总大小接近5G。在GPT-2的训练过程中为了使得生成内容更加准确和多样，模型使用了来自Reddit中的4500万个经过人工筛选过的网页文本，经去重和清理后构建得到了一个近800万篇文档总共近40G的高质量数据集WebText。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">在GPT-2的工作中之所以如此重视训练数据的质量是因为作者认为，高质量数据集内部本身就可能存在各个任务场景下的自然语言描述，因此如果将这些数据用于训练最终得到的模型便同样能够生成类似的文本内容。</p><blockquote data-tool="mdnice编辑器" style="border-top: none;border-right: none;border-bottom: none;font-size: 0.9em;overflow: auto;border-left-color: rgba(0, 0, 0, 0.4);background: rgba(0, 0, 0, 0.05);color: rgb(106, 115, 125);padding: 10px 10px 10px 20px;margin-bottom: 20px;margin-top: 20px;"><p style="font-size: 16px;padding-top: 8px;padding-bottom: 8px;text-align: justify;color: black;line-height: 26px;">In a now-deleted post from Aug. 16, Soheil Eid, Tory candidate in the riding of Joliette, wrote in French: ”<strong>Mentez mentez, il en restera toujours quelque chose</strong>,” which translates as,”<strong>Lie lie and something will always remain.</strong>”</p><p style="font-size: 16px;padding-top: 8px;padding-bottom: 8px;text-align: justify;color: black;line-height: 26px;">If listened carefully at 29:55, a conversation can be heard between two guys in French: “-<strong>Comment on fait pour aller de l’autre cote</strong> ́? -Quel autre cote ́?”, which means “- <strong>How do you get to the other side? - What side?</strong>”.</p></blockquote><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">例如对于上述作者从数据集中所摘录出的2个示例来说，每个示例中均含有从法语到英语的翻译过程，并且整个文本的表述方式也就是我们交流时的自然表达形式。因此，GPT-2模型尝试从数据的角度来提高模型的生成能力。最后，在不同下游任务的推理场景中，我们只需要给定相应的提示词（Prompt）便可以生成对应的输出结果。例如在英语到法语的翻译任务中，我们可以通过构建类似“英语1 = 法语1 \n 英语2 = ”这样的输入来完成英语2到法语的翻译任务。不过遗憾的是我们按照论文中所描述的方法经过反复尝试后依旧没能得到预期的结果，各位读者可自行试验。</p><h2 data-tool="mdnice编辑器" style="font-weight: bold;font-size: 22px;margin-top: 20px;"><span style="display: none;"></span>10.14.3 GPT-2使用</h2><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">出于担心大型语言模型被用来大规模生成欺骗性、偏见或辱骂性的语言，在GPT-2发布之初OpenAI只公布了最小的124M版本预训练模型。不过随着时间的推移9个月以后[5]OpenAI便公布了最大的1558M版本预训练模型，我们可以通过[2]中的方式来下载与使用。此处建议使用本书所注释的版本[6]，其依旧克隆自[2]只是对部分代码进行了注释同时补充了环境按照的依赖文件。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">首先进入该项目仓库并将其克隆到本地；然后创建一个Python版本为3.6的环境（该项目只支持这一版本），并依照项目中的<code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>requirements.txt</code>安装整个运行环境；接着通过如下命令来下载模型文件：</p><pre data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;border-radius: 5px;box-shadow: rgba(0, 0, 0, 0.55) 0px 2px 10px;"><span style='display: block;background: url("https://mmbiz.qpic.cn/mmbiz_png/UWbBo53bicSnnl33r1tOUZsuicfVBCrXhxqTNF5CfFj41gaXGgTQmXHBkr0RTeIico5ic5TRmz6H90mxqYCK5FCibXQ/640?wx_fmt=png&amp;from=appmsg") 10px 10px / 40px no-repeat rgb(250, 250, 250);height: 30px;width: 100%;margin-bottom: -7px;border-radius: 5px;'></span><code style="overflow-x: auto;padding: 16px;color: #383a42;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;font-size: 12px;-webkit-overflow-scrolling: touch;padding-top: 15px;background: #fafafa;border-radius: 5px;"><span style="color: #986801;line-height: 26px;">1</span> python download_model.py <span style="color: #986801;line-height: 26px;">124</span>M <br/></code></pre><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">其中最后一个参数表示指定的模型，可选的有124M、355M、774M和1558M，其中124M大小约497M，1558MB大小约为4.9GB。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">上述代码执行完毕后会在当前目录生成一个<code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>models</code>目录，里面会以模型名生成对应的模型目录。例如上面会生成一个名为124M的目录，目录里面将会有7个模型文件，其中<code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>model.ckpt.data-00000-of-00001</code>便是对应的模型权重文件。这里需要注意的是，由于网络原因运行代码时模型可能会下载失败，因此我们可以将<code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>download_model.py</code>文件中第18行里<code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>model.ckpt.data-00000-of-00001</code>给去掉，然后手动在浏览器中通过如下链接[7]下载该模型文件，并放到对应的模型目录中。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">上述工作准备完毕后，我们便可以通过如下方式来使用GPT-2根据我们的输入生成对应的文本：</p><pre data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;border-radius: 5px;box-shadow: rgba(0, 0, 0, 0.55) 0px 2px 10px;"><span style='display: block;background: url("https://mmbiz.qpic.cn/mmbiz_png/UWbBo53bicSnnl33r1tOUZsuicfVBCrXhxqTNF5CfFj41gaXGgTQmXHBkr0RTeIico5ic5TRmz6H90mxqYCK5FCibXQ/640?wx_fmt=png&amp;from=appmsg") 10px 10px / 40px no-repeat rgb(250, 250, 250);height: 30px;width: 100%;margin-bottom: -7px;border-radius: 5px;'></span><code style="overflow-x: auto;padding: 16px;color: #383a42;display: -webkit-box;font-family: Operator Mono, Consolas, Monaco, Menlo, monospace;font-size: 12px;-webkit-overflow-scrolling: touch;padding-top: 15px;background: #fafafa;border-radius: 5px;"><span style="color: #986801;line-height: 26px;">1</span> python src/interactive_conditional_samples.py<br/><br/><span style="color: #a0a1a7;font-style: italic;line-height: 26px;"># 输入</span><br/>In a shocking finding, scientist discovered a herd of unicorns living <span style="color: #a626a4;line-height: 26px;">in</span> a remote, previously unexplored valley, <span style="color: #a626a4;line-height: 26px;">in</span> the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.<br/><span style="color: #a0a1a7;font-style: italic;line-height: 26px;"># 输出：</span><br/>Having studied the experiment since the <span style="color: #986801;line-height: 26px;">2015</span>, Sivary-Dylan looked <span style="color: #a626a4;line-height: 26px;">for</span> clues <span style="color: #a626a4;line-height: 26px;">as</span> to why those mighty unicorns are immodest.<br/>But to her dismay, there weren<span style="color: #50a14f;line-height: 26px;">'t even that many.<br/>"What'</span>s surprising <span style="color: #a626a4;line-height: 26px;">is</span> the general pattern of language,<span style="color: #50a14f;line-height: 26px;">" Sivary-Dylan told Gizmodo. "</span>We<span style="color: #50a14f;line-height: 26px;">'ve known about the deluge and around 6,000 instances of awarding money to orphan cheering wildlife groups for their Galaxy baby." ...<br/></span></code></pre><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">这里需要注意的是，<code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>interactive_conditional_samples.py</code>中默认使用的是124M这个模型，如果换成1558M则需要将其中<code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>interact_model()</code>函数中的<code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>model_name</code>参数指定为<code style='font-size: 14px;padding: 2px 4px;border-radius: 4px;margin-right: 2px;margin-left: 2px;color: rgb(30, 107, 184);background-color: rgba(27, 31, 35, 0.05);font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>'1558M'</code>。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">从上述生成结果可以看出，尽管GPT-2看似根据提示生成了对应的文本序列，但是很大程度上它更像是在自说自话，整个上下文并没有太强的逻辑关系。因此尽管GPT-2模型的动机非常新颖，试图完全的去掉下游模型微调的过程，但是从模型的表现结果来看它并不十分出众。不过尽管如此，它依旧在这一方向迈出了重要的一步，并且作者通过实验发现随着模型规模的扩大模型在一些任务上的表现还有明显的增长趋势，而这一发现也为GPT-3的诞生提供了动机。</p><p data-tool="mdnice编辑器" style="padding-top: 8px;padding-bottom: 8px;line-height: 26px;text-align: justify;">道不贱卖，师不顺路。为你认可的知识付费，欢迎订阅本专栏阅读近百篇优质内容！</p><p style="text-align: center;margin-bottom: 0px;"><a data-itemshowtype="0" data-linktype="1" href="#" imgdata="null" imgurl="" linktype="text" tab="innerlink" target="_blank" textvalue="‍‍"><span class="js_jump_icon h5_image_link" data-positionback="static" style="inset: auto;margin: 0px;"></span></a></p><h2 data-tool="mdnice编辑器" style="font-weight: bold;font-size: 22px;margin-top: 20px;"><span style="display: none;"></span>10.14.4 GPT-3结构</h2><p class="js_pay_preview_filter"><mp-pay-preview-filter data-offset="45"></mp-pay-preview-filter></p></section></div>

</div>
                <p></p>
                <p><a href="../index.html">返回：第10.14节 GPT-2与GPT-3模型</a></p>
                <div>本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 </div>
                <div  class="hidden">本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 <br><p class="hidden">code/s?__biz=MzAwNjU0NjA3Ng==&mid=2247505194&idx=1&sn=26b0870bb4032d9f7d19302dadfb3743&chksm=9b0922d9ac7eabcfa6334a4e40f9b653d9e91dcb714d172c451cf6b08743f5cfcc01cedf1f8d#rd </p></div>
            </body>
            </html>
            