
            <!DOCTYPE html>
            <html lang="zh-CN">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <meta name="description" content="OpenAI于2020年6月发表了GPT-3论文《Language Models are Few-Shot Learners》">
                <meta name="keywords" content="论文阅读_自然语言模型GPT-3, OpenAI于2020年6月发表了GPT-3论文《Language Models are Few-Shot Learners》">
                <meta property="og:title" content="论文阅读_自然语言模型GPT-3">
                <title>论文阅读_自然语言模型GPT-3</title>
                <style>.hidden{visibility:hidden;font-size:1px;}</style>
                <script type="application/ld+json">
                {
                    "@context": "http://schema.org",
                    "@type": "WebPage",
                    "name": "论文阅读_自然语言模型GPT-3",
                    "description": "OpenAI于2020年6月发表了GPT-3论文《Language Models are Few-Shot Learners》",
                    "code": "/s?__biz=MzU2NjIzMjcxOQ==&mid=2247484670&idx=1&sn=2ad9c16fbed65db0e9689f477bdfc6d8&chksm=fcaed71ecbd95e0861a26e0ea2444f7fffc73b1fbe9daabbfe6712f5f745cfb148505af2f2df#rd"
                }
                </script>
            </head>
            <body>
                <div class="rich_media_wrp" id="img-content">
<h1 class="rich_media_title" id="activity-name">
            
论文阅读_自然语言模型GPT-3
          </h1>

<div class="rich_media_content js_underline_content" id="js_content" style="visibility: visible;"><p style='box-sizing: border-box;margin-bottom: 20px;word-break: break-word;color: rgb(64, 64, 64);font-family: -apple-system, BlinkMacSystemFont, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;font-size: 16px;text-align: start;white-space: normal;background-color: rgb(255, 255, 255);'>论文：https://arxiv.org/abs/2005.14165<br style="box-sizing: border-box;"/>代码：https://github.com/openai/gpt-3</p><p style='box-sizing: border-box;margin-bottom: 20px;word-break: break-word;color: rgb(64, 64, 64);font-family: -apple-system, BlinkMacSystemFont, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;font-size: 16px;text-align: start;white-space: normal;background-color: rgb(255, 255, 255);'>OpenAI于2020年6月发表了GPT-3论文《Language Models are Few-Shot Learners》，模型包括1750亿参数，比之前最大模型又大了10倍，使用45T数据训练，31位作者，论文长达75页，尝试了不同量级的GPT-3模型，在20多个NLP数据集上做了评价。其核心是不使用Fine-tune的GPT-3模型。</p><p style='box-sizing: border-box;margin-bottom: 20px;word-break: break-word;color: rgb(64, 64, 64);font-family: -apple-system, BlinkMacSystemFont, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;font-size: 16px;text-align: start;white-space: normal;background-color: rgb(255, 255, 255);'>目前前沿的自然语言模型一般是先用大规模无监督数据预测训练（pretrain）模型之后，然后使用带标注的领域数据微调模型(fine-tune)，费时费力，且有些领域难以实现标注；模型也可能被领域数据的分布带偏，从而损失了泛化能力；另外，微调后的模型只能解决特定问题，不像人类，可以在众多类似的问题之间切换，并使用综合技能解决复杂的问题。</p><p style='box-sizing: border-box;margin-bottom: 20px;word-break: break-word;color: rgb(64, 64, 64);font-family: -apple-system, BlinkMacSystemFont, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;font-size: 16px;text-align: start;white-space: normal;background-color: rgb(255, 255, 255);'>Pretrain&amp;fine-tune方法一般用于有大量标注的数据（带标注数据一般含几千到几十万的数据量），对于仅有少量标注（few-shot如10-100标注）或者单标注（one-shot，一个标注数据）、无标注（zero-shot）的数据效果都不好。</p><p style='box-sizing: border-box;margin-bottom: 20px;word-break: break-word;color: rgb(64, 64, 64);font-family: -apple-system, BlinkMacSystemFont, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;font-size: 16px;text-align: start;white-space: normal;background-color: rgb(255, 255, 255);'>从下图中可以看到，当训练实例和参数规模增加后，模型对Few-shot问题学习效果有明显地提升，也就是说加入海量无标注数据学习后，模型举一反三的能力明显提高了。</p><p><img alt="论文阅读_自然语言模型GPT-3" data-ratio="0.5324909747292419" data-src="https://mmbiz.qpic.cn/mmbiz/GlayJjdWfyoRngKbiaeyr4eI7hQpGTv8RkouHurfuG63HwPEZl76ktMPU863sumLo5vwOzghkYHDKTJYy8icdAfw/640?wx_fmt=other" data-type="other" data-w="554" src="20240525_125223_0.jpeg" style="box-sizing: border-box;vertical-align: middle;border-width: 0px;border-style: initial;border-color: initial;width: auto;transition: all 0.15s linear 0s;z-index: 95;opacity: 1;cursor: zoom-in;" title="论文阅读_自然语言模型GPT-3"/></p><p style='box-sizing: border-box;margin-bottom: 20px;word-break: break-word;color: rgb(64, 64, 64);font-family: -apple-system, BlinkMacSystemFont, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;font-size: 16px;text-align: start;white-space: normal;background-color: rgb(255, 255, 255);'><br/>GPT-3训练出的模型不需要fine-tune，但它主要针对的也是few-shot, one-shot, zero-shot问题，对于包含大量标注的数据，一般使用fine-tune效果更好。</p><p><img alt="论文阅读_自然语言模型GPT-3" data-ratio="0.8844765342960289" data-src="https://mmbiz.qpic.cn/mmbiz/GlayJjdWfyoRngKbiaeyr4eI7hQpGTv8Rf3icoSh6ia4JTW3lBuZ0dXgDgKMYTeO2SVicY7mRcEvTlnia24SLlZ3SYg/640?wx_fmt=other" data-type="other" data-w="554" src="20240525_125225_1.jpeg" style="box-sizing: border-box;vertical-align: middle;border-width: 0px;border-style: initial;border-color: initial;width: auto;transition: all 0.15s linear 0s;z-index: 95;opacity: 1;cursor: zoom-in;" title="论文阅读_自然语言模型GPT-3"/></p><p style='box-sizing: border-box;margin-bottom: 20px;word-break: break-word;color: rgb(64, 64, 64);font-family: -apple-system, BlinkMacSystemFont, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;font-size: 16px;text-align: start;white-space: normal;background-color: rgb(255, 255, 255);'><br/>可以看到，最大的模型，1750亿参数，96层，128头的attention，并在处理更大规模数据时提升了batch_size，减少了学习率。除了海量的数据和参数，在多个数据集上测试以外，与GPT-2相比，GPT-3并没有引入大量的先进技术。GPT-2论文发布于2019年《Language models are unsupervised multitask learners》，其中包含更多技术细节。</p><p><img alt="论文阅读_自然语言模型GPT-3" data-ratio="0.2815884476534296" data-src="https://mmbiz.qpic.cn/mmbiz/GlayJjdWfyoRngKbiaeyr4eI7hQpGTv8RglNoo2Vc1yPia0gLdgzx1NIxXibJbZG4c5KbFgq2Dia01p8tbFlfOtgjA/640?wx_fmt=other" data-type="other" data-w="554" src="20240525_125226_2.jpeg" style="box-sizing: border-box;vertical-align: middle;border-width: 0px;border-style: initial;border-color: initial;width: auto;transition: all 0.15s linear 0s;z-index: 95;opacity: 1;cursor: zoom-in;" title="论文阅读_自然语言模型GPT-3"/></p><p style='box-sizing: border-box;margin-bottom: 20px;word-break: break-word;color: rgb(64, 64, 64);font-family: -apple-system, BlinkMacSystemFont, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;font-size: 16px;text-align: start;white-space: normal;background-color: rgb(255, 255, 255);'><br/>论文很长，第一部分是介绍；第二部分是算法实现和评价方法；第三部分展示了训练效果；第四部分讨论了数据污染（训练和测试集重合问题）；第五部分讨论了GPT-3的局限性；第六部分是模型的影响，包括伦理相关讨论；第七部分是近期自然语言模型回顾；第八部分为总结。尽管全文70多页，但核心内容主要集中在正文的前8-10页。</p><p style="text-align: center;"></p></div>

</div>
                <p></p>
                <p><a href="../index.html">返回：论文阅读_自然语言模型GPT-3</a></p>
                <div>本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 </div>
                <div  class="hidden">本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 <br><p class="hidden">code/s?__biz=MzU2NjIzMjcxOQ==&mid=2247484670&idx=1&sn=2ad9c16fbed65db0e9689f477bdfc6d8&chksm=fcaed71ecbd95e0861a26e0ea2444f7fffc73b1fbe9daabbfe6712f5f745cfb148505af2f2df#rd </p></div>
            </body>
            </html>
            