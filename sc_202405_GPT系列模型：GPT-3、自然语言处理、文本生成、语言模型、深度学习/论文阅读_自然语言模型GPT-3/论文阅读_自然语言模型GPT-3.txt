


            
论文阅读_自然语言模型GPT-3
          




论文：https://arxiv.org/abs/2005.14165代码：https://github.com/openai/gpt-3OpenAI于2020年6月发表了GPT-3论文《Language Models are Few-Shot Learners》，模型包括1750亿参数，比之前最大模型又大了10倍，使用45T数据训练，31位作者，论文长达75页，尝试了不同量级的GPT-3模型，在20多个NLP数据集上做了评价。其核心是不使用Fine-tune的GPT-3模型。目前前沿的自然语言模型一般是先用大规模无监督数据预测训练（pretrain）模型之后，然后使用带标注的领域数据微调模型(fine-tune)，费时费力，且有些领域难以实现标注；模型也可能被领域数据的分布带偏，从而损失了泛化能力；另外，微调后的模型只能解决特定问题，不像人类，可以在众多类似的问题之间切换，并使用综合技能解决复杂的问题。Pretrain&fine-tune方法一般用于有大量标注的数据（带标注数据一般含几千到几十万的数据量），对于仅有少量标注（few-shot如10-100标注）或者单标注（one-shot，一个标注数据）、无标注（zero-shot）的数据效果都不好。从下图中可以看到，当训练实例和参数规模增加后，模型对Few-shot问题学习效果有明显地提升，也就是说加入海量无标注数据学习后，模型举一反三的能力明显提高了。GPT-3训练出的模型不需要fine-tune，但它主要针对的也是few-shot, one-shot, zero-shot问题，对于包含大量标注的数据，一般使用fine-tune效果更好。可以看到，最大的模型，1750亿参数，96层，128头的attention，并在处理更大规模数据时提升了batch_size，减少了学习率。除了海量的数据和参数，在多个数据集上测试以外，与GPT-2相比，GPT-3并没有引入大量的先进技术。GPT-2论文发布于2019年《Language models are unsupervised multitask learners》，其中包含更多技术细节。论文很长，第一部分是介绍；第二部分是算法实现和评价方法；第三部分展示了训练效果；第四部分讨论了数据污染（训练和测试集重合问题）；第五部分讨论了GPT-3的局限性；第六部分是模型的影响，包括伦理相关讨论；第七部分是近期自然语言模型回顾；第八部分为总结。尽管全文70多页，但核心内容主要集中在正文的前8-10页。




