
            <!DOCTYPE html>
            <html lang="zh-CN">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <meta name="description" content="NLP任务范式的发展： word-vectors+Task Specific Architecures Mu">
                <meta name="keywords" content="大模型 GPT-3, NLP任务范式的发展： word-vectors+Task Specific Architecures Mu">
                <meta property="og:title" content="大模型 GPT-3">
                <title>大模型 GPT-3</title>
                <style>.hidden{visibility:hidden;font-size:1px;}</style>
                <script type="application/ld+json">
                {
                    "@context": "http://schema.org",
                    "@type": "WebPage",
                    "name": "大模型 GPT-3",
                    "description": "NLP任务范式的发展： word-vectors+Task Specific Architecures Mu",
                    "code": "/s?__biz=MzU2ODgyNzEzNA==&mid=2247484545&idx=1&sn=c93d67dfc29f888a6fd972dcffe79398&chksm=fc894910cbfec0068f345ce97ec834dc7d293ef2cb04299dbd9cbb2e887f0b8fd56ed15485b0#rd"
                }
                </script>
            </head>
            <body>
                <div class="rich_media_wrp" id="img-content">
<h1 class="rich_media_title" id="activity-name">
            
大模型 GPT-3
          </h1>

<div class="rich_media_content js_underline_content autoTypeSetting24psection" id="js_content" style="visibility: visible;"><p style="letter-spacing: 0.578px;white-space: normal;text-align: center;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-galleryid="" data-ratio="0.6046296296296296" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZE2EkicTxmBC8PksuX5yzAwIOYYC5AJZoa8zlYU1ghypIdgyTCn59lFzw/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130345_0.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;">NLP任务范式的发展： </p><ol class="list-paddingleft-1" style="width: 577.422px;letter-spacing: 0.578px;white-space: normal;"><li><p>word-vectors+Task Specific Architecures </p></li><li><p>Multi Layer RNNs</p></li><li><p>Pre-trained transformers + Fine-tuning</p></li></ol><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5111111111111111" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZEvgQBibmVa6uOJXwfRAxv0AANtJm7Pib1PNibJz04UyMznnYoauSib7O02A/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130346_1.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;">一般使用类似于Bert解决问题的方式：Pre-training + Fine-tuning的模式<br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5111111111111111" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZE0bNaMM96lf8VKJrsMduhTkS7IbFF3ExnjQXG2E42ZCBW5pibA6DyQVw/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130348_2.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><strong>Pre-Training-&gt;Fine-Tuning范式的问题</strong><br/></p><ol class="list-paddingleft-1" style="width: 577.422px;letter-spacing: 0.578px;white-space: normal;"><li><p>需要很大的task-specific datasets来进行finetuning</p></li><li><p>针对每个任务均需要重复类似的fine-tuning过程</p></li><li><p>会导致很多份类似的模型</p></li><li><p>fine-tuning后的模型仅在自己的任务上可能有好效果</p></li><li><p>经常在训练集合上效果好但真实环境就比较差</p></li></ol><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;">人类则不同： 仅需要较小数据即可学习 &amp; 能够进行迁移<br/></p><p style="letter-spacing: 0.578px;white-space: normal;">所以能否让模型具有人类的类似特性</p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.4787037037037037" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZEEV7DuhticeS1SypjXHibjWSeTmZSOey88Y53tzUVc0yeyq4ricZ6dReEg/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130349_3.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5101851851851852" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZErVWUXO5HVJ3ePwbkamqX3f6P3vFNQ7TcxXGG4jQ8tj5MDM3JMh7eiag/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130350_4.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZESiciafVLCCySEvdMjZhNPGmt9qLMnL0bxrY1EyWpaBsTrX32nQtzcG4g/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130351_5.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><strong>解题的思路</strong></p><ol class="list-paddingleft-1" style="width: 577.422px;letter-spacing: 0.578px;white-space: normal;"><li><p>Scale-Up</p></li><li><p>In Context Learning</p></li></ol><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><strong>Scale-Up</strong><br/></p><p style="letter-spacing: 0.578px;white-space: normal;">OpenAI的研究表明， <strong>Larger is Better!</strong><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.7814814814814814" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZEVNnNrwk1vNNicT4VpqvicCrOVoB6icFQoIU3ZKALue3Fs9D3h3q5aVnQg/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130353_6.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.562962962962963" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZEkeyq5icywcy37sX5YE1ADZcuIUg849XmP094z7etWXfWZ1w9OQ3p7BQ/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130355_7.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;">在GPT-3之前，模型的参数一般就是亿OR十亿级别， GPT-3直接到了1750亿， 是GPT-2的100多倍<br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5861111111111111" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZEAu98yOCicNiaG5YjA7wzzUiaUI59KwhQ3vKjc738ZNB5Iuaib5kI10hxGA/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130356_8.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.3990740740740741" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZEZoe7NXYBNtjERZAziaYhfW0icKbHGibaS1LqibdJXMjpqTCd85FZ3sum0Q/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130357_9.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.4111111111111111" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWLWSK5c601BzNLsfibOlDvZELlxb1Xial4sl6rxGzpSdk9SFn2J6eYDGUMS68N7OmAdQuee1m6ZYQeA/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130358_10.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;">OpenAI的研究有以下发现：<br/></p><ol class="list-paddingleft-1" style="width: 577.422px;letter-spacing: 0.578px;white-space: normal;"><li><p>模型效果和规模有很强的关联（参数量， 训练数据量， 处理的token数， 计算量）， 和模型结构弱相关。</p></li><li><p>性能亿scale几乎是power-law的关系</p></li><li><p>Transfer improves with test performance</p></li><li><p>large models are more sample efficient<br/></p></li></ol><p style="letter-spacing: 0.578px;white-space: normal;"><strong><br/></strong></p><p style="letter-spacing: 0.578px;white-space: normal;"><strong><br/></strong></p><p style="letter-spacing: 0.578px;white-space: normal;"><strong>In Context Learning</strong></p><p style="letter-spacing: 0.578px;white-space: normal;">In Context Learning: 在pre-training的时候，学习不同的skill和子任务， 然后在进行inference的时候， 使用NLP的方式提示models完成任务。</p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5518518518518518" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbpJlvsGAib52EN7wV1icfuOSYRIh1VswvsVbcf3YZeh4rxeHNoubcjWwQ/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130400_11.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.562037037037037" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbibbRFzyCA2rp2fPbnJ8zIfGmc5HZT6gyWJj3VqbQbJR8jckm2eUXiagg/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130401_12.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;">In-Context Learning是Meta-Learning<br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5009259259259259" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbAtulb8agZMicrV3RsIs8HtzU6iboP5GVWob9FTouI46hNanCQsATDuMQ/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130403_13.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5898148148148148" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbEfZc5bUEl0DxYRpKjjdTG8yZn6L3VrKANddVXjFRkz6iajw60KI4LVA/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130404_14.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.65" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbpNhEqxbgcFKh2Y7XSRSIdhTFtTH8m1micwClavgzicXHt5wQibbZfa2ew/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130405_15.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;">上图可以得到以下结论：<br/></p><ol class="list-paddingleft-1" style="width: 577.422px;letter-spacing: 0.578px;white-space: normal;"><li><p>Larger models learn better in-context learning， 大模型容量较大</p></li><li><p>prompt matters， prompt对效果影响较大， 性价比最高，效果提升最快的是one-shot</p></li></ol><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5175925925925926" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdb5SoIrct89icOaev52txHmX0H5yt7xgvj14iaekONJh7wH3bZGatLFIRg/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130407_16.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;">In-Context Learning的特点：<br/></p><ol class="list-paddingleft-1" style="width: 577.422px;letter-spacing: 0.578px;white-space: normal;"><li><p><span style="letter-spacing: 0.578px;">在pre-training的时候，学习不同的</span><span style="letter-spacing: 0.578px;">skill</span><span style="letter-spacing: 0.578px;">和子任务， 然后在进行i</span><span style="letter-spacing: 0.578px;">nference的时候， 使用</span><span style="letter-spacing: 0.578px;">NLP的方式提示</span><span style="letter-spacing: 0.578px;">models完成任务</span></p></li><li><p>模型仅train一次</p></li><li><p>在使用的过程中， 权重固定无需再train</p></li></ol><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><strong>GPT-3结构</strong></p><ol class="list-paddingleft-1" style="width: 577.422px;letter-spacing: 0.578px;white-space: normal;"><li><p>GPT-3为仅使用decoder结构，其中使用了masked self-attention， 对比之下， bert使用encoder， T5使用 encoder + decoder</p></li><li><p><span style="letter-spacing: 0.578px;">GPT-3非常大</span><span style="letter-spacing: 0.578px;">， 参数为</span><span style="letter-spacing: 0.578px;">17</span><span style="letter-spacing: 0.578px;">5B</span><span style="letter-spacing: 0.578px;">， 是GPT-2的117倍</span><span style="letter-spacing: 0.578px;">， 更多参数， </span><span style="letter-spacing: 0.578px;">更大的训练语料</span><span style="letter-spacing: 0.578px;">， 更长的训练时间， </span><span style="letter-spacing: 0.578px;">更大的</span><span style="letter-spacing: 0.578px;">cont</span><span style="letter-spacing: 0.578px;">ext window</span><span style="letter-spacing: 0.578px;">（更适用于one-shot</span><span style="letter-spacing: 0.578px;">）</span>, 模型使用300B tokens进行训练<br/><br/></p></li></ol><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.48333333333333334" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbJdpFy4dIWpQgeq0agsfBeJXCRf8GMMX83pO9ngMp71K99PahCZzlBA/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130408_17.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;">GPT-3为仅使用decoder结构，其中使用了masked self-attention， 对比之下， bert使用encoder， T5使用 encoder + decoder</p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.43703703703703706" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbA88AjsRUK6r6jF8KbM7eFoAXMMs688cpqMFrE9CZKjsqibjqia9icGX1g/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130410_18.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.6129629629629629" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbC7YQTbiaTdxc6szjQzAjIdzvM9eOAzJNp0WzW1jW6f4Dib7AdO4KkCNA/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130411_19.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;">GPT-3非常大， 参数为175B， 是GPT-2的117倍， 更多参数， 更大的训练语料， 更长的训练时间， 更大的context window（更适用于one-shot）<br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.46944444444444444" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbpWwscNuMZMSTyc7TZib7vWYDj3M5sx4rhGzVia0xGKEHMfhpeFdgVib9Q/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130413_20.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.44351851851851853" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbKZuRuR1IohUkct7jR0ibNPkC90Eb0htNLG0UAa7wYG1H8qyl6UCUwVg/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130414_21.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.425" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbgzbzMscPqV5bdd5viaia17YgYdsUksXRu1MefhnIxa8emPZJcUm8VCaw/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130415_22.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><strong>训练数据集</strong></p><p style="letter-spacing: 0.578px;white-space: normal;text-align: center;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-galleryid="" data-ratio="0.5314814814814814" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbkXj4NYNasZBrmaawpJpdqd07CegQQEQPtXS1kiamgPFmJK2qLd0CoMQ/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130416_23.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;text-align: center;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;text-align: center;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-galleryid="" data-ratio="0.4601851851851852" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbBygFmBP3Yv8wkPCW46uebqhiaNcU25QgyCVBjZJH6hV8fvChGcKy8xg/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130418_24.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;text-align: center;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;text-align: left;">中间省略了较多效果图， GPT-3可以在很多场景去的好的效果， 但很多都达不到fine-tuned SOTA......<br/></p><p style="letter-spacing: 0.578px;white-space: normal;text-align: left;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><strong>目前的发展趋势</strong><br/></p><ol class="list-paddingleft-1" style="width: 577.422px;letter-spacing: 0.578px;white-space: normal;"><li><p>模型容量越大，效果越好（逐渐也有一些不同的声音）</p></li><li><p>有越多提示演示效果越好：few-shot&gt;one-shot&gt;zero-shot</p></li><li><p>目前还没有看到模型size的天花板瓶颈所在<br/></p></li></ol><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.3675925925925926" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbzcI9kLwy8CLaruw6U1lLOoGw1ZOmJwDibLWunzD5TXUm7VULBVMQXsA/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130419_25.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.28888888888888886" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbiazqVSmm2uy0tp4CNDiaukkq96NxcHSicicaaNuxOeTztz4CqTr3Zdsf2A/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130421_26.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5583333333333333" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbOMia0knI6Wtr2VonlKG68djVgc8ntO44dMpn3NDvf8FMHSkv12R447g/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130422_27.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5074074074074074" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbxrIMtS4pz7uDVe7zgqa8v3tKcJGibAE44b85PwicE2hhe32W9cUDGbaQ/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130423_28.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.4935185185185185" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbnneia9ETboAsJcyXJfwD0kic3gZkZ1crQeFk9yib78kiasTqakI3icsibvBw/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130424_29.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5398148148148149" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdb4rgSNbIw59BUaHg865GsNJEickfUXlMg0h8hicBS53NiaNzZW2FzmKK4Q/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130425_30.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><img alt="大模型 GPT-3" class="rich_pages wxw-img" data-ratio="0.5203703703703704" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/qFb06zicSYWJDwP1iaqD6K1lHibV0otNAdbshyq1gNGm0ibia5Licq4Zsc48ibIN39iaJEnl0DsOdTnibSOntPy5ey0VskQ/640?wx_fmt=png" data-type="png" data-w="1080" src="20240525_130427_31.jpeg" title="大模型 GPT-3"/></p><p style="letter-spacing: 0.578px;white-space: normal;"><br/></p><p style="letter-spacing: 0.578px;white-space: normal;">中间省略了较多页对于模型偏见，风险的讨论。<br/></p><p style="letter-spacing: 0.578px;white-space: normal;text-align: center;"></p><p style="letter-spacing: 0.578px;white-space: normal;text-align: left;">PaLM参数量为540B<br/></p><ol class="list-paddingleft-1" style="width: 577.422px;letter-spacing: 0.578px;white-space: normal;"><li><p style="text-align: left;">参数量相当于是GPT-3的3倍</p></li><li><p style="text-align: left;">在28个NLP tasks上超过了GPT-3</p></li><li><p style="text-align: left;">在一部分任务上超过了SOTA</p></li><li><p style="text-align: left;">增加scale+chain of thought prompting 带来的效果提升</p></li></ol><p><br/></p><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div>

</div>
                <p></p>
                <p><a href="../index.html">返回：大模型 GPT-3</a></p>
                <div>本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 </div>
                <div  class="hidden">本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 <br><p class="hidden">code/s?__biz=MzU2ODgyNzEzNA==&mid=2247484545&idx=1&sn=c93d67dfc29f888a6fd972dcffe79398&chksm=fc894910cbfec0068f345ce97ec834dc7d293ef2cb04299dbd9cbb2e887f0b8fd56ed15485b0#rd </p></div>
            </body>
            </html>
            