


            
AI应用产业落地
          




目录1、LLM大语言模型的局限性2、适合企业的AI 应用开发范式3、AI产业界&学术界对AI发展的展望4、RAG的介绍5、AI-Agent的介绍如果从大类划分AI人工智能领域的话，我们主要划分为：🦜 自然语言技术（NLP）👁️ 计算机视觉技术（CV）本次我们主要介绍自然语言（NLP）领域LLM大模型的应用，先说结论，在实际的LLM大模型的应用用产业落地，最靠谱的是这两条工程路线：📕 RAG（Retrieval-Augmented Generation 检索增强生成）🤖 人工智能代理 AI-Agent （功能更强大）这里先不急着解释啥是RAG，下文会说明，这里先介绍下人工智能代理 AI-Agent ，可以把它视为RAG的进阶版，但是，这个AI应用功能更加强大，不仅具备LLM大模型的基础能力，还会根据目标和环境，自主选择要使用的工具、调用函数API和调用代码解释器。我们称它具备Function-call的能力。一、LLM大语言模型的局限性在介绍为什么是这两条工程路线之前，我们先了解现在面向To C端的Chat-GPT等一众聊天应用的局限性。To C 端Chat-GPT聊天机器人首先有几个局限性：☝🏻1、大语言模型都有【知识截止时间】限制的，以Chat-GPT（GPT-3.5-Turbo）为例，它的训练数据截止时间是在2022年1月份。☝🏻 2、生成式大语言模型具有【幻觉】，用白话讲就是如果碰到它不知道的问题时，它会一本正经的胡说八道🤬。☝🏻 3、大语言模型的训练数据是通用型的，没有特定领域的专业知识。如果你问Chat-GPT（GPT-3.5-Turbo）2023年发布的苹果15Pro手机参数时，它大概率会告诉你🤖：我不知道。这还不算大问题，至少它不知道会老实反馈给你。让人头痛的是如果你问它没有见过的知识时，它大概率会一本正经的对你胡说八道🤬。所以，如果我们想做一个产业落地应用，比如：企业知识库、或企业自己的AI智能客服，单纯拿LLM大语言模型来做是不现实的，即便强如GPT-3.5的大语言模型。二、适合企业的AI 应用开发范式AI应用产业落地，需要我们在 AI 软件工程上做一些创新性的开发构建。在众多的实践中，从业者主要做的工作是对开源LLM大模型进行微调（Fine-tuning），比较常见的微调手段是冻结预训练LLM大模型的参数，然后自主构建深度神经网络层，微调深度神经网络层的参数。有能力的企业，通过对预训练LLM大模型进行全参微调，训练自己的企业大模型（比如国内的阿里通义千问、百度文心一言等）。从实践后的经验来看，全参微调无疑是提升LLM大模型性能的杀手锏。当然，这种方法也是最消耗资源、代价最高的微调方式。无论何种微调手段，微调的作用其实都是对LLM大模型做如下两个方面的优化：📘解决大模型知识截止问题，追加特定领域或实时数据，让大模型学习和理解。☝🏻 “数据对齐”，防止胡说八道，🈲止扭曲事实性内容的描述，禁止表达有害的、负面的言论。要对LLM大模型进行微调，看似简单，其实是一件没有太多选择，且十分有挑战的事情。🧐 没有太多选择，是因为你只能选择开源系的LLM大模型。十分有挑战，是因为💰高昂的成本阻挡了大部分组织。LLM大模型的训练成本跟模型的参数成正比关系。以目前最具代表性的闭源派Open ai 公司的GPT家族产品来看：GPT-3（参数约1750亿），预训练一次花费约140万美金；GPT-4（参数约16000亿），预训练一次花费约6300万美金；用来微调的开源模型参数一般有：7B（70亿）、13B（130亿）、70B（700亿）三个规格，并不是开源的没有千亿参数的模型，而是国内一、二线企业没几家有能力对千亿参数模型做全参微调。而低于7B（70亿）参数的开源模型因为基础素质太差，微调对于整体能力的提升无济于事。开源LLMs+Fine-tuning微调，结合模型的参数量，需要一定量的GPU开销、微调训练策略、匹配的数据集、AI算法工程师、数据预处理工程师……想想这些硬件、成本和人才需求，基本劝退绝大多数的小微企业。所以，想把AI应用做到产业落地，唯一可行的方法只剩下：RAG路线和AI-Agent路线。下表所列，将企业级别（非严谨划分）和自然语言 AI 领域工程难度，做了参照：难度描述国外一线国内一线国内二、三线小微企业L1自主开发LLMs✅✅✅绰绰有余❌设备约束❌不可能❌不可能L2LLMs ➕ 全参微调✅✅✅绰绰有余✅✅小参数可能✅小参数可能❌不可能L3LLMs ➕ RAG✅✅✅绰绰有余✅✅✅绰绰有余✅✅✅绰绰有余✅部分可能市面上很多名头的AI应用（语言类的），无非是 RAG 和 AI-Agent 两个工程范式的变种而已。📝PS：很多投资机构不看好闭源大模型，并非他们有多少高明的洞见，亦或闭源大模型不够优秀，仅是因为目前高性能的AI大模型开发范式是以高耗能、高门槛（GPU设备等）为代价。Old-money资本从不缺钱垄断底层技术，绝大多数投资机构他们在AI赛道上没得选，在核心的底层大模型开发上被排除在外，只能在上层应用上拥抱开源。三、AI产业界&学术界对AI发展的展望2024百度AI开发者大会[2]结合目前 AI 行业的发展动态，李彦宏在2024年百度开发者大会上分享了百度踩了无数坑之后的经验总结，把今后百度LLM大模型的发展和实践重心放在以下3方面：📌基础模型的MOE混合专家架构
解决高能耗的开发范式📌大模型的“蒸馏”技术
把大模型的知识蒸馏到小模型上📌RAG技术
与MOE架构结合，打造更强悍的AI-Agent我们再来看看AI领域大佬——吴恩达（Andrew Ng）博士对产业发展的看法吴恩达（Andrew Ng）博士履历：斯坦福大学 (Stanford University) · 计算机科学系客座教授；DeepLearning.AI的创始人，Landing AI的创始人兼CEO， AI Fund的普通合伙人，曾出任百度AI Group及谷歌Brain负责人。在机器学习、机器人和相关领域撰写或合作撰写了逾200篇研究论文。2013年入选美国《时代》周刊全球最具影响力的100人。大佬履历太耀眼，此处省略。2024年3月29日美国红杉资本AI Ascent活动吴恩达演讲[3]以下是吴恩达在此处会议的部分内容总结：1、用 GPT-3.5进行零样本提示，它的正确率是 48%。GPT-4的表现要好得多，正确率是 67%。但是，如果你在 GPT-3.5的基础上建立一个 AI-Agent 的工作流，它甚至能比 GPT-4做得更好。2、Agents工作流的出现，语言模型的能力有望在今年得到显著提升。3、AI Agents 有四种设计模式：一、反思（让模型检查和修正自己的输出）；二、工具（调用搜索、代码执行等外部工具）；三、规划（拆解复杂任务，制定执行计划）；四、多 Agent 协作（让模型扮演不同角色，通过协作完成任务）为了防止断章取义，有兴趣的可以查看该会议内容和其他嘉宾发言。✍🏻业界苦现在LLM大模型的开发范式久矣，这种以堆叠大参数、消耗电能，靠硬件设备大力出奇迹的方式显然把很多从业者排除在外，产业界的发展正在往小模型、MOE混合专家架构、多模态的方向发展，让AI技术普及开来，并从中获取商业利益。四、RAG的介绍RAG（Retrieval-Augmented Generation ）检索增强生成，最早是由Facebook AI 研究机构（FAIR）与其合作者在2021年发布的论文“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”提出的，其作用是帮助模型查找外部信息以改善响应。RAG技术十分强大，它已经被必应搜索、百度搜索以及其他大公司的产品所采用。从智能体的角度看：☝🏻 RAG = LLMs模型 （有截止的通用数据）+  向量数据库（私有领域数据）RAG的业界工程实践表明，在多数实际应用上，该技术可以解决LLM大模型的知识截止问题，很大工程度上减少了大模型的幻觉（🤬胡说八道）问题。且在部分应用场景下，可以媲美LLMs大模型+  Fine-tuning微调的效果。RAG的具体产品形式有：👋🏻 企业知识库问答系统🧑🏻‍💻企业智能客服RAG是一个AI应用工程实践，从工程各环节的难度和企业开发够得着（非严谨定义）的角度总结👇：工程环节小企业能做的1、文档分割算法✅自研，✅调用API2、词嵌入模型❌自研，✅调用LLMs模型API3、向量数据库检索算法❌自研，✅调用算法API4、提示词工程✅自研5、LLMs大模型❌自研，✅调用LLMs模型API我们在这里并不讨论具体开发上的方案和难点，我们先以业务视角介绍RAG产业落地应用——企业知识库产品的大体流程：👋🏻离线工作：使用文档分割算法、词嵌入模型，将企业知识库数据（支持Markdown、PDF、Word、Excel等格式）导入到向量数据库中。👋🏻在线工作：Step1：用户输入查询问题Query；Step2：将用户的查询Query转化为向量化；Step3：使用KNN算法，将用户向量化后的查询Query跟企业知识库数据匹配；Step4：提示词模版，结合返回的最相似 Top-k 段落知识和用户查询Query；Step5：将组合的Prompt输入给LLM大模型；Step6：LLM大模型返回结果给用户；RAG的整个工程，除了我们事先预编译的服务外，整个过程：1️⃣ 用户检索知识的结果2️⃣LLM大语言返回给用户的回答都是没有人为事先“硬编码”、具体规则干预的。且返回给用户的回答，并不是简单的知识抄写，而是结合具体知识和用户查询做了总结回复的，这种结果的智能化更多的是LLM大语言模型的基础能力提供的，当然，也跟RAG工程的设计有必然联系。所以，RAG技术应用可以明显的跟以前的软件开发范式（事先“硬编码”写死规则）区别开，让整个系统的数据处理过程可以根据不同的泛化环境（用户输入、内部结构）而调整，这种处理过程的智能化、最终体现为结果的智能化。这里演示下使用Meta公司4月18日最新开源的Llama3 的模型，结合百度弱智吧数据集做个演示：五、AI-Agent的介绍从智能体的角度看：☝🏻 AI-Agent =  LLMs模型 （有截止的通用数据）+  向量数据库（私有领域数据）+ 动态规划（分拆任务及观察）+ Function-call 工具（调用API或代码解释器）上述我们介绍了结合LLM大模型+RAG技术带来的明显变革，脱离与人类工程师事先“预编码”的工程手段，实现了系统数据处理的智能化。但是，这种程度的RAG技术，还不能真正体现出系统通用人工智能AGI（Artificial-General-Intelligence）的效果。因为这阶段的AI应用就像一个聪明的大脑🧠，它看上去会思考、推理、总结你提出的问题。但是我们并不仅仅要求它会思考，人类希望在用户向它提出问题的时候，它不仅能帮我们寻求最合适的答案，还需要它能理解我们更复杂的指令，并要求它在用户需要的时候，有能力帮用户实现！如果我们希望AI应用表现得像是一个人类的话，那我们应该以 Like-human 的角度看待AI应用的工程结构。我们先来看，为什么LLM大模型具备生成能力？拿人类孩子的学习做个对比：👦🏻人类小朋友的学习= 22年+[学习Data：数学、语文、英语、物理、艺术……]+因人而异的费用🤖GPT-3的学习= 3个月+[通用Data：语言的词法、语法、语意、逻辑，约45TB资料]+140万美金那么，我们可以这么看待AI工程的结构：第一阶段：LLM（有截止知识的大脑🧠）第二阶段：RAG = LLM（有截止知识的大脑🧠）+ 向量数据库（📕私有领域知识）第三阶段：AI-Agent = LLM（有截止知识的大脑🧠）+ 向量数据库（ 📕私有领域知识）+Planning（思考🤔和观察👀）+ Memory（长短期记忆 💬）+ Tools （工具应用🔨）如果说RAG看上去仅仅是造就一个知识不过时的聪明的大脑🧠，那 AI-Agent 智能体就是给大脑🧠装上思维链⛓️，再加上手脚👋🦶，让它进一步帮助人类完成目标。AI-Agent 的具体产品形式有：💬高级问答系统🧑🏻‍💻 高级电商客服当然，AI-Agent还有很多实际的应用，我们这里就不开展描述了。我们还是以业务视角介绍AI-Agent产业落地应用——高级电商客服的大体流程：👋🏻离线工作：使用文档分割算法、词嵌入模型，将企业知识库数据（支持Markdown、PDF、Word、Excel等格式）导入到向量数据库中。👋🏻在线工作（以线性Agent组件举例）：Step1：用户输入查询问题Query；Step2：将用户的查询Query转化为向量化；Step3：使用KNN算法，将用户向量化后的查询Query跟企业知识库数据匹配；Step4：结合返回的最相似Top-k段落知识和用户查询Query,组合成Prompt，输入给Agent。Step5：[> 🧠Agent开始🤔拆解任务；> 🤖查询下缓存，了解用户以前的查询问题；> 🤖结合查询，首先我要确认下问题1，🔧调用业务API1查询，返回结果1；> 🤖拿到结果1，需要再确认下问题2，🔧调用业务API2查询，返回结果2；> 🤖对比结果1和结果2，我终于知道最终答案了❗️]Step6：🧠Agent返回结果给用户；reAct介绍引用论文[4]通过代码演示Agent思维链式例最后，我们拿RAG工程路线和AI-Agent工程路线打造的客服问答系统做个对比展示：RAG路线AI-Agent路线参考资料[1] 本文作者电子邮箱: kerwin@euler.ac.cn[2] 2024年百度AI开发者大会: https://create.baidu.com/?lng=zh[3]2024年3月29日美国红杉资本AI Ascent活动吴恩达演讲视频地址: https://www.douyin.com/search/%E7%BE%8E%E5%9B%BD%E7%BA%A2%E6%9D%89%E8%B5%84%E6%9C%ACAI%20Ascent%E6%B4%BB%E5%8A%A8%E4%B8%8A%20%E5%90%B4%E6%81%A9%E8%BE%BE?aid=f73a13d9-954e-4b21-b11a-3ad205b4c99a&modal_id=7350934566363827497&type=video[4] reAct介绍引用论文: https://arxiv.org/pdf/2210.03629.pdf




