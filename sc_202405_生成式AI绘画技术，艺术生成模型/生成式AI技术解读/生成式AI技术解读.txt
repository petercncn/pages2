


            
生成式AI技术解读
          




生成式AI（Generative AI）是一种人工智能技术，利用机器学习模型和深度学习技术，通过研究历史数据的模式来生成新内容，可以是文本、图像、音频或视频。生成式AI不是根据给定的规则或数据生成输出，而是自主生成全新内容，类似于人类的创造（图1）。比如近来广受关注的聊天机器人ChatGPT，其所采用的核心模型GPT-3就可以生成高质量的自然语言文本，可用于聊天、写作、自动化客服等领域。文|梦飞图1 基于规则的聊天机器人通常使用“if-else”语句或其他类似技术对预设的问题和回答进行匹配，而类似ChatGPT的聊天机器人则通过预训练的大型语言模型实时生成自然的答案　　生成式AI发展简史　　随着大数据和计算机处理能力的不断增强，人们开始期望计算机能够像人一样创造内容和产生创新想法。这导致了生成式AI技术的兴起，使计算机可以模仿人类的创造力，为我们带来更多的创新和发展。　　生成式AI的历史可以追溯到20世纪50年代和60年代，当时人们开始使用概率论和信息论来建立语言模型。这些模型可以将输入的文本数据转换成概率分布，然后基于这些概率分布生成新的文本（图2）。　　图2 早在人工智能出现以前人们就已经开始语言模型的相关研究，比如1906年马尔可夫提出的马尔可夫链可以应用于多个不同的领域，而它也算得上是一种简单的语言模型　　20世纪90年代，随着神经网络技术的发展，生成式AI进入一个新的阶段。神经网络可以通过训练来自动学习文本数据的潜在结构和模式，并且可以用这些模式来生成新的文本。其中一个著名的生成式模型是基于循环神经网络（RNN）的语言模型，它可以预测一个句子中的下一个单词，并将其作为输入来预测后面的单词，从而生成连续的文本。　　近年来，随着深度学习技术的进一步发展，生成式AI取得了重大进展。其中出现了不少著名的模型，包括生成对抗网络（GAN）、变分自编码器（VAE）、扩散模型（Diffusion Models）和转换器模型（Transformer）等。这些模型可以生成高质量的图像、视频、音频和自然语言文本，并在许多应用领域产生了巨大的影响，例如自然语言处理、计算机视觉和音频处理等。　　生成式AI的工作流程　　生成式AI的基本原理是使用概率模型或神经网络模型，将已有数据的结构和规律学习到模型中，并基于这些结构和规律生成新的数据。　　1、模型训练　　首先，需要通过大量的训练数据来训练生成式AI模型。在训练过程中，生成式AI模型会学习输入数据的概率分布和结构，这些数据可以是文本、图像、音频或视频等。　　2、模型选择　　模型训练完成后，需要选择合适的模型来生成新的数据。不同类型的数据需要选择不同的模型来生成，比如自然语言文本可以使用RNN或LSTM模型来生成，图像可以使用GAN、VAE或扩散模型来生成（图3）。　　图3 不同类型的生成式模型比较　　3、生成数据　　一旦选择好合适的模型，就可以使用该模型来生成新的数据。生成新数据的方式通常是随机采样或条件采样。随机采样是指从模型学习到的数据分布中随机抽样生成新的数据，而条件采样是指在输入一些条件的情况下，从模型学习到的条件分布中采样生成新的数据。　　4、评估生成结果　　生成的新数据需要经过评估来判断其是否符合预期。评估生成结果的质量是一个开放性的问题，它可以基于客观指标进行，也可以依赖人类主观感受进行，比如自然语言文本可以基于语法正确性、连贯性、意义合理性等指标进行评估，图像可以基于视觉质量、真实感等指标进行评估。　　5、调整模型　　根据生成结果的评估，可以对模型进行调整和优化，从而提高生成结果的质量。调整模型的方式通常包括增加训练数据、调整模型参数、优化模型结构等方法。　　总的来说，生成式AI的工作过程是一个迭代的过程，需要不断地调整模型和评估生成结果，从而得到更好的生成效果。以上是从生成的角度来说的，如果换成客户端使用的角度，其操作过程就简单得多了（图4）。　　第1步：用户输入条件设置，比如希望生成的内容的关键词、主题或上下文等信息。现在流行的说法称之为：Prompt（提示）。　　第2步：等待服务端模型的处理。　　第3步：获取服务端模型返回的生成内容。　　图4 用户的输入通过受训的语言模型处理最终输出全新的文本　　生成对抗网络（GAN）　　上面我们对生成式AI有了初步了解，接下来就对其发展过程中的几个关键技术分别加以介绍。首先是生成对抗网络（Generative Adversarial Networks，简称GAN），这是一种深度学习模型，由加拿大蒙特利尔大学Ian Goodfellow等人于2014年提出。随后几年，GAN模型迅速发展，成为AI绘画的主流模型。2018年，NVIDIA公司发布基于GAN模型的StyleGAN，用户只需随便涂抹几笔，StyleGAN即可将其所“画”的不规则色块转化为优美的风景、人物图片，吸引了不少人注意。2019年，一个名为“不存在的人”英语网站更是将AI绘画推向一个高潮，用户每次刷新该网站，都会自动生成一幅真假难辨的人脸画像，其逼真程度令人惊叹，在图像生成领域产生巨大影响。　　如今在网上可以找到不少GAN的有趣应用。以GANpaint为例，打开它的网页即可看到AI生成的多幅精美图片（图5，http://gandissect.res.ibm.com/ganpaint.html）。　　图5 基于GAN模型的AI绘画工具GANpaint　　图片左侧提供了tree（树）、grass（草地）、door（门）、sky（天空）、cloud（云）、brick（砖块）、dome（屋顶）等绘画元素，这些元素每一个都对应于一组20个神经元。下方则有draw（绘画）、remove（移除）等操作模式。选择某个绘画元素，再设置draw模式，然后在画面中随意涂抹即可将该元素添加到当前画面中。不用担心会“涂坏”画面，因为它会智能计算，自动确保画面的完整。反之，如果设置的是remove模式，则会消除画面中的相关元素。这一工具的实现原理，在https://gandissect.csail.mit.edu中有详细介绍。　　GAN由两个神经网络组成：生成器（Generator）和判别器（Discriminator）。生成器用于生成与真实数据（比如真人照片）类似的虚假数据（比如AI绘制的虚拟人照片），判别器则用于区分真实数据和虚假数据。这两个神经网络通过博弈的方式进行训练，即生成器试图欺骗判别器，而判别器则努力区分真实数据和虚假数据。训练过程中，生成器逐渐学习生成更加逼真的虚假数据，而判别器逐渐学习如何更好地区分真实数据和虚假数据。当生成器生成的虚假数据能够“骗”过判别器时，就说明生成器已经学会了生成与真实数据相似的数据了（图6）。　　图6 生成对抗网络GAN中生成器和判别器互相博弈　　如上图所示，GAN生成器的工作流程大致如下：　　1、随机噪声输入：生成器将随机的噪声向量作为输入，通常是基于某种概率分布的高维向量。　　2、通过神经网络生成图片：生成器使用深度神经网络将噪声向量映射到生成的图像空间，输出生成的图片。这个过程可以看作是将抽象的高维噪声向量转换成具体的图像表示。　　3、评估生成图片：生成器生成的图片经过判别器的评估。　　4、优化生成器：如果生成器生成的图片被判别器认为是真实的，那么生成器就被认为是生成了高质量的图像。生成器的目标就是通过不断地迭代，使自己生成的图片更加接近真实的数据分布。　　5、训练结束：通过不断训练迭代，生成器会逐渐提高自己的生成能力，最终生成高质量的图像。　　GAN判别器是一个二分类器，它的目标是将生成器生成的数据与真实数据分开，评估生成器生成的图像是否与真实数据相似。它的工作流程大致如下：　　1、接收输入数据：判别器将生成器生成的图像或真实数据输入到神经网络中。　　2、提取特征：判别器通过一系列卷积层、池化层、激活函数等，将输入数据映射到一组特征向量。　　3、二分类：判别器使用一个Sigmoid函数（一种激活函数）将特征向量映射到一个0到1之间的值，表示输入数据属于真实数据的概率。如果判别器判断输入数据属于真实数据，则输出接近1的概率值；如果判断输入数据属于生成器生成的数据，则输出接近0的概率值。　　4、优化判别器：判别器通过不断训练迭代，优化自己的判别能力，使其可以更准确地将生成器生成的图像与真实数据分开，从而推动生成器的生成能力不断提升。　　GAN不只应用于图像生成领域，它也可以处理其他类型的数据（如文本），在自然语言处理、金融、医学、游戏开发等多个领域都有出色的表现。当然GAN的艺术表现可能是最为出名的。2018年，法国艺术团体Obvious使用名为Eerie AI的生成对抗网络创建了《爱德蒙·贝拉米肖像》（Edmond de Belamy，图7），这个根本不存在的虚拟人肖像画被以40多万美元的高价成功拍卖，在当时社会引发了广泛的讨论。与《爱德蒙·贝拉米肖像》同批创作的肖像作品共有11幅，被称之为《贝拉米家族》（图8）。　　图7 基于GAN创作的《爱德蒙·贝拉米肖像》，其训练数据采集了14～20世纪的15000幅肖像画作品　　图8 《贝拉米家族》系列肖像画　　扩散模型（Diffusion Models）　　GAN之前，AI绘画效果基本上都是些似是而非的“抽象画”，没有得到太多的关注。GAN出现之后，其逼真的绘画效果让人眼前一亮，但即使如此AI绘画依然只是在小众的圈子里流行。直到2021年OpenAI的DALL-E出世，及其后推出的升级版DALL-E 2，才真正让AI绘画进入大众视野，一时风靡网络。之所以如此，一方面是DALL-E 2的绘画效果几乎可以与专业画师相媲美，一方面是它在用户端的操作极其简单：用户只需输入自然语言文本对绘画内容进行描述，即可得到相应的精美绘画作品。OpenAI网站的博客上给出了生动的例子，输入文本提示（Prompt）：“an illustration of baby daikon radish in a tutu walking a dog（穿着芭蕾舞裙的小白萝卜在遛狗）”，即可得到一系列创意的插画作品（图9）。　　图9 将小白萝卜、狗、芭蕾舞裙这些元素组合到一起，构成超现实主义的插画　　在DALL-E之后，类似的AI绘画大模型纷纷涌现，比如Stable Diffusion、Midjourney、Disco Diffusion。这里，我们可以在线体验一下Stable Diffusion的魅力。首先打开网页https://replicate.com/stability-ai/stable-diffusion，然后在Input下的Prompt框中输入绘画提示语，比如“山外青山楼外楼，国画”。不过国外的AI模型对中文的支持较弱，所以我们可以将提示语翻译成英语：“Hill outside Castle peak building outside building, traditional Chinese painting”，提交后就可以得到一幅不错的画作了（图10）。　　图10 为“山外青山楼外楼”古诗配画　　我们还可以试试另外一个网站（https://huggingface.co/spaces/stabilityai/stable-diffusion），同样也是基于Stable Diffusion的应用。输入上文一样的提示语，提交后即可得4幅画作样品，看起来有模有样（图11）。　　图11国画风格的AI绘画　　上述DALL-E/DALL-E 2、Stable Diffusion、Midjourney、Disco Diffusion，它们的共同点都是基于扩散模型（Diffusion Models）。扩散模型的灵感来自非平衡热力学，它定义了一个马尔可夫扩散步骤链，以缓慢地向数据添加随机噪声，然后学习逆转扩散过程以从噪声中构建所需的数据样本（图12）。　　图12 通过缓慢添加（去除）噪声生成样品的正向（反向）扩散过程的马尔可夫链（DDPM，Ho et al. 2020）　　扩散模型有多个变体，比较知名的有去噪扩散概率模型（Denoising Diffusion Probabilistic Models，DDPM），另外还有扩散概率模型（Diffusion Probabilistic Models，DPM）、噪声条件评分网络（Noise-Conditioned Score Networks，NCSN）等，它们都是基于连续扩散的过程来生成样本，但是在取样方式、噪声参数调节以及去噪过程等方面则有所不同。每个模型都有其局限性和假设，这可能会影响它们在某些任务上的表现。　　如今的AI绘画应用中扩散模型风头正劲，远远压过了GAN。但实际上在图像生成领域，这两个模型各有优缺点。扩散模型要比GAN慢，迭代更多，但是可以产生高质量的、更加逼真的视觉效果。GAN比扩散模型更快、更高效，但生成的图像质量要偏弱。　　文生图技术（Text to Image）　　近来这一波AI绘画的浪潮，文生图技术（Text to Image）是重要的推手之一。之前再智能的AI绘画工具也都需要用户亲自操作画笔（哪怕是画几个最简陋的图形）。但自DALL-E起，普通用户只需输入文本描述，AI就可以自动绘画，其方便程度超乎想象，也因此AI绘画得以在大众中广泛流行。　　任何方便用户的功能，其背后必然有大量的工作及复杂的技术支撑，文生图也是如此。以DALL-E为例，为了能够识别用户输入的文本所对应的图像，OpenAI在图像生成功能之外应用了预训练的模型CLIP（Contrastive Language–Image Pre-training，图13）。　　图13 虚线上方是CLIP模型的训练过程，建立了文本和图像的联合表示空间。虚线下方是文本到图像的生成过程，通过CLIP文本嵌入，调节产生最终图像的扩散解码器。注意：CLIP模型在图像生成的训练过程中被冻结（Ramesh et al. 2022）　　任何技术都不会是一蹴而就的，文生图技术同样是多年发展的结果，它大概经历了以下过程：　　二十世纪九十年代末期，一些早期的文本生成模型，如n-gram模型、循环神经网络（Recurrent Neural Network，RNN）等开始应用于自然语言处理领域，但是这些模型还不能直接生成图像。　　2005年，Hinton等人提出了深度信念网络（Deep Belief Network，DBN）模型，可以学习到输入文本和图像之间的复杂关系，从而实现文本到图像的转换。　　2014年，Reed等人提出一种基于GAN（Generative Adversarial Networks）的文本到图像生成方法，该方法首次将GAN应用于图像生成领域，并在一定程度上实现了文本到图像的转换。　　2015年，基于条件GAN的文本到图像生成方法出现，该方法能够根据输入的文本描述生成与之匹配的图像，并且具有更好的生成效果和多样性。　　2016年，Reed等人提出一种基于卷积神经网络（Convolutional Neural Network，CNN）和RNN的文本到图像生成方法，可以生成高分辨率的逼真图像。　　2018年，Google提出基于Transformer模型的文本到图像生成方法，能够根据输入的文本生成高质量的图像，这一方法在多个任务上取得了较好的结果。　　近年来，随着深度学习技术的不断发展和优化，基于GAN、VAE、Transformer等模型的文本到图像生成方法不断被改进和拓展，具有更高的生成质量和多样性，同时也被广泛应用于电商、广告、艺术创作等领域。　　生成式AI的未来发展趋势　　生成式AI是一项高度复杂的技术，它涉及到深度学习、自然语言处理、计算机视觉、强化学习和概率论等多个领域，因此它面临着许多挑战和限制。下面是生成式AI面临的主要挑战以及未来的发展趋势。　　1、数据质量问题　　生成式AI需要大量的高质量数据来进行训练，而现实中的数据往往存在噪声、偏差和错误，这使得生成式AI面临着许多挑战。为了解决这个问题，未来需要进一步开发和改进数据清洗和预处理技术，以提高数据质量和可用性。　　2、计算资源限制　　生成式AI需要大量的计算资源来进行训练和推理，这使得它在实际应用中面临着计算资源限制的问题。为了解决这个问题，未来需要进一步发展高效的计算和算法优化技术，以提高生成式AI的效率和性能。　　3、可解释性问题　　生成式AI的黑盒性质使得它们的工作原理难以理解和解释，这限制了它们在实际应用中的可用性和可靠性。为了解决这个问题，未来需要进一步发展可解释性的生成式AI技术，以提高它们的透明度和可理解性。　　4、多模态和跨模态生成问题　　未来的生成式AI需要进一步开发多模态和跨模态生成技术，以支持多种数据类型的生成，包括文本、图像、音频和视频等。　　5、法律和道德问题　　生成式AI可能存在版权、隐私和道德等方面的问题，这需要在应用生成式AI时加强法律和道德方面的考虑和约束，以确保其合法和道德的使用。　　总之，生成式AI仍然是一个快速发展和不断创新的领域，未来将会面临许多挑战和机遇。通过不断的研究和发展，我们可以期待生成式AI在更多领域的应用，为人类带来更多的价值和改变。另外，生成式AI涉及的领域广泛，内容丰富，一篇短文不足以窥其全貌，最后我们放几张较为全面的生成式AI分类图，希望给那些想要深入深究的朋友一点参考。CF　　图14 根据输入和生成格式划分的生成式AI模型分类（图源：arXiv:2301.04655v1）　　图15 生产式AI的代表产品时间线，其中除2021年发布的LaMDA和2023年发布的Muse之外，其他皆为2022年发布（图源：arXiv:2301.04655v1）原文刊登于2022 年 12月15 日出版《电脑爱好者》第 24 期END更多精彩，敬请期待……




