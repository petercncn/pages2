
            <!DOCTYPE html>
            <html lang="zh-CN">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <meta name="description" content="教模型死记硬背">
                <meta name="keywords" content="生成式模型的奇葩应用：生成式检索, 教模型死记硬背">
                <meta property="og:title" content="生成式模型的奇葩应用：生成式检索">
                <title>生成式模型的奇葩应用：生成式检索</title>
                <style>.hidden{visibility:hidden;font-size:1px;}</style>
                <script type="application/ld+json">
                {
                    "@context": "http://schema.org",
                    "@type": "WebPage",
                    "name": "生成式模型的奇葩应用：生成式检索",
                    "description": "教模型死记硬背",
                    "code": "/s?__biz=MzI4MzEyOTIzOA==&mid=2648565034&idx=1&sn=dd217ba1b178dca95fef8269c244626c&chksm=f3a621d3c4d1a8c5aabc2e826a3622807b56b831dee4d45bf43ad6d665318d1fa4eede355009#rd"
                }
                </script>
            </head>
            <body>
                <div class="rich_media_wrp" id="img-content">
<h1 class="rich_media_title" id="activity-name">
            
生成式模型的奇葩应用：生成式检索
          </h1>

<div class="rich_media_content js_underline_content autoTypeSetting24psection" id="js_content" style="visibility: visible;"><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com" style='margin-bottom: 0px;padding-left: 10px;padding-right: 10px;background-attachment: scroll;background-clip: border-box;background-image: none;background-origin: padding-box;background-position: 0% 0%;background-repeat: no-repeat;background-size: auto;width: auto;font-family: Optima, "Microsoft YaHei", PingFangSC-regular, serif;font-size: 16px;color: rgb(0, 0, 0);line-height: 1.5em;word-spacing: 0em;letter-spacing: 0em;word-break: break-word;text-align: left;'><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">最近在学习一个奇特的技术，叫做<strong style="background-attachment: scroll;background-clip: border-box;background-image: none;background-origin: padding-box;background-position: 0% 0%;background-repeat: no-repeat;background-size: auto;width: auto;height: auto;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;">生成式检索</strong>。</p><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">生成式检索是一种利用生成式语言模型来检索的全新信息检索方法。不同于依赖外部索引的传统方法，生成式检索利用单个强大的模型来处理查询和文档语料库。在这个注重推理能力，讲agent的年代，生成式检索却走向了让模型“死记硬背”的另一个极端。</p><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">生成式检索用简单的话来说，就是对于输入查询（query），让模型直接生成出语料库里相关文章的id，是的，你没有看错，是直接生成id！当然，这就会涉及到一个很重要的问题——id长啥样？当前的主流做法有几种：</p><ol class="list-paddingleft-1" data-tool="mdnice编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;"><li><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.8em;letter-spacing: 0em;">原子id（atomic id），即每篇文章用一个独立的token来表示。这实际上是挂羊头卖狗肉，说是生成式，但因为每个文章都有独立token，所以<strong style="color: rgb(0, 0, 0);background-attachment: scroll;background-clip: border-box;background-image: none;background-origin: padding-box;background-position: 0% 0%;background-repeat: no-repeat;background-size: auto;width: auto;height: auto;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;">等价于一个分类模型</strong>。</section></li><li><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.8em;letter-spacing: 0em;">朴素id（naive id），即id是一个字符串，这个字符串没什么特别的，就是id常用的形式，可能是<code style='color: rgb(30, 107, 184);font-size: 14px;line-height: 1.8em;letter-spacing: 0em;background: none 0% 0% / auto no-repeat scroll padding-box border-box rgba(27, 31, 35, 0.05);width: auto;margin-left: 2px;margin-right: 2px;padding: 2px 4px;border-style: none;border-width: 3px;border-color: rgb(0, 0, 0) rgba(0, 0, 0, 0.4) rgba(0, 0, 0, 0.4);border-radius: 4px;font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>12345</code>这样的数字串，也可能是<code style='color: rgb(30, 107, 184);font-size: 14px;line-height: 1.8em;letter-spacing: 0em;background: none 0% 0% / auto no-repeat scroll padding-box border-box rgba(27, 31, 35, 0.05);width: auto;margin-left: 2px;margin-right: 2px;padding: 2px 4px;border-style: none;border-width: 3px;border-color: rgb(0, 0, 0) rgba(0, 0, 0, 0.4) rgba(0, 0, 0, 0.4);border-radius: 4px;font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>apdkcr</code>这样的hash串。然后模型在推理阶段是用自己的词表生成出这个id字符串。因为id没有明确含义，这种做法着实是挺难为模型的，相当于问一个人“为人民服务”出现在毛选的第几卷第几本第几页第几行第几个字。</section></li><li><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.8em;letter-spacing: 0em;">语义id（semantic id）。和2一样，这里的id还是一个字符串，不同的是这个id是要包含语义信息的。这个大类有很多细分的做法，我列两种比较有代表性的。</section></li><ol class="list-paddingleft-1" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;color: rgb(0, 0, 0);list-style-type: lower-alpha;"><li><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.8em;letter-spacing: 0em;">直接生成URL。是论文<span style="color: rgb(30, 107, 184);font-weight: bold;">Large Language Models are Built-in Autoregressive Search Engines</span><sup style="line-height: 0;color: rgb(30, 107, 184);font-weight: bold;">[1]</sup>里的做法。他们讨论的语料是维基百科，页面的url包含非常明确的页面主题信息。这种做法是比较符合大家对生成式模型的直觉的。其实生成目标也不局限在URL，tag、category都可以使用。</section></li><li><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.8em;letter-spacing: 0em;">层次聚类。该领域经典论文Transformer Memory as a Differentiable Search Index的做法。是将大的语料库用embedding进行层次聚类，直到簇的大小符合要求。这样就把语料库转换成了一棵检索树，文章的id就是从根到叶子的一条路径。虽然还是数字id，但相比于一个自增或者随机的数已经结构化了很多。</section></li></ol></ol><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">从后面的结果表格中可以看出，原子id效果是最好的，这个比较符合预期，但语义id效果最差，感觉这里是还有一些空间的。</p><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img alt="生成式模型的奇葩应用：生成式检索" class="rich_pages wxw-img" data-imgfileid="501081378" data-ratio="0.4172661870503597" data-src="https://mmbiz.qpic.cn/mmbiz_png/fI41EfAgQvvp0Ftz1BiaxUBLd8ql4yBZssdV0buaZtEPoptgtHBoQ08FVceIHUOiaMSdj0YOEDqMlNjcW7ib8N5icw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="834" src="20240526_235400_0.jpeg" style="display: block;margin-right: auto;margin-left: auto;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;object-fit: fill;box-shadow: rgba(0, 0, 0, 0) 0px 0px 0px 0px;" title="生成式模型的奇葩应用：生成式检索"/><figcaption style="color: rgb(136, 136, 136);font-size: 14px;line-height: 1.5em;letter-spacing: 0em;text-align: center;margin-top: 5px;">层次聚类示意图</figcaption></figure><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">说完了id的问题，另一个重要问题是<code style='color: rgb(30, 107, 184);font-size: 14px;line-height: 1.8em;letter-spacing: 0em;background: none 0% 0% / auto no-repeat scroll padding-box border-box rgba(27, 31, 35, 0.05);width: auto;height: auto;margin-left: 2px;margin-right: 2px;padding: 2px 4px;border-style: none;border-width: 3px;border-color: rgb(0, 0, 0) rgba(0, 0, 0, 0.4) rgba(0, 0, 0, 0.4);border-radius: 4px;font-family: "Operator Mono", Consolas, Monaco, Menlo, monospace;word-break: break-all;'>怎么让模型记住这么多id?</code>方法也很土很暴力，就是搞一堆(query, docid) pair 让模型学就完了。如果这种数据不够，那有一些方法合成，比较典型的有：</p><ol class="list-paddingleft-1" data-tool="mdnice编辑器" style="margin-top: 8px;margin-bottom: 8px;padding-left: 25px;"><li><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.8em;letter-spacing: 0em;">DAQ(Document as query):通常是在文章里取一段内容做query。</section></li><li><section style="margin-top: 5px;margin-bottom: 5px;color: rgb(1, 1, 1);line-height: 1.8em;letter-spacing: 0em;">D2Q(Document to query):再搞一个模型根据内容生成一些可以回答的问题。</section></li></ol><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">显然D2Q搞出来的数据更有可能接近使用场景的真实数据，所以效果好很多。D2Q其实不是什么新方法，四五年前就有人在IR任务里拿来增强模型。这两种方法都可以产生大量的训练数据，让模型充分死记硬背。</p><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img alt="生成式模型的奇葩应用：生成式检索" class="rich_pages wxw-img" data-imgfileid="501081381" data-ratio="0.75" data-src="https://mmbiz.qpic.cn/mmbiz_png/fI41EfAgQvvp0Ftz1BiaxUBLd8ql4yBZsb5X1Z34aF0f6oBBJ8cC1YjGAE19qvU5aXjfFPNgpt37EdCOLcE2jRQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240526_235404_1.jpeg" style="display: block;margin-right: auto;margin-left: auto;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;object-fit: fill;box-shadow: rgba(0, 0, 0, 0) 0px 0px 0px 0px;" title="生成式模型的奇葩应用：生成式检索"/><figcaption style="color: rgb(136, 136, 136);font-size: 14px;line-height: 1.5em;letter-spacing: 0em;text-align: center;margin-top: 5px;">Doc2Query不是什么新鲜玩意儿</figcaption></figure><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">最后看下这种方法的效果和存在的问题。</p><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">根据<span style="color: rgb(30, 107, 184);font-weight: bold;">这篇论文</span><sup style="line-height: 0;color: rgb(30, 107, 184);font-weight: bold;">[2]</sup>的数据，在语料库不大（100k左右）的情况下这个方法还是表现不错的，可以超过bm25和经典的dual encoder+ann。但不得不说，这个成本可不低，100k文档的训练数据可能是4-5M条，要跑一会。</p><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><img alt="生成式模型的奇葩应用：生成式检索" class="rich_pages wxw-img" data-imgfileid="501081380" data-ratio="0.6546296296296297" data-src="https://mmbiz.qpic.cn/mmbiz_png/fI41EfAgQvvp0Ftz1BiaxUBLd8ql4yBZsIX3HeK6lwibb0FiaCRObeAzqd9cUseRyibSrGzMbV5dnymUdCxFO3WHvQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="20240526_235407_2.jpeg" style="display: block;margin-right: auto;margin-left: auto;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;object-fit: fill;box-shadow: rgba(0, 0, 0, 0) 0px 0px 0px 0px;" title="生成式模型的奇葩应用：生成式检索"/><figcaption style="color: rgb(136, 136, 136);font-size: 14px;line-height: 1.5em;letter-spacing: 0em;text-align: center;margin-top: 5px;">Corpus小的时候generative retrieval表现还不错。原子id (At.)效果最好，语义id (Sm.)效果最差。</figcaption></figure><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">但当检索范围变大，这种方法的效果下降非常明显。这个结论也不意外，背几首古诗和背新华字典的难度肯定不同。随着语料库变大很快就差于经典的召回方法了。</p><figure data-tool="mdnice编辑器" style="margin-top: 10px;margin-bottom: 10px;display: flex;flex-direction: column;justify-content: center;align-items: center;"><figcaption style="color: rgb(136, 136, 136);font-size: 14px;line-height: 1.5em;letter-spacing: 0em;text-align: center;margin-top: 5px;">Corpus变大模型性能迅速下降</figcaption></figure><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">另一个明显的问题是死记硬背导致的新文档更新问题。当语料库里增加新的文档之后，模型要重新训练，速度慢不说，效果还可能有各种问题。也有<span style="color: rgb(30, 107, 184);font-weight: bold;">不少文章</span><sup style="line-height: 0;color: rgb(30, 107, 184);font-weight: bold;">[3]</sup>专门研究这个问题。</p><p data-tool="mdnice编辑器" style="line-height: 1.8em;letter-spacing: 0em;text-indent: 0em;padding-top: 8px;padding-bottom: 8px;">以上就是对近期学习的一个简单总结。总的来说这个东西学术味道浓了一些，实用价值在现阶段应该还不大。但确实难说后面会不会有跟LLM结合的点。更多相关内容可以看这个<span style="color: rgb(30, 107, 184);font-weight: bold;">Github Repo</span><sup style="line-height: 0;color: rgb(30, 107, 184);font-weight: bold;">[4]</sup>。</p><section data-tool="mdnice编辑器" style="margin-top: 30px;margin-bottom: 15px;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;background-attachment: scroll;background-clip: border-box;background-image: none;background-origin: padding-box;background-position: 0% 0%;background-repeat: no-repeat;background-size: auto;width: auto;height: auto;"><span style="display: block;font-size: 18px;line-height: 1.5em;letter-spacing: 0em;font-weight: bold;">参考资料</span></section><section data-tool="mdnice编辑器" style="border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;background-attachment: scroll;background-clip: border-box;background-image: none;background-origin: padding-box;background-position: 0% 0%;background-repeat: no-repeat;background-size: auto;width: auto;height: auto;"><span style="display: flex;font-size: 14px;line-height: 1.8em;letter-spacing: 0em;"><span style='line-height: 1.8em;letter-spacing: 0em;color: rgba(0, 0, 0, 0.6);display: inline;width: 10%;background-image: none;background-position: initial;background-size: initial;background-repeat: initial;background-attachment: initial;background-origin: initial;background-clip: initial;font-size: 80%;font-family: ptima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;padding-top: 2px;'>[1]</span><p style="text-indent: 0em;display: inline;word-break: break-all;flex-basis: 0%;flex-grow: 1;line-height: 1.8em;letter-spacing: 0em;">Large Language Models are Built-in Autoregressive Search Engines: <em style="background-attachment: scroll;background-clip: border-box;background-image: none;background-origin: padding-box;background-position: 0% 0%;background-repeat: no-repeat;background-size: auto;width: auto;height: auto;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;">http://arxiv.org/abs/2305.09612</em></p></span><span style="display: flex;font-size: 14px;line-height: 1.8em;letter-spacing: 0em;"><span style='line-height: 1.8em;letter-spacing: 0em;color: rgba(0, 0, 0, 0.6);display: inline;width: 10%;background-image: none;background-position: initial;background-size: initial;background-repeat: initial;background-attachment: initial;background-origin: initial;background-clip: initial;font-size: 80%;font-family: ptima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;padding-top: 2px;'>[2]</span><p style="text-indent: 0em;display: inline;word-break: break-all;flex-basis: 0%;flex-grow: 1;line-height: 1.8em;letter-spacing: 0em;">How Does Generative Retrieval Scale to Millions of Passages?: <em style="background-attachment: scroll;background-clip: border-box;background-image: none;background-origin: padding-box;background-position: 0% 0%;background-repeat: no-repeat;background-size: auto;width: auto;height: auto;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;">http://arxiv.org/abs/2305.11841</em></p></span><span style="display: flex;font-size: 14px;line-height: 1.8em;letter-spacing: 0em;"><span style='line-height: 1.8em;letter-spacing: 0em;color: rgba(0, 0, 0, 0.6);display: inline;width: 10%;background-image: none;background-position: initial;background-size: initial;background-repeat: initial;background-attachment: initial;background-origin: initial;background-clip: initial;font-size: 80%;font-family: ptima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;padding-top: 2px;'>[3]</span><p style="text-indent: 0em;display: inline;word-break: break-all;flex-basis: 0%;flex-grow: 1;line-height: 1.8em;letter-spacing: 0em;">DSI++: Updating Transformer Memory with New Documents: <em style="background-attachment: scroll;background-clip: border-box;background-image: none;background-origin: padding-box;background-position: 0% 0%;background-repeat: no-repeat;background-size: auto;width: auto;height: auto;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;">http://arxiv.org/abs/2212.09744</em></p></span><span style="display: flex;font-size: 14px;line-height: 1.8em;letter-spacing: 0em;"><span style='line-height: 1.8em;letter-spacing: 0em;color: rgba(0, 0, 0, 0.6);display: inline;width: 10%;background-image: none;background-position: initial;background-size: initial;background-repeat: initial;background-attachment: initial;background-origin: initial;background-clip: initial;font-size: 80%;font-family: ptima-Regular, Optima, PingFangSC-light, PingFangTC-light, "PingFang SC", Cambria, Cochin, Georgia, Times, "Times New Roman", serif;padding-top: 2px;'>[4]</span><p style="text-indent: 0em;display: inline;word-break: break-all;flex-basis: 0%;flex-grow: 1;line-height: 1.8em;letter-spacing: 0em;">awesome-generative-information-retrieval: <em style="background-attachment: scroll;background-clip: border-box;background-image: none;background-origin: padding-box;background-position: 0% 0%;background-repeat: no-repeat;background-size: auto;width: auto;height: auto;border-style: none;border-width: 3px;border-color: rgba(0, 0, 0, 0.4);border-radius: 0px;">https://github.com/gabriben/awesome-generative-information-retrieval</em></p></span></section></section><p><br/></p><p style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></div>

</div>
                <p></p>
                <p><a href="../index.html">返回：生成式模型的奇葩应用：生成式检索</a></p>
                <div>本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 </div>
                <div  class="hidden">本文由“公众号文章抓取器”生成，请忽略上文所有联系方式或指引式信息。有问题可以联系：五人工作室，官网：www.Wuren.Work，QQ微信同号1976.424.585 <br><p class="hidden">code/s?__biz=MzI4MzEyOTIzOA==&mid=2648565034&idx=1&sn=dd217ba1b178dca95fef8269c244626c&chksm=f3a621d3c4d1a8c5aabc2e826a3622807b56b831dee4d45bf43ad6d665318d1fa4eede355009#rd </p></div>
            </body>
            </html>
            